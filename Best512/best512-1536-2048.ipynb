{"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Best-512_0:512_15labels.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0rs0NoritMk","outputId":"92b77bac-3521-4e3b-cf37-6f33a0d5c9f1","execution":{"iopub.status.busy":"2023-06-10T16:02:28.833235Z","iopub.execute_input":"2023-06-10T16:02:28.833597Z","iopub.status.idle":"2023-06-10T16:02:37.463610Z","shell.execute_reply.started":"2023-06-10T16:02:28.833568Z","shell.execute_reply":"2023-06-10T16:02:37.462462Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"2.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install transformers\n%pip install sentencepiece\n%pip install tensorflow==2.7.0\n%pip install stanza\n%pip install tensorflow-addons\n%pip install nltk","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:02:37.465849Z","iopub.execute_input":"2023-06-10T16:02:37.466605Z","iopub.status.idle":"2023-06-10T16:03:36.303036Z","shell.execute_reply.started":"2023-06-10T16:02:37.466569Z","shell.execute_reply":"2023-06-10T16:03:36.301780Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.29.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.7.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.13.0rc0, 2.13.0rc1)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.7.0\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nCollecting stanza\n  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.5/802.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.23.5)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.28.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from stanza) (1.16.0)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\nInstalling collected packages: stanza\nSuccessfully installed stanza-1.5.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (0.20.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (21.3)\nRequirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow-addons) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport re\nimport unicodedata\nimport nltk\n#from transformers import pipeline\nfrom nltk.corpus import stopwords\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\nfrom tqdm import tqdm\nimport pickle\nfrom sklearn.metrics import confusion_matrix,f1_score,classification_report\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras import regularizers\n#from transformers import *\nfrom transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\nimport pandas as pd\nfrom transformers import AutoTokenizer, TFAutoModel\nimport numpy as np\nimport gc\nimport math\nimport json\nimport stanza\nfrom tensorflow.keras import *\nimport tensorflow as tf\nfrom tensorflow.keras import *\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom transformers import TFRobertaModel,RobertaTokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.initializers import RandomUniform\n\nfrom numpy.random import seed\nimport random as python_random\nimport os\nimport sys\n\nnp.random.seed(1)\npython_random.seed(1)\ntf.random.set_seed(1)","metadata":{"id":"wYwcFK5gixXz","execution":{"iopub.status.busy":"2023-06-10T16:03:36.306542Z","iopub.execute_input":"2023-06-10T16:03:36.306891Z","iopub.status.idle":"2023-06-10T16:03:41.617591Z","shell.execute_reply.started":"2023-06-10T16:03:36.306859Z","shell.execute_reply":"2023-06-10T16:03:41.616631Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# huggingface dataset access token\n\nfrom huggingface_hub import login\nlogin(token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:03:41.620262Z","iopub.execute_input":"2023-06-10T16:03:41.621115Z","iopub.status.idle":"2023-06-10T16:03:41.768688Z","shell.execute_reply.started":"2023-06-10T16:03:41.621078Z","shell.execute_reply":"2023-06-10T16:03:41.767608Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# importing datasets\n\nfrom datasets import load_dataset\ndata = load_dataset(\"maneshkarun/median-3000\")","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:03:41.770050Z","iopub.execute_input":"2023-06-10T16:03:41.770487Z","iopub.status.idle":"2023-06-10T16:03:44.276940Z","shell.execute_reply.started":"2023-06-10T16:03:41.770452Z","shell.execute_reply":"2023-06-10T16:03:44.275997Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset parquet/maneshkarun--median-3000 to /root/.cache/huggingface/datasets/parquet/maneshkarun--median-3000-d9224ad77edfd979/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae1cf80e1e8b4059b1e9afcfdf46f035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48647d1408b7447f8ea6c2cab5729037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f6f33db330a4f23ab181e8bdbd9ee19"}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/maneshkarun--median-3000-d9224ad77edfd979/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4d371cc42e484aacfa173e55fc7972"}},"metadata":{}}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:03:44.278436Z","iopub.execute_input":"2023-06-10T16:03:44.279487Z","iopub.status.idle":"2023-06-10T16:03:44.287696Z","shell.execute_reply.started":"2023-06-10T16:03:44.279451Z","shell.execute_reply":"2023-06-10T16:03:44.286654Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'title', 'hyperpartisan', 'url', 'published_at', 'bias', 'word_count', 'cleaned_data', 'pos_tagged'],\n        num_rows: 500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_data = data['train']","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:03:44.288945Z","iopub.execute_input":"2023-06-10T16:03:44.289968Z","iopub.status.idle":"2023-06-10T16:03:44.296795Z","shell.execute_reply.started":"2023-06-10T16:03:44.289928Z","shell.execute_reply":"2023-06-10T16:03:44.295843Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_text = train_data['cleaned_data']","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:03:44.298363Z","iopub.execute_input":"2023-06-10T16:03:44.299467Z","iopub.status.idle":"2023-06-10T16:03:44.318828Z","shell.execute_reply.started":"2023-06-10T16:03:44.299374Z","shell.execute_reply":"2023-06-10T16:03:44.317973Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"texts = []\nlabels = []\n\ncount=0\n\nfor record in train_data:\n\n        count=count+1\n        new_sen = record['cleaned_data'].split()\n\n        if len(new_sen) >= 2048:\n          new_sen = new_sen[1536:2048]\n        \n        elif len(new_sen) < 512:\n          new_sen = new_sen[0:len(new_sen)]\n        \n        else:\n          new_sen = new_sen[-512:]\n          \n        new_sen = ' '.join(new_sen)\n\n        texts.append(new_sen)\n        labels.append(record['bias'])\n   \nlen_list = [len(ele.split()) for ele in texts]\n\nprint(labels)\nprint(len(labels))\n\nres = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n\nprint(\"Average Length %s\" % res) \nprint('Found %s texts.' % len(texts))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ZinwFiui-A3","outputId":"1a4d3851-73a3-444b-a286-ec608b7c3197","execution":{"iopub.status.busy":"2023-06-10T16:03:44.319906Z","iopub.execute_input":"2023-06-10T16:03:44.320180Z","iopub.status.idle":"2023-06-10T16:03:44.553698Z","shell.execute_reply.started":"2023-06-10T16:03:44.320156Z","shell.execute_reply":"2023-06-10T16:03:44.552739Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 0, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 1, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 2, 4, 2, 2, 0, 2, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 0, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 0, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 4, 1, 1, 2, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 3, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 0, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 3, 2, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 2, 2, 2, 4, 2, 1, 2, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 1, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 1, 4, 2, 4, 4, 0, 2, 2, 2, 4, 4, 4, 2, 2, 2, 3, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 4, 1, 4, 4, 0, 4, 4, 4, 4, 4, 4, 3, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 1, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 3, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 4, 4, 4, 3]\n500\nAverage Length 512.0\nFound 500 texts.\n","output_type":"stream"}]},{"cell_type":"code","source":"summarized_data = pd.DataFrame(texts,\n               columns =['text'])\nsummarized_data['label'] = labels\nprint(summarized_data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LprCHRM2aWb8","outputId":"3377113e-6825-4977-bf5d-ac9c08887b56","execution":{"iopub.status.busy":"2023-06-10T16:03:44.556827Z","iopub.execute_input":"2023-06-10T16:03:44.557182Z","iopub.status.idle":"2023-06-10T16:03:44.579185Z","shell.execute_reply.started":"2023-06-10T16:03:44.557156Z","shell.execute_reply":"2023-06-10T16:03:44.576820Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"                                                  text  label\n0    on the back of our necks and he would say you ...      4\n1    has no choice but to consistently move to his ...      4\n2    not by much TARP decreased federal spending by...      2\n3    in New York on September 12 2001 the globe8217...      4\n4    Texas Sen Lloyd Bentsen Bentsen said he and se...      4\n..                                                 ...    ...\n495  each day either knocking into somebody8217s ap...      3\n496  cops and guards both by night and by day where...      4\n497  irregular and inconsistent8221 The dropout amo...      4\n498  distress as real as that may be to all too man...      4\n499  white children In all 63 percent of the 3700 c...      3\n\n[500 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_model():\n    inps = Input(shape = (max_len,), dtype='int64')\n    masks= Input(shape = (max_len,), dtype='int64')\n    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n    dropout_0= Dropout(0.5)(dense_0)\n    pred = Dense(5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n    print(model.summary())\n    return model   ","metadata":{"id":"VoY1gHZoaZmG","execution":{"iopub.status.busy":"2023-06-10T16:03:44.582888Z","iopub.execute_input":"2023-06-10T16:03:44.583793Z","iopub.status.idle":"2023-06-10T16:03:44.592051Z","shell.execute_reply.started":"2023-06-10T16:03:44.583753Z","shell.execute_reply":"2023-06-10T16:03:44.591115Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\ntotal_accuracy=0\ntotal_weighted_f1=0\ntotal_micro_f1=0\ntotal_weighted_precision=0\ntotal_micro_precision=0\ntotal_weighted_recall=0\ntotal_micro_recall=0\n\nfor i in range(5):\n  gc.collect()\n  tf.keras.backend.clear_session()\n  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n  max_len=512\n  sentences=summarized_data['text']\n  labels=summarized_data['label']\n  len(sentences),len(labels)\n  model_0=create_model()\n  input_ids=[]\n  attention_masks=[]\n\n  for sent in sentences:\n    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n    input_ids.append(dbert_inps['input_ids'])\n    attention_masks.append(dbert_inps['attention_mask'])\n  input_ids=np.asarray(input_ids)\n\n  attention_masks=np.array(attention_masks)\n  labels=np.array(labels)\n  train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n  log_dir='dbert_model'\n\n  model_save_path='./drive/MyDrive/Best-512/best-512-0-512'+str(i)+'-15labels.h5'\n\n  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n  gpu_info = !nvidia-smi\n  gpu_info = '\\n'.join(gpu_info)\n  if gpu_info.find('failed') >= 0:\n    print('Not connected to a GPU')\n  else:\n    print(gpu_info)\n  history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n  pred_labels=[]\n\n  model_saved= create_model()\n  model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n  model_saved.load_weights('./drive/MyDrive/Best-512/best-512-0-512'+str(i)+'-15labels.h5')\n\n  for i in range(0,len(val_inp)):\n    pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n    pred_label = pred.argmax(axis=1)\n    pred_labels.append(pred_label)\n  accuracy=accuracy_score(val_label, pred_labels)\n  print(\"Accuracy: \"+str(accuracy))\n  total_accuracy=total_accuracy+accuracy\n  \n  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n  print(\"Weighted F1: \"+ str(weighted_f1))\n  total_weighted_f1=total_weighted_f1+weighted_f1\n  micro_f1=f1_score(val_label,pred_labels, average='micro')\n  print(\"Micro F1: \"+ str(micro_f1))\n  total_micro_f1=total_micro_f1+micro_f1\n\n  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n  print(\"Weighted Precision: \" + str(weighted_precision))\n  total_weighted_precision=total_weighted_precision+weighted_precision\n  micro_precision=precision_score(val_label, pred_labels, average='micro')\n  print(\"Micro Precision: \" + str(micro_precision))\n  total_micro_precision=total_micro_precision+micro_precision\n\n  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n  print(\"Weighted Recall: \" + str(weighted_recall))\n  total_weighted_recall=total_weighted_recall+weighted_recall\n  micro_recall=recall_score(val_label, pred_labels, average='micro')\n  print(\"Micro Recall: \" + str(micro_recall))\n  total_micro_recall=total_micro_recall+micro_recall\n\n\nprint(\"Average Accuracy: \"+str(total_accuracy/5))\nprint(\"Average Weighted F1: \"+str(total_weighted_f1/5))\nprint(\"Average Micro F1: \"+str(total_micro_f1/5))\nprint(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\nprint(\"Average Micro Precision: \"+str(total_micro_precision/5))\nprint(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\nprint(\"Average Micro Recall: \"+str(total_micro_recall/5))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x9kO4eVwCHKg","outputId":"a3776971-f469-4ae7-dd89-06b3b2630cf1","execution":{"iopub.status.busy":"2023-06-10T16:03:44.593813Z","iopub.execute_input":"2023-06-10T16:03:44.594230Z","iopub.status.idle":"2023-06-10T16:24:52.200995Z","shell.execute_reply.started":"2023-06-10T16:03:44.594192Z","shell.execute_reply":"2023-06-10T16:24:52.200024Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d99edd9edce4e39a38fcc2c5c9b7bd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf46b3e3697456f8cdaf05ec8b2c4c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bc4c35e9c14096960673ac752e2d6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77e853e728c94662a4af8441bd4e0e00"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:04:13 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0    34W / 250W |  15857MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 86s 667ms/step - loss: 7.0993 - accuracy: 0.6622 - val_loss: 6.9308 - val_accuracy: 0.7400\nEpoch 2/5\n57/57 [==============================] - 35s 612ms/step - loss: 6.5103 - accuracy: 0.8600 - val_loss: 6.5496 - val_accuracy: 0.8400\nEpoch 3/5\n57/57 [==============================] - 33s 586ms/step - loss: 6.1996 - accuracy: 0.9089 - val_loss: 6.4521 - val_accuracy: 0.8000\nEpoch 4/5\n57/57 [==============================] - 33s 586ms/step - loss: 6.0009 - accuracy: 0.9244 - val_loss: 6.3803 - val_accuracy: 0.8200\nEpoch 5/5\n57/57 [==============================] - 33s 586ms/step - loss: 5.8169 - accuracy: 0.9467 - val_loss: 6.0765 - val_accuracy: 0.8400\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 97ms/step\n1/1 [==============================] - 0s 88ms/step\n1/1 [==============================] - 0s 93ms/step\n1/1 [==============================] - 0s 87ms/step\n1/1 [==============================] - 0s 83ms/step\n1/1 [==============================] - 0s 84ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 84ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\nAccuracy: 0.84\nWeighted F1: 0.7965322580645161\nMicro F1: 0.8399999999999999\nWeighted Precision: 0.7626666666666668\nMicro Precision: 0.84\nWeighted Recall: 0.84\nMicro Recall: 0.84\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:08:27 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   54C    P0    37W / 250W |  15905MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 85s 680ms/step - loss: 7.0479 - accuracy: 0.6667 - val_loss: 6.6245 - val_accuracy: 0.8200\nEpoch 2/5\n57/57 [==============================] - 34s 604ms/step - loss: 6.4229 - accuracy: 0.8911 - val_loss: 6.3954 - val_accuracy: 0.8400\nEpoch 3/5\n57/57 [==============================] - 33s 582ms/step - loss: 6.1642 - accuracy: 0.9178 - val_loss: 6.4020 - val_accuracy: 0.8400\nEpoch 4/5\n57/57 [==============================] - 34s 601ms/step - loss: 5.9929 - accuracy: 0.9333 - val_loss: 6.3583 - val_accuracy: 0.8600\nEpoch 5/5\n57/57 [==============================] - 34s 600ms/step - loss: 5.7714 - accuracy: 0.9489 - val_loss: 6.2373 - val_accuracy: 0.8800\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 82ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 76ms/step\n1/1 [==============================] - 0s 90ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 88ms/step\n1/1 [==============================] - 0s 92ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 82ms/step\n1/1 [==============================] - 0s 82ms/step\n1/1 [==============================] - 0s 82ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 71ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 66ms/step\nAccuracy: 0.88\nWeighted F1: 0.8609032258064516\nMicro F1: 0.88\nWeighted Precision: 0.8507878787878788\nMicro Precision: 0.88\nWeighted Recall: 0.88\nMicro Recall: 0.88\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:12:41 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   55C    P0    38W / 250W |  15909MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 83s 654ms/step - loss: 7.0536 - accuracy: 0.6733 - val_loss: 6.8586 - val_accuracy: 0.7400\nEpoch 2/5\n57/57 [==============================] - 34s 603ms/step - loss: 6.4690 - accuracy: 0.8889 - val_loss: 6.4862 - val_accuracy: 0.8600\nEpoch 3/5\n57/57 [==============================] - 34s 606ms/step - loss: 6.1551 - accuracy: 0.9311 - val_loss: 6.4864 - val_accuracy: 0.8000\nEpoch 4/5\n57/57 [==============================] - 33s 582ms/step - loss: 5.9834 - accuracy: 0.9489 - val_loss: 6.3301 - val_accuracy: 0.8600\nEpoch 5/5\n57/57 [==============================] - 33s 582ms/step - loss: 5.8033 - accuracy: 0.9689 - val_loss: 6.1630 - val_accuracy: 0.8400\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 94ms/step\n1/1 [==============================] - 0s 87ms/step\n1/1 [==============================] - 0s 101ms/step\n1/1 [==============================] - 0s 85ms/step\n1/1 [==============================] - 0s 85ms/step\n1/1 [==============================] - 0s 85ms/step\n1/1 [==============================] - 0s 78ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 101ms/step\n1/1 [==============================] - 0s 62ms/step\nAccuracy: 0.86\nWeighted F1: 0.8162051282051282\nMicro F1: 0.8599999999999999\nWeighted Precision: 0.7872222222222223\nMicro Precision: 0.86\nWeighted Recall: 0.86\nMicro Recall: 0.86\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:16:54 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   55C    P0    38W / 250W |  15913MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 85s 677ms/step - loss: 6.9894 - accuracy: 0.6822 - val_loss: 6.6449 - val_accuracy: 0.8000\nEpoch 2/5\n57/57 [==============================] - 34s 601ms/step - loss: 6.4842 - accuracy: 0.8622 - val_loss: 6.4822 - val_accuracy: 0.8400\nEpoch 3/5\n57/57 [==============================] - 34s 601ms/step - loss: 6.1216 - accuracy: 0.9156 - val_loss: 6.3381 - val_accuracy: 0.8600\nEpoch 4/5\n57/57 [==============================] - 33s 582ms/step - loss: 5.9311 - accuracy: 0.9311 - val_loss: 6.1290 - val_accuracy: 0.8600\nEpoch 5/5\n57/57 [==============================] - 34s 600ms/step - loss: 5.7187 - accuracy: 0.9533 - val_loss: 6.0081 - val_accuracy: 0.8800\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 99ms/step\n1/1 [==============================] - 0s 72ms/step\n1/1 [==============================] - 0s 70ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 71ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 70ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\nAccuracy: 0.88\nWeighted F1: 0.8542916666666668\nMicro F1: 0.88\nWeighted Precision: 0.8605714285714285\nMicro Precision: 0.88\nWeighted Recall: 0.88\nMicro Recall: 0.88\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:21:02 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   56C    P0    38W / 250W |  15917MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 83s 653ms/step - loss: 6.9560 - accuracy: 0.7356 - val_loss: 7.0316 - val_accuracy: 0.6600\nEpoch 2/5\n57/57 [==============================] - 35s 608ms/step - loss: 6.4717 - accuracy: 0.8756 - val_loss: 6.4760 - val_accuracy: 0.8400\nEpoch 3/5\n57/57 [==============================] - 33s 583ms/step - loss: 6.1816 - accuracy: 0.9244 - val_loss: 6.4612 - val_accuracy: 0.8000\nEpoch 4/5\n57/57 [==============================] - 33s 582ms/step - loss: 5.9542 - accuracy: 0.9556 - val_loss: 6.3344 - val_accuracy: 0.8200\nEpoch 5/5\n57/57 [==============================] - 34s 605ms/step - loss: 5.7986 - accuracy: 0.9600 - val_loss: 6.2751 - val_accuracy: 0.8200\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 69ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 69ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\nAccuracy: 0.84\nWeighted F1: 0.7965322580645161\nMicro F1: 0.8399999999999999\nWeighted Precision: 0.7626666666666668\nMicro Precision: 0.84\nWeighted Recall: 0.84\nMicro Recall: 0.84\nAverage Accuracy: 0.86\nAverage Weighted F1: 0.8248929073614558\nAverage Micro F1: 0.8599999999999998\nAverage Weighted Precision: 0.8047829725829727\nAverage Micro Precision: 0.86\nAverage Weighted Recall: 0.86\nAverage Micro Recall: 0.86\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]}]}