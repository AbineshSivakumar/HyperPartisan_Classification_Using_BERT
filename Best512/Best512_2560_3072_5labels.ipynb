{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:57:53.184910Z","iopub.status.busy":"2023-06-10T16:57:53.184150Z","iopub.status.idle":"2023-06-10T16:58:53.983531Z","shell.execute_reply":"2023-06-10T16:58:53.982271Z","shell.execute_reply.started":"2023-06-10T16:57:53.184873Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.29.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.7.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.13.0rc0, 2.13.0rc1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.7.0\u001b[0m\u001b[31m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Collecting stanza\n","  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.5/802.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.2.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.23.5)\n","Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.28.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from stanza) (1.16.0)\n","Requirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.0.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.64.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.12.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.5.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2023.5.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n","Installing collected packages: stanza\n","Successfully installed stanza-1.5.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (0.20.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow-addons) (3.0.9)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install transformers\n","%pip install sentencepiece\n","%pip install tensorflow==2.7.0\n","%pip install stanza\n","%pip install tensorflow-addons\n","%pip install nltk\n","%pip install datasets"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T16:57:44.483384Z","iopub.status.busy":"2023-06-10T16:57:44.482654Z","iopub.status.idle":"2023-06-10T16:57:53.181859Z","shell.execute_reply":"2023-06-10T16:57:53.180024Z","shell.execute_reply.started":"2023-06-10T16:57:44.483341Z"},"id":"K0rs0NoritMk","outputId":"92b77bac-3521-4e3b-cf37-6f33a0d5c9f1","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]},{"name":"stdout","output_type":"stream","text":["2.12.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:58:53.986012Z","iopub.status.busy":"2023-06-10T16:58:53.985636Z","iopub.status.idle":"2023-06-10T16:58:59.256427Z","shell.execute_reply":"2023-06-10T16:58:59.255495Z","shell.execute_reply.started":"2023-06-10T16:58:53.985971Z"},"id":"wYwcFK5gixXz","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import re\n","import unicodedata\n","import nltk\n","#from transformers import pipeline\n","from nltk.corpus import stopwords\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n","from tqdm import tqdm\n","import pickle\n","from sklearn.metrics import confusion_matrix,f1_score,classification_report\n","import matplotlib.pyplot as plt\n","import itertools\n","from sklearn.utils import shuffle\n","from tensorflow.keras import regularizers\n","#from transformers import *\n","from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n","import pandas as pd\n","from transformers import AutoTokenizer, TFAutoModel\n","import numpy as np\n","import gc\n","import math\n","import json\n","import stanza\n","from tensorflow.keras import *\n","import tensorflow as tf\n","from tensorflow.keras import *\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import classification_report\n","from transformers import TFRobertaModel,RobertaTokenizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.initializers import RandomUniform\n","\n","from numpy.random import seed\n","import random as python_random\n","import os\n","import sys\n","\n","np.random.seed(1)\n","python_random.seed(1)\n","tf.random.set_seed(1)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:58:59.260063Z","iopub.status.busy":"2023-06-10T16:58:59.259319Z","iopub.status.idle":"2023-06-10T16:58:59.642697Z","shell.execute_reply":"2023-06-10T16:58:59.641686Z","shell.execute_reply.started":"2023-06-10T16:58:59.260029Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["# huggingface dataset access token\n","\n","from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:58:59.645359Z","iopub.status.busy":"2023-06-10T16:58:59.645065Z","iopub.status.idle":"2023-06-10T16:59:02.201904Z","shell.execute_reply":"2023-06-10T16:59:02.200873Z","shell.execute_reply.started":"2023-06-10T16:58:59.645334Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset parquet/maneshkarun--median-3000 to /root/.cache/huggingface/datasets/parquet/maneshkarun--median-3000-d9224ad77edfd979/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9524a39c74e4152b338def725f022b6","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72a2a228fd0148af9ad94ea6e6d4b029","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37c047fd9b1040238341fcdc6fc25953","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/maneshkarun--median-3000-d9224ad77edfd979/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b04173488ba4353af9566d4a2672c67","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# importing datasets\n","\n","from datasets import load_dataset\n","\n","# data = load_dataset(\"maneshkarun/median-3000\")\n","data = load_dataset(\"maneshkarun/median3k_10000s\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:59:02.204755Z","iopub.status.busy":"2023-06-10T16:59:02.203275Z","iopub.status.idle":"2023-06-10T16:59:02.212071Z","shell.execute_reply":"2023-06-10T16:59:02.211181Z","shell.execute_reply.started":"2023-06-10T16:59:02.204719Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'title', 'hyperpartisan', 'url', 'published_at', 'bias', 'word_count', 'cleaned_data', 'pos_tagged'],\n","        num_rows: 500\n","    })\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:59:02.214263Z","iopub.status.busy":"2023-06-10T16:59:02.213624Z","iopub.status.idle":"2023-06-10T16:59:02.222063Z","shell.execute_reply":"2023-06-10T16:59:02.221188Z","shell.execute_reply.started":"2023-06-10T16:59:02.214230Z"},"trusted":true},"outputs":[],"source":["train_data = data['train']"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:59:02.224251Z","iopub.status.busy":"2023-06-10T16:59:02.223614Z","iopub.status.idle":"2023-06-10T16:59:02.241301Z","shell.execute_reply":"2023-06-10T16:59:02.240389Z","shell.execute_reply.started":"2023-06-10T16:59:02.224199Z"},"trusted":true},"outputs":[],"source":["train_text = train_data['cleaned_data']"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T16:59:02.243503Z","iopub.status.busy":"2023-06-10T16:59:02.242819Z","iopub.status.idle":"2023-06-10T16:59:02.466893Z","shell.execute_reply":"2023-06-10T16:59:02.465865Z","shell.execute_reply.started":"2023-06-10T16:59:02.243469Z"},"id":"2ZinwFiui-A3","outputId":"1a4d3851-73a3-444b-a286-ec608b7c3197","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 0, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 1, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 2, 4, 2, 2, 0, 2, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 0, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 0, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 4, 1, 1, 2, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 3, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 0, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 3, 2, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 2, 2, 2, 4, 2, 1, 2, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 1, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 1, 4, 2, 4, 4, 0, 2, 2, 2, 4, 4, 4, 2, 2, 2, 3, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 4, 1, 4, 4, 0, 4, 4, 4, 4, 4, 4, 3, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 1, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 3, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 4, 4, 4, 3]\n","500\n","Average Length 512.0\n","Found 500 texts.\n"]}],"source":["texts = []\n","labels = []\n","\n","count=0\n","\n","for record in train_data:\n","    count=count+1\n","    new_sen = record['cleaned_data'].split()\n","    if len(new_sen) >= 3072:\n","        new_sen = new_sen[2560:3072]\n","        \n","    elif len(new_sen) < 512:\n","        new_sen = new_sen[0:len(new_sen)]\n","        \n","    else:\n","        new_sen = new_sen[-512:]\n","          \n","    new_sen = ' '.join(new_sen)\n","\n","    texts.append(new_sen)\n","    labels.append(record['bias'])\n","    \n","len_list = [len(ele.split()) for ele in texts]\n","print(labels)\n","print(len(labels))\n","\n","res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n","\n","print(\"Average Length %s\" % res) \n","print('Found %s texts.' % len(texts))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T16:59:02.468738Z","iopub.status.busy":"2023-06-10T16:59:02.468409Z","iopub.status.idle":"2023-06-10T16:59:02.486099Z","shell.execute_reply":"2023-06-10T16:59:02.484898Z","shell.execute_reply.started":"2023-06-10T16:59:02.468707Z"},"id":"LprCHRM2aWb8","outputId":"3377113e-6825-4977-bf5d-ac9c08887b56","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                  text  label\n","0    Christmas Eve 122448doubling numbers JAY By th...      4\n","1    talk to diplomats who are in the tank who are ...      4\n","2    White House 8220 Statement from the President ...      2\n","3    instructive After all Feinstein has made her r...      4\n","4    investigation by the Breakthrough Institute ov...      4\n","..                                                 ...    ...\n","495  the 8220bad actor8221 Typically the rest of th...      3\n","496  elections may ultimately pave the way for the ...      4\n","497  existed Families that could afford to do so se...      4\n","498  its imperialist alliances the European Union a...      4\n","499  people In Stevens8217 case it took many months...      3\n","\n","[500 rows x 2 columns]\n"]}],"source":["summarized_data = pd.DataFrame(texts,\n","               columns =['text'])\n","summarized_data['label'] = labels\n","print(summarized_data)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:59:02.489836Z","iopub.status.busy":"2023-06-10T16:59:02.489568Z","iopub.status.idle":"2023-06-10T16:59:02.497290Z","shell.execute_reply":"2023-06-10T16:59:02.495326Z","shell.execute_reply.started":"2023-06-10T16:59:02.489813Z"},"id":"VoY1gHZoaZmG","trusted":true},"outputs":[],"source":["def create_model():\n","    inps = Input(shape = (max_len,), dtype='int64')\n","    masks= Input(shape = (max_len,), dtype='int64')\n","    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n","    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n","    dropout_0= Dropout(0.5)(dense_0)\n","    pred = Dense(5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n","    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n","    print(model.summary())\n","    return model   "]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T16:59:02.499534Z","iopub.status.busy":"2023-06-10T16:59:02.498861Z","iopub.status.idle":"2023-06-10T17:20:11.395110Z","shell.execute_reply":"2023-06-10T17:20:11.393815Z","shell.execute_reply.started":"2023-06-10T16:59:02.499502Z"},"id":"x9kO4eVwCHKg","outputId":"a3776971-f469-4ae7-dd89-06b3b2630cf1","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dddcfe94ec54941b41b34c8e9ccf7eb","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cdc1ee9fdc24c0ab5601fb805a6faf1","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fd0d92d55574172bb6072ed2890c0e1","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8745ba3b6f2d4f78a23046a16dc81694","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Sat Jun 10 16:59:32 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    31W / 250W |  15857MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n","57/57 [==============================] - 86s 687ms/step - loss: 7.1237 - accuracy: 0.6600 - val_loss: 7.0781 - val_accuracy: 0.6400\n","Epoch 2/5\n","57/57 [==============================] - 35s 607ms/step - loss: 6.5038 - accuracy: 0.8800 - val_loss: 6.5063 - val_accuracy: 0.8600\n","Epoch 3/5\n","57/57 [==============================] - 33s 587ms/step - loss: 6.1895 - accuracy: 0.9111 - val_loss: 6.3321 - val_accuracy: 0.8600\n","Epoch 4/5\n","57/57 [==============================] - 33s 585ms/step - loss: 6.0240 - accuracy: 0.9133 - val_loss: 6.2245 - val_accuracy: 0.8600\n","Epoch 5/5\n","57/57 [==============================] - 33s 583ms/step - loss: 5.8191 - accuracy: 0.9444 - val_loss: 6.2061 - val_accuracy: 0.8600\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 59ms/step\n","1/1 [==============================] - 0s 69ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 59ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 59ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 115ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 59ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","Accuracy: 0.86\n","Weighted F1: 0.8155555555555556\n","Micro F1: 0.8599999999999999\n","Weighted Precision: 0.7776470588235295\n","Micro Precision: 0.86\n","Weighted Recall: 0.86\n","Micro Recall: 0.86\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Sat Jun 10 17:03:44 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P0    33W / 250W |  15905MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n","57/57 [==============================] - 82s 650ms/step - loss: 6.9589 - accuracy: 0.7178 - val_loss: 7.0109 - val_accuracy: 0.6800\n","Epoch 2/5\n","57/57 [==============================] - 34s 604ms/step - loss: 6.4520 - accuracy: 0.8778 - val_loss: 6.5031 - val_accuracy: 0.8400\n","Epoch 3/5\n","57/57 [==============================] - 34s 599ms/step - loss: 6.1928 - accuracy: 0.9178 - val_loss: 6.3547 - val_accuracy: 0.8800\n","Epoch 4/5\n","57/57 [==============================] - 33s 581ms/step - loss: 6.0575 - accuracy: 0.9244 - val_loss: 6.1889 - val_accuracy: 0.8800\n","Epoch 5/5\n","57/57 [==============================] - 34s 599ms/step - loss: 5.7870 - accuracy: 0.9622 - val_loss: 5.9791 - val_accuracy: 0.9000\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 90ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","Accuracy: 0.9\n","Weighted F1: 0.8825714285714286\n","Micro F1: 0.9\n","Weighted Precision: 0.8894736842105263\n","Micro Precision: 0.9\n","Weighted Recall: 0.9\n","Micro Recall: 0.9\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Sat Jun 10 17:07:58 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    33W / 250W |  15909MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n","57/57 [==============================] - 83s 652ms/step - loss: 6.9767 - accuracy: 0.7156 - val_loss: 6.7537 - val_accuracy: 0.8200\n","Epoch 2/5\n","57/57 [==============================] - 33s 585ms/step - loss: 6.3996 - accuracy: 0.8978 - val_loss: 6.6534 - val_accuracy: 0.8200\n","Epoch 3/5\n","57/57 [==============================] - 34s 601ms/step - loss: 6.1620 - accuracy: 0.9289 - val_loss: 6.3915 - val_accuracy: 0.8400\n","Epoch 4/5\n","57/57 [==============================] - 33s 582ms/step - loss: 5.9889 - accuracy: 0.9378 - val_loss: 6.3755 - val_accuracy: 0.8400\n","Epoch 5/5\n","57/57 [==============================] - 34s 601ms/step - loss: 5.7857 - accuracy: 0.9556 - val_loss: 6.3695 - val_accuracy: 0.8600\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 59ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 59ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 59ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 59ms/step\n","1/1 [==============================] - 0s 79ms/step\n","1/1 [==============================] - 0s 88ms/step\n","1/1 [==============================] - 0s 62ms/step\n","Accuracy: 0.86\n","Weighted F1: 0.8344444444444444\n","Micro F1: 0.8599999999999999\n","Weighted Precision: 0.836\n","Micro Precision: 0.86\n","Weighted Recall: 0.86\n","Micro Recall: 0.86\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Sat Jun 10 17:12:10 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    33W / 250W |  15913MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n","57/57 [==============================] - 82s 650ms/step - loss: 6.9757 - accuracy: 0.7044 - val_loss: 6.7500 - val_accuracy: 0.8000\n","Epoch 2/5\n","57/57 [==============================] - 34s 602ms/step - loss: 6.4227 - accuracy: 0.8867 - val_loss: 6.4463 - val_accuracy: 0.8400\n","Epoch 3/5\n","57/57 [==============================] - 34s 600ms/step - loss: 6.1245 - accuracy: 0.9267 - val_loss: 6.3102 - val_accuracy: 0.8600\n","Epoch 4/5\n","57/57 [==============================] - 34s 599ms/step - loss: 5.9164 - accuracy: 0.9356 - val_loss: 6.0791 - val_accuracy: 0.8800\n","Epoch 5/5\n","57/57 [==============================] - 33s 581ms/step - loss: 5.7145 - accuracy: 0.9644 - val_loss: 5.9399 - val_accuracy: 0.8800\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 69ms/step\n","1/1 [==============================] - 0s 89ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 60ms/step\n","Accuracy: 0.88\n","Weighted F1: 0.8531764705882352\n","Micro F1: 0.88\n","Weighted Precision: 0.8496057347670251\n","Micro Precision: 0.88\n","Weighted Recall: 0.88\n","Micro Recall: 0.88\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Sat Jun 10 17:16:14 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P0    33W / 250W |  15917MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n","57/57 [==============================] - 83s 677ms/step - loss: 6.9432 - accuracy: 0.7311 - val_loss: 6.6643 - val_accuracy: 0.8200\n","Epoch 2/5\n","57/57 [==============================] - 34s 602ms/step - loss: 6.4032 - accuracy: 0.8978 - val_loss: 6.4464 - val_accuracy: 0.8800\n","Epoch 3/5\n","57/57 [==============================] - 33s 581ms/step - loss: 6.1569 - accuracy: 0.9200 - val_loss: 6.3252 - val_accuracy: 0.8800\n","Epoch 4/5\n","57/57 [==============================] - 33s 582ms/step - loss: 5.9498 - accuracy: 0.9467 - val_loss: 6.2218 - val_accuracy: 0.8800\n","Epoch 5/5\n","57/57 [==============================] - 34s 601ms/step - loss: 5.7750 - accuracy: 0.9667 - val_loss: 6.0673 - val_accuracy: 0.9200\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 78ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 85ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","Accuracy: 0.92\n","Weighted F1: 0.9130812324929972\n","Micro F1: 0.92\n","Weighted Precision: 0.9129629629629629\n","Micro Precision: 0.92\n","Weighted Recall: 0.92\n","Micro Recall: 0.92\n","Average Accuracy: 0.884\n","Average Weighted F1: 0.8597658263305321\n","Average Micro F1: 0.884\n","Average Weighted Precision: 0.8531378881528087\n","Average Micro Precision: 0.884\n","Average Weighted Recall: 0.884\n","Average Micro Recall: 0.884\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","total_accuracy=0\n","total_weighted_f1=0\n","total_micro_f1=0\n","total_weighted_precision=0\n","total_micro_precision=0\n","total_weighted_recall=0\n","total_micro_recall=0\n","\n","for i in range(5):\n","    gc.collect()\n","    tf.keras.backend.clear_session()\n","    dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n","    max_len=512\n","    sentences=summarized_data['text']\n","    labels=summarized_data['label']\n","    len(sentences),len(labels)\n","    model_0=create_model()\n","    input_ids=[]\n","    attention_masks=[]\n","\n","    for sent in sentences:\n","        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n","        input_ids.append(dbert_inps['input_ids'])\n","        attention_masks.append(dbert_inps['attention_mask'])\n","    input_ids=np.asarray(input_ids)\n","\n","    attention_masks=np.array(attention_masks)\n","    labels=np.array(labels)\n","    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n","    log_dir='dbert_model'\n","\n","    model_save_path='./kaggle/working/roberta-best-512-0-512-' + str(i) + '-4labels.h5'\n","\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n","    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n","    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","    gpu_info = !nvidia-smi\n","    gpu_info = '\\n'.join(gpu_info)\n","    if gpu_info.find('failed') >= 0:\n","        print('Not connected to a GPU')\n","    else:\n","        print(gpu_info)\n","    history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n","    pred_labels=[]\n","\n","    model_saved= create_model()\n","    model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","    model_saved.load_weights('./kaggle/working/roberta-best-512-0-512-' + str(i) + '-4labels.h5')\n","\n","    for i in range(0,len(val_inp)):\n","        pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n","        pred_label = pred.argmax(axis=1)\n","        pred_labels.append(pred_label)\n","    accuracy=accuracy_score(val_label, pred_labels)\n","    print(\"Accuracy: \"+str(accuracy))\n","    total_accuracy=total_accuracy+accuracy\n","  \n","    weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n","    print(\"Weighted F1: \"+ str(weighted_f1))\n","    total_weighted_f1=total_weighted_f1+weighted_f1\n","    micro_f1=f1_score(val_label,pred_labels, average='micro')\n","    print(\"Micro F1: \"+ str(micro_f1))\n","    total_micro_f1=total_micro_f1+micro_f1\n","\n","    weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n","    print(\"Weighted Precision: \" + str(weighted_precision))\n","    total_weighted_precision=total_weighted_precision+weighted_precision\n","    micro_precision=precision_score(val_label, pred_labels, average='micro')\n","    print(\"Micro Precision: \" + str(micro_precision))\n","    total_micro_precision=total_micro_precision+micro_precision\n","\n","    weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n","    print(\"Weighted Recall: \" + str(weighted_recall))\n","    total_weighted_recall=total_weighted_recall+weighted_recall\n","    micro_recall=recall_score(val_label, pred_labels, average='micro')\n","    print(\"Micro Recall: \" + str(micro_recall))\n","    total_micro_recall=total_micro_recall+micro_recall\n","\n","\n","print(\"Average Accuracy: \"+str(total_accuracy/5))\n","print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n","print(\"Average Micro F1: \"+str(total_micro_f1/5))\n","print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n","print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n","print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n","print(\"Average Micro Recall: \"+str(total_micro_recall/5))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Best-512_0:512_15labels.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
