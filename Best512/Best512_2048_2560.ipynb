{"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Best-512_0:512_15labels.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0rs0NoritMk","outputId":"92b77bac-3521-4e3b-cf37-6f33a0d5c9f1","execution":{"iopub.status.busy":"2023-06-10T16:32:55.727223Z","iopub.execute_input":"2023-06-10T16:32:55.727574Z","iopub.status.idle":"2023-06-10T16:32:55.733741Z","shell.execute_reply.started":"2023-06-10T16:32:55.727546Z","shell.execute_reply":"2023-06-10T16:32:55.732166Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install transformers\n%pip install sentencepiece\n%pip install tensorflow==2.7.0\n%pip install stanza\n%pip install tensorflow-addons\n%pip install nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport re\nimport unicodedata\nimport nltk\n#from transformers import pipeline\nfrom nltk.corpus import stopwords\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\nfrom tqdm import tqdm\nimport pickle\nfrom sklearn.metrics import confusion_matrix,f1_score,classification_report\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras import regularizers\n#from transformers import *\nfrom transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\nimport pandas as pd\nfrom transformers import AutoTokenizer, TFAutoModel\nimport numpy as np\nimport gc\nimport math\nimport json\nimport stanza\nfrom tensorflow.keras import *\nimport tensorflow as tf\nfrom tensorflow.keras import *\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom transformers import TFRobertaModel,RobertaTokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.initializers import RandomUniform\n\nfrom numpy.random import seed\nimport random as python_random\nimport os\nimport sys\n\nnp.random.seed(1)\npython_random.seed(1)\ntf.random.set_seed(1)","metadata":{"id":"wYwcFK5gixXz","execution":{"iopub.status.busy":"2023-06-10T16:33:56.196621Z","iopub.execute_input":"2023-06-10T16:33:56.196950Z","iopub.status.idle":"2023-06-10T16:34:00.862809Z","shell.execute_reply.started":"2023-06-10T16:33:56.196919Z","shell.execute_reply":"2023-06-10T16:34:00.861709Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# huggingface dataset access token\n\nfrom huggingface_hub import login\nlogin(token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:34:00.864345Z","iopub.execute_input":"2023-06-10T16:34:00.865328Z","iopub.status.idle":"2023-06-10T16:34:01.018735Z","shell.execute_reply.started":"2023-06-10T16:34:00.865284Z","shell.execute_reply":"2023-06-10T16:34:01.017444Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# importing datasets\n\nfrom datasets import load_dataset\ndata = load_dataset(\"maneshkarun/median-3000\")","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:34:01.021954Z","iopub.execute_input":"2023-06-10T16:34:01.022375Z","iopub.status.idle":"2023-06-10T16:34:03.631005Z","shell.execute_reply.started":"2023-06-10T16:34:01.022325Z","shell.execute_reply":"2023-06-10T16:34:03.629872Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset parquet/maneshkarun--median-3000 to /root/.cache/huggingface/datasets/parquet/maneshkarun--median-3000-d9224ad77edfd979/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0ca1eaa5f24505a976a005b784b90e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"111035f50916426cb9522b03f1b6b03b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d73003b3ab364ec6b1b550ea756ac49d"}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/maneshkarun--median-3000-d9224ad77edfd979/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c38d1874406f4ea39cece8b1fe1a17b1"}},"metadata":{}}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:34:03.632695Z","iopub.execute_input":"2023-06-10T16:34:03.633820Z","iopub.status.idle":"2023-06-10T16:34:03.643138Z","shell.execute_reply.started":"2023-06-10T16:34:03.633782Z","shell.execute_reply":"2023-06-10T16:34:03.642212Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'title', 'hyperpartisan', 'url', 'published_at', 'bias', 'word_count', 'cleaned_data', 'pos_tagged'],\n        num_rows: 500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_data = data['train']","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:34:03.644761Z","iopub.execute_input":"2023-06-10T16:34:03.645409Z","iopub.status.idle":"2023-06-10T16:34:03.654823Z","shell.execute_reply.started":"2023-06-10T16:34:03.645374Z","shell.execute_reply":"2023-06-10T16:34:03.653814Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_text = train_data['cleaned_data']","metadata":{"execution":{"iopub.status.busy":"2023-06-10T16:34:03.656383Z","iopub.execute_input":"2023-06-10T16:34:03.657037Z","iopub.status.idle":"2023-06-10T16:34:03.672843Z","shell.execute_reply.started":"2023-06-10T16:34:03.657004Z","shell.execute_reply":"2023-06-10T16:34:03.671735Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"texts = []\nlabels = []\n\ncount=0\n\nfor record in train_data:\n    count=count+1\n    new_sen = record['cleaned_data'].split()\n    if len(new_sen) >= 2560:\n        new_sen = new_sen[2048:2560]\n        \n    elif len(new_sen) < 512:\n        new_sen = new_sen[0:len(new_sen)]\n        \n    else:\n        new_sen = new_sen[-512:]\n          \n    new_sen = ' '.join(new_sen)\n\n    texts.append(new_sen)\n    labels.append(record['bias'])\n    \nlen_list = [len(ele.split()) for ele in texts]\nprint(labels)\nprint(len(labels))\n\nres = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n\nprint(\"Average Length %s\" % res) \nprint('Found %s texts.' % len(texts))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ZinwFiui-A3","outputId":"1a4d3851-73a3-444b-a286-ec608b7c3197","execution":{"iopub.status.busy":"2023-06-10T16:34:03.674697Z","iopub.execute_input":"2023-06-10T16:34:03.675388Z","iopub.status.idle":"2023-06-10T16:34:03.892540Z","shell.execute_reply.started":"2023-06-10T16:34:03.675353Z","shell.execute_reply":"2023-06-10T16:34:03.891564Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 0, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 1, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 2, 4, 2, 2, 0, 2, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 0, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 0, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 4, 1, 1, 2, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 3, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 0, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 3, 2, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 2, 2, 2, 4, 2, 1, 2, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 1, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 1, 4, 2, 4, 4, 0, 2, 2, 2, 4, 4, 4, 2, 2, 2, 3, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 4, 1, 4, 4, 0, 4, 4, 4, 4, 4, 4, 3, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 1, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 3, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 4, 4, 4, 3]\n500\nAverage Length 512.0\nFound 500 texts.\n","output_type":"stream"}]},{"cell_type":"code","source":"summarized_data = pd.DataFrame(texts,\n               columns =['text'])\nsummarized_data['label'] = labels\nprint(summarized_data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LprCHRM2aWb8","outputId":"3377113e-6825-4977-bf5d-ac9c08887b56","execution":{"iopub.status.busy":"2023-06-10T16:34:03.894044Z","iopub.execute_input":"2023-06-10T16:34:03.894372Z","iopub.status.idle":"2023-06-10T16:34:03.914344Z","shell.execute_reply.started":"2023-06-10T16:34:03.894340Z","shell.execute_reply":"2023-06-10T16:34:03.913122Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"                                                  text  label\n0    thats quite true And on top of that you know h...      4\n1    theyve actually enacted laws which have been v...      4\n2    Friedman Emily 8220 Romney Warns of Obama8217s...      2\n3    have repeatedly been playing the losing hands ...      4\n4    is still playing catchup An analysis by DBL In...      4\n..                                                 ...    ...\n495  8216Where I8217m gonna go with six kids8217822...      3\n496  soldiers have been demobilized 55 million stud...      4\n497  A nowin situation resulted Teachers wee malign...      4\n498  increasingly French presidents of both parties...      4\n499  Bell said After the initial report is filed an...      3\n\n[500 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_model():\n    inps = Input(shape = (max_len,), dtype='int64')\n    masks= Input(shape = (max_len,), dtype='int64')\n    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n    dropout_0= Dropout(0.5)(dense_0)\n    pred = Dense(5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n    print(model.summary())\n    return model   ","metadata":{"id":"VoY1gHZoaZmG","execution":{"iopub.status.busy":"2023-06-10T16:34:03.917717Z","iopub.execute_input":"2023-06-10T16:34:03.918153Z","iopub.status.idle":"2023-06-10T16:34:03.924638Z","shell.execute_reply.started":"2023-06-10T16:34:03.918120Z","shell.execute_reply":"2023-06-10T16:34:03.923729Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\ntotal_accuracy=0\ntotal_weighted_f1=0\ntotal_micro_f1=0\ntotal_weighted_precision=0\ntotal_micro_precision=0\ntotal_weighted_recall=0\ntotal_micro_recall=0\n\nfor i in range(5):\n    gc.collect()\n    tf.keras.backend.clear_session()\n    dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n    max_len=512\n    sentences=summarized_data['text']\n    labels=summarized_data['label']\n    len(sentences),len(labels)\n    model_0=create_model()\n    input_ids=[]\n    attention_masks=[]\n\n    for sent in sentences:\n        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n        input_ids.append(dbert_inps['input_ids'])\n        attention_masks.append(dbert_inps['attention_mask'])\n    input_ids=np.asarray(input_ids)\n\n    attention_masks=np.array(attention_masks)\n    labels=np.array(labels)\n    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n    log_dir='dbert_model'\n\n    model_save_path='./kaggle/working/roberta-best-512-0-512-' + str(i) + '-4labels.h5'\n\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n    gpu_info = !nvidia-smi\n    gpu_info = '\\n'.join(gpu_info)\n    if gpu_info.find('failed') >= 0:\n        print('Not connected to a GPU')\n    else:\n        print(gpu_info)\n    history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n    pred_labels=[]\n\n    model_saved= create_model()\n    model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n    model_saved.load_weights('./kaggle/working/roberta-best-512-0-512-' + str(i) + '-4labels.h5')\n\n    for i in range(0,len(val_inp)):\n        pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n        pred_label = pred.argmax(axis=1)\n        pred_labels.append(pred_label)\n    accuracy=accuracy_score(val_label, pred_labels)\n    print(\"Accuracy: \"+str(accuracy))\n    total_accuracy=total_accuracy+accuracy\n  \n    weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n    print(\"Weighted F1: \"+ str(weighted_f1))\n    total_weighted_f1=total_weighted_f1+weighted_f1\n    micro_f1=f1_score(val_label,pred_labels, average='micro')\n    print(\"Micro F1: \"+ str(micro_f1))\n    total_micro_f1=total_micro_f1+micro_f1\n\n    weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n    print(\"Weighted Precision: \" + str(weighted_precision))\n    total_weighted_precision=total_weighted_precision+weighted_precision\n    micro_precision=precision_score(val_label, pred_labels, average='micro')\n    print(\"Micro Precision: \" + str(micro_precision))\n    total_micro_precision=total_micro_precision+micro_precision\n\n    weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n    print(\"Weighted Recall: \" + str(weighted_recall))\n    total_weighted_recall=total_weighted_recall+weighted_recall\n    micro_recall=recall_score(val_label, pred_labels, average='micro')\n    print(\"Micro Recall: \" + str(micro_recall))\n    total_micro_recall=total_micro_recall+micro_recall\n\n\nprint(\"Average Accuracy: \"+str(total_accuracy/5))\nprint(\"Average Weighted F1: \"+str(total_weighted_f1/5))\nprint(\"Average Micro F1: \"+str(total_micro_f1/5))\nprint(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\nprint(\"Average Micro Precision: \"+str(total_micro_precision/5))\nprint(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\nprint(\"Average Micro Recall: \"+str(total_micro_recall/5))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x9kO4eVwCHKg","outputId":"a3776971-f469-4ae7-dd89-06b3b2630cf1","execution":{"iopub.status.busy":"2023-06-10T16:34:03.926180Z","iopub.execute_input":"2023-06-10T16:34:03.926796Z","iopub.status.idle":"2023-06-10T16:55:12.223342Z","shell.execute_reply.started":"2023-06-10T16:34:03.926762Z","shell.execute_reply":"2023-06-10T16:55:12.222346Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23814c100a01450e83661b594d990797"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7175975dbbac447e9a7fcf0aafca046a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82fc7acfb5994856a024aac7edb9c921"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66dd966cd8ec45eba5c5933b12b8293d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:34:34 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    32W / 250W |  15857MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 87s 658ms/step - loss: 7.1413 - accuracy: 0.6467 - val_loss: 7.0550 - val_accuracy: 0.6200\nEpoch 2/5\n57/57 [==============================] - 35s 609ms/step - loss: 6.4844 - accuracy: 0.8733 - val_loss: 6.6363 - val_accuracy: 0.8200\nEpoch 3/5\n57/57 [==============================] - 36s 633ms/step - loss: 6.2387 - accuracy: 0.9067 - val_loss: 6.4342 - val_accuracy: 0.8400\nEpoch 4/5\n57/57 [==============================] - 33s 583ms/step - loss: 5.9921 - accuracy: 0.9244 - val_loss: 6.5825 - val_accuracy: 0.8200\nEpoch 5/5\n57/57 [==============================] - 33s 586ms/step - loss: 5.7875 - accuracy: 0.9378 - val_loss: 6.4682 - val_accuracy: 0.8000\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 129ms/step\n1/1 [==============================] - 0s 80ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 96ms/step\n1/1 [==============================] - 0s 95ms/step\n1/1 [==============================] - 0s 89ms/step\n1/1 [==============================] - 0s 98ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 84ms/step\nAccuracy: 0.84\nWeighted F1: 0.7985964912280703\nMicro F1: 0.8399999999999999\nWeighted Precision: 0.7712987012987014\nMicro Precision: 0.84\nWeighted Recall: 0.84\nMicro Recall: 0.84\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:38:48 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   59C    P0    38W / 250W |  15905MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 84s 656ms/step - loss: 7.0454 - accuracy: 0.6844 - val_loss: 6.8299 - val_accuracy: 0.7800\nEpoch 2/5\n57/57 [==============================] - 34s 604ms/step - loss: 6.4625 - accuracy: 0.8867 - val_loss: 6.4624 - val_accuracy: 0.8600\nEpoch 3/5\n57/57 [==============================] - 34s 606ms/step - loss: 6.1969 - accuracy: 0.9156 - val_loss: 6.3728 - val_accuracy: 0.8200\nEpoch 4/5\n57/57 [==============================] - 33s 584ms/step - loss: 5.9761 - accuracy: 0.9289 - val_loss: 6.6581 - val_accuracy: 0.8200\nEpoch 5/5\n57/57 [==============================] - 34s 598ms/step - loss: 5.7525 - accuracy: 0.9533 - val_loss: 6.0942 - val_accuracy: 0.9000\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 70ms/step\n1/1 [==============================] - 0s 69ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 86ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 69ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\nAccuracy: 0.9\nWeighted F1: 0.8902992776057792\nMicro F1: 0.9\nWeighted Precision: 0.883015873015873\nMicro Precision: 0.9\nWeighted Recall: 0.9\nMicro Recall: 0.9\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:42:56 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   59C    P0    38W / 250W |  15909MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 85s 691ms/step - loss: 6.9801 - accuracy: 0.7244 - val_loss: 6.8899 - val_accuracy: 0.7800\nEpoch 2/5\n57/57 [==============================] - 36s 627ms/step - loss: 6.4616 - accuracy: 0.8933 - val_loss: 6.5460 - val_accuracy: 0.8400\nEpoch 3/5\n57/57 [==============================] - 33s 582ms/step - loss: 6.1638 - accuracy: 0.9267 - val_loss: 6.4863 - val_accuracy: 0.8400\nEpoch 4/5\n57/57 [==============================] - 34s 604ms/step - loss: 5.9825 - accuracy: 0.9444 - val_loss: 6.2981 - val_accuracy: 0.8600\nEpoch 5/5\n57/57 [==============================] - 33s 582ms/step - loss: 5.7604 - accuracy: 0.9689 - val_loss: 6.2692 - val_accuracy: 0.8400\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 82ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 83ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 65ms/step\nAccuracy: 0.86\nWeighted F1: 0.8343517138599107\nMicro F1: 0.8599999999999999\nWeighted Precision: 0.8317279411764705\nMicro Precision: 0.86\nWeighted Recall: 0.86\nMicro Recall: 0.86\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:47:05 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   59C    P0    38W / 250W |  15913MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 85s 684ms/step - loss: 7.0163 - accuracy: 0.6844 - val_loss: 6.8055 - val_accuracy: 0.7600\nEpoch 2/5\n57/57 [==============================] - 34s 604ms/step - loss: 6.4161 - accuracy: 0.8911 - val_loss: 6.5586 - val_accuracy: 0.8200\nEpoch 3/5\n57/57 [==============================] - 36s 625ms/step - loss: 6.0979 - accuracy: 0.9289 - val_loss: 6.5256 - val_accuracy: 0.8600\nEpoch 4/5\n57/57 [==============================] - 35s 622ms/step - loss: 5.8229 - accuracy: 0.9667 - val_loss: 6.3071 - val_accuracy: 0.8800\nEpoch 5/5\n57/57 [==============================] - 34s 598ms/step - loss: 5.6811 - accuracy: 0.9689 - val_loss: 6.0276 - val_accuracy: 0.9000\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 71ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 66ms/step\nAccuracy: 0.9\nWeighted F1: 0.8902992776057792\nMicro F1: 0.9\nWeighted Precision: 0.883015873015873\nMicro Precision: 0.9\nWeighted Recall: 0.9\nMicro Recall: 0.9\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n                                thPoolingAndCrossAt               'input_2[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Sat Jun 10 16:51:19 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   59C    P0    38W / 250W |  15917MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n","output_type":"stream"},{"name":"stdout","text":"57/57 [==============================] - 85s 686ms/step - loss: 6.9260 - accuracy: 0.7467 - val_loss: 6.9318 - val_accuracy: 0.8000\nEpoch 2/5\n57/57 [==============================] - 34s 605ms/step - loss: 6.4226 - accuracy: 0.9044 - val_loss: 6.5254 - val_accuracy: 0.8400\nEpoch 3/5\n57/57 [==============================] - 36s 627ms/step - loss: 6.1532 - accuracy: 0.9289 - val_loss: 6.4010 - val_accuracy: 0.8600\nEpoch 4/5\n57/57 [==============================] - 33s 581ms/step - loss: 5.9380 - accuracy: 0.9578 - val_loss: 6.5600 - val_accuracy: 0.8000\nEpoch 5/5\n57/57 [==============================] - 33s 582ms/step - loss: 5.8174 - accuracy: 0.9622 - val_loss: 6.2520 - val_accuracy: 0.8200\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n                                thPoolingAndCrossAt               'input_4[0][0]']                \n                                tentions(last_hidde                                               \n                                n_state=(None, 512,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,878,533\nTrainable params: 109,878,533\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 108ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\nAccuracy: 0.86\nWeighted F1: 0.8343529411764706\nMicro F1: 0.8599999999999999\nWeighted Precision: 0.8318279569892474\nMicro Precision: 0.86\nWeighted Recall: 0.86\nMicro Recall: 0.86\nAverage Accuracy: 0.8720000000000001\nAverage Weighted F1: 0.849579940295202\nAverage Micro F1: 0.8719999999999999\nAverage Weighted Precision: 0.8401772690992331\nAverage Micro Precision: 0.8720000000000001\nAverage Weighted Recall: 0.8720000000000001\nAverage Micro Recall: 0.8720000000000001\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}