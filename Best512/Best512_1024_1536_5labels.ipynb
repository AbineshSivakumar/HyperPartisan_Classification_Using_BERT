{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T15:36:24.497251Z","iopub.status.busy":"2023-06-10T15:36:24.496536Z","iopub.status.idle":"2023-06-10T15:37:23.909817Z","shell.execute_reply":"2023-06-10T15:37:23.908366Z","shell.execute_reply.started":"2023-06-10T15:36:24.497087Z"},"trusted":true},"outputs":[],"source":["# %pip install transformers\n","# %pip install sentencepiece\n","# %pip install tensorflow\n","# %pip install stanza\n","# %pip install tensorflow-addons\n","# %pip install nltk\n","# %pip install datasets"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T15:36:24.488789Z","iopub.status.busy":"2023-06-10T15:36:24.488186Z","iopub.status.idle":"2023-06-10T15:36:24.494523Z","shell.execute_reply":"2023-06-10T15:36:24.493144Z","shell.execute_reply.started":"2023-06-10T15:36:24.488735Z"},"id":"K0rs0NoritMk","outputId":"92b77bac-3521-4e3b-cf37-6f33a0d5c9f1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.7.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T15:37:23.914250Z","iopub.status.busy":"2023-06-10T15:37:23.913897Z","iopub.status.idle":"2023-06-10T15:37:23.953387Z","shell.execute_reply":"2023-06-10T15:37:23.952069Z","shell.execute_reply.started":"2023-06-10T15:37:23.914215Z"},"id":"wYwcFK5gixXz","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import re\n","import unicodedata\n","import nltk\n","#from transformers import pipeline\n","from nltk.corpus import stopwords\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n","from tqdm import tqdm\n","import pickle\n","from sklearn.metrics import confusion_matrix,f1_score,classification_report\n","import matplotlib.pyplot as plt\n","import itertools\n","from sklearn.utils import shuffle\n","from tensorflow.keras import regularizers\n","#from transformers import *\n","from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n","import pandas as pd\n","from transformers import AutoTokenizer, TFAutoModel\n","import numpy as np\n","import gc\n","import math\n","import json\n","import stanza\n","from tensorflow.keras import *\n","import tensorflow as tf\n","from tensorflow.keras import *\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import classification_report\n","from transformers import TFRobertaModel,RobertaTokenizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.initializers import RandomUniform\n","\n","from numpy.random import seed\n","import random as python_random\n","import os\n","import sys\n","\n","np.random.seed(1)\n","python_random.seed(1)\n","tf.random.set_seed(1)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T15:37:23.957784Z","iopub.status.busy":"2023-06-10T15:37:23.957505Z","iopub.status.idle":"2023-06-10T15:37:24.058979Z","shell.execute_reply":"2023-06-10T15:37:24.057948Z","shell.execute_reply.started":"2023-06-10T15:37:23.957759Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/ubuntu/.cache/huggingface/token\n","Login successful\n"]}],"source":["# huggingface dataset access token\n","\n","from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T15:37:24.062599Z","iopub.status.busy":"2023-06-10T15:37:24.061664Z","iopub.status.idle":"2023-06-10T15:37:24.911087Z","shell.execute_reply":"2023-06-10T15:37:24.910168Z","shell.execute_reply.started":"2023-06-10T15:37:24.062563Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/maneshkarun___parquet/maneshkarun--median3k_10000s-a12d2bed8c5e7733/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n","100%|██████████| 1/1 [00:00<00:00, 700.80it/s]\n"]}],"source":["# importing datasets\n","\n","from datasets import load_dataset\n","# data = load_dataset(\"maneshkarun/median-3000\")\n","data = load_dataset(\"maneshkarun/median3k_10000s\")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T15:37:24.913470Z","iopub.status.busy":"2023-06-10T15:37:24.912775Z","iopub.status.idle":"2023-06-10T15:37:24.920594Z","shell.execute_reply":"2023-06-10T15:37:24.919592Z","shell.execute_reply.started":"2023-06-10T15:37:24.913419Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'title', 'hyperpartisan', 'url', 'published_at', 'bias', 'word_count', 'cleaned_data', 'pos_tagged'],\n","        num_rows: 10000\n","    })\n","})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T15:37:24.923024Z","iopub.status.busy":"2023-06-10T15:37:24.921854Z","iopub.status.idle":"2023-06-10T15:37:24.929143Z","shell.execute_reply":"2023-06-10T15:37:24.928211Z","shell.execute_reply.started":"2023-06-10T15:37:24.922991Z"},"trusted":true},"outputs":[],"source":["train_data = data['train']"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T15:37:24.931583Z","iopub.status.busy":"2023-06-10T15:37:24.930367Z","iopub.status.idle":"2023-06-10T15:37:24.952569Z","shell.execute_reply":"2023-06-10T15:37:24.951723Z","shell.execute_reply.started":"2023-06-10T15:37:24.931548Z"},"trusted":true},"outputs":[],"source":["train_text = train_data['cleaned_data']"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T15:37:24.955170Z","iopub.status.busy":"2023-06-10T15:37:24.954824Z","iopub.status.idle":"2023-06-10T15:37:25.270353Z","shell.execute_reply":"2023-06-10T15:37:25.269257Z","shell.execute_reply.started":"2023-06-10T15:37:24.955138Z"},"id":"2ZinwFiui-A3","outputId":"1a4d3851-73a3-444b-a286-ec608b7c3197","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 0, 0, 0, 4, 4, 0, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 3, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 1, 0, 4, 3, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 1, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 0, 4, 4, 2, 2, 4, 3, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 1, 4, 4, 4, 4, 2, 2, 2, 2, 3, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 3, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 0, 4, 0, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 1, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 0, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 0, 4, 0, 4, 2, 4, 4, 4, 0, 4, 2, 4, 3, 4, 4, 2, 4, 2, 2, 2, 3, 4, 2, 4, 2, 4, 4, 4, 4, 2, 0, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 0, 4, 4, 4, 0, 2, 0, 2, 4, 2, 2, 1, 2, 4, 2, 2, 2, 4, 3, 1, 3, 1, 2, 2, 4, 0, 4, 4, 4, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 0, 2, 2, 0, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 3, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 0, 4, 4, 2, 4, 3, 2, 4, 2, 4, 4, 3, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 0, 4, 1, 4, 2, 4, 0, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 2, 2, 2, 2, 4, 1, 4, 4, 2, 2, 4, 2, 0, 4, 3, 2, 3, 0, 2, 2, 2, 2, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 2, 2, 1, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 0, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 0, 4, 2, 4, 1, 2, 2, 4, 4, 4, 2, 4, 4, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 0, 4, 0, 1, 4, 2, 4, 2, 2, 0, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 2, 4, 4, 0, 2, 2, 2, 2, 4, 2, 4, 2, 4, 1, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 3, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 2, 3, 4, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 2, 2, 0, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 3, 3, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 3, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 0, 4, 1, 3, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 2, 1, 2, 4, 4, 4, 2, 3, 2, 4, 0, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 1, 4, 3, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 0, 2, 4, 4, 4, 2, 4, 2, 4, 0, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 1, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 3, 4, 4, 4, 4, 2, 2, 2, 4, 1, 2, 4, 4, 4, 2, 2, 4, 0, 3, 4, 4, 4, 2, 4, 4, 4, 3, 3, 4, 4, 4, 2, 4, 3, 2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 3, 2, 4, 4, 1, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 1, 4, 3, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 0, 4, 2, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 3, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 2, 0, 2, 4, 4, 4, 2, 4, 4, 2, 3, 4, 4, 2, 2, 4, 2, 2, 0, 2, 2, 2, 4, 1, 4, 4, 2, 2, 2, 4, 2, 3, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 3, 2, 2, 2, 4, 4, 4, 4, 2, 4, 3, 4, 4, 1, 4, 2, 2, 4, 2, 4, 4, 0, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 0, 4, 4, 4, 0, 2, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 1, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 0, 2, 2, 4, 4, 2, 2, 2, 3, 2, 2, 4, 4, 4, 4, 2, 4, 4, 0, 2, 4, 2, 4, 4, 2, 4, 1, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 3, 0, 4, 4, 2, 0, 2, 2, 3, 4, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 2, 2, 3, 3, 4, 4, 1, 4, 2, 4, 2, 4, 3, 4, 4, 4, 2, 2, 4, 4, 0, 3, 2, 2, 4, 4, 2, 4, 2, 1, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 1, 2, 2, 2, 4, 3, 2, 4, 2, 4, 4, 2, 4, 2, 3, 1, 4, 4, 2, 2, 0, 4, 4, 2, 4, 4, 3, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 0, 4, 2, 0, 2, 2, 4, 4, 0, 0, 4, 2, 2, 2, 1, 0, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 1, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 2, 2, 2, 4, 2, 4, 1, 4, 2, 4, 4, 4, 2, 3, 2, 2, 1, 2, 4, 4, 2, 3, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 3, 2, 2, 4, 2, 2, 4, 4, 2, 4, 3, 4, 2, 0, 4, 4, 4, 2, 2, 4, 4, 2, 2, 3, 3, 4, 4, 4, 4, 4, 3, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 0, 1, 4, 2, 4, 2, 2, 1, 2, 4, 2, 4, 2, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 2, 1, 2, 4, 4, 2, 0, 4, 4, 4, 2, 4, 2, 4, 2, 0, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 3, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 1, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 0, 0, 2, 4, 4, 4, 2, 2, 4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 1, 4, 2, 4, 2, 0, 2, 2, 2, 4, 2, 3, 4, 3, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 3, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 0, 4, 2, 2, 1, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 2, 2, 3, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 3, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 0, 2, 4, 3, 2, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 1, 2, 2, 3, 2, 2, 2, 2, 2, 1, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 0, 2, 4, 4, 2, 4, 4, 2, 4, 2, 3, 4, 2, 2, 2, 4, 0, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 3, 4, 4, 4, 2, 4, 3, 4, 4, 1, 4, 4, 0, 2, 2, 4, 2, 4, 2, 3, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 1, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 0, 0, 2, 0, 0, 0, 0, 4, 4, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 2, 4, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 3, 2, 2, 2, 2, 4, 4, 0, 4, 2, 2, 0, 3, 2, 2, 2, 2, 1, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 0, 0, 4, 4, 4, 4, 3, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 4, 3, 4, 4, 4, 4, 2, 2, 2, 2, 0, 2, 2, 2, 2, 4, 3, 4, 2, 2, 4, 2, 4, 2, 4, 1, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 1, 2, 0, 2, 2, 4, 2, 2, 2, 2, 2, 3, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 0, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 0, 4, 2, 0, 2, 4, 2, 4, 4, 4, 2, 0, 2, 2, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 2, 2, 4, 3, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 4, 3, 2, 2, 2, 2, 2, 4, 4, 2, 2, 0, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 3, 4, 3, 1, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 0, 2, 3, 4, 2, 3, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 1, 2, 2, 2, 4, 2, 4, 1, 3, 2, 4, 2, 4, 2, 4, 1, 2, 4, 4, 4, 3, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 3, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 0, 2, 2, 4, 4, 0, 4, 4, 4, 3, 2, 4, 4, 0, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 0, 4, 0, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4, 0, 2, 4, 2, 2, 2, 4, 4, 3, 4, 4, 2, 2, 2, 4, 4, 2, 4, 3, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 1, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 0, 1, 3, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 1, 1, 2, 1, 1, 2, 2, 0, 1, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 3, 4, 2, 4, 2, 2, 2, 3, 4, 4, 4, 0, 2, 4, 4, 0, 2, 4, 0, 4, 0, 2, 4, 2, 4, 4, 4, 0, 2, 0, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 0, 2, 1, 4, 2, 2, 4, 4, 1, 4, 4, 0, 2, 2, 4, 4, 2, 0, 2, 2, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 2, 4, 4, 4, 4, 0, 2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 1, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 0, 4, 4, 4, 2, 4, 2, 2, 3, 2, 4, 2, 4, 4, 4, 2, 4, 1, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 3, 4, 4, 3, 3, 4, 4, 4, 2, 4, 2, 4, 2, 0, 4, 2, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 3, 2, 4, 2, 2, 0, 1, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 3, 2, 2, 0, 4, 4, 0, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 1, 2, 4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 1, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 0, 2, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 2, 0, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 2, 3, 4, 2, 4, 4, 3, 4, 2, 2, 2, 4, 4, 4, 0, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 3, 4, 4, 2, 4, 2, 2, 3, 4, 4, 4, 3, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 0, 2, 2, 2, 2, 4, 4, 4, 0, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 0, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 0, 4, 0, 0, 4, 0, 0, 0, 0, 4, 4, 0, 4, 0, 2, 0, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 2, 2, 1, 4, 2, 4, 2, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 1, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 0, 4, 4, 2, 4, 2, 2, 2, 0, 2, 4, 3, 4, 4, 2, 4, 0, 4, 4, 2, 2, 4, 3, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 3, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 2, 3, 4, 2, 4, 2, 2, 1, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 0, 4, 2, 4, 2, 2, 4, 2, 2, 2, 1, 2, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 1, 4, 2, 4, 2, 2, 2, 4, 1, 4, 4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 3, 4, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 2, 0, 4, 4, 0, 3, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 3, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 3, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 4, 4, 0, 4, 4, 2, 0, 2, 0, 0, 0, 0, 0, 4, 0, 2, 2, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 0, 4, 4, 4, 2, 0, 1, 4, 2, 4, 1, 2, 4, 3, 4, 0, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 0, 4, 4, 4, 4, 2, 4, 2, 2, 0, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 0, 4, 2, 2, 4, 2, 4, 4, 3, 4, 2, 2, 3, 3, 4, 4, 2, 4, 4, 4, 4, 2, 2, 3, 2, 4, 4, 4, 2, 4, 2, 4, 4, 0, 2, 4, 4, 2, 4, 1, 4, 2, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 0, 2, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 0, 4, 1, 4, 4, 2, 4, 4, 4, 2, 2, 3, 2, 2, 2, 4, 2, 2, 2, 4, 3, 4, 3, 4, 2, 0, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 2, 3, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 1, 2, 2, 2, 2, 2, 2, 4, 2, 4, 3, 2, 2, 4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 2, 4, 1, 2, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 0, 4, 4, 0, 3, 2, 3, 4, 4, 4, 2, 2, 4, 2, 3, 2, 4, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 3, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 3, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 0, 2, 2, 4, 4, 4, 4, 4, 1, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 3, 4, 4, 0, 2, 4, 4, 2, 3, 2, 4, 2, 4, 4, 4, 0, 2, 2, 4, 2, 3, 4, 4, 4, 4, 3, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 3, 4, 4, 2, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 2, 4, 3, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 3, 4, 2, 4, 4, 2, 2, 4, 1, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 1, 2, 0, 4, 4, 4, 4, 4, 4, 4, 2, 0, 2, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 3, 2, 2, 2, 4, 4, 2, 4, 4, 4, 0, 2, 2, 4, 2, 4, 2, 2, 4, 0, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 3, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 4, 4, 1, 2, 4, 4, 4, 2, 2, 0, 2, 4, 4, 2, 0, 4, 2, 0, 2, 3, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 3, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 3, 2, 4, 1, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 3, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 0, 0, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 1, 0, 2, 4, 2, 3, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 0, 2, 2, 2, 0, 4, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 0, 2, 4, 2, 2, 2, 2, 2, 4, 3, 2, 4, 1, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 0, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 2, 0, 2, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 3, 1, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 0, 2, 4, 2, 2, 2, 2, 2, 2, 0, 4, 2, 2, 4, 2, 4, 1, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 3, 0, 0, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 2, 4, 2, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 0, 4, 2, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 0, 2, 4, 1, 4, 1, 4, 4, 2, 4, 0, 2, 4, 4, 4, 2, 4, 0, 1, 2, 2, 2, 4, 0, 2, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 2, 2, 0, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 1, 2, 4, 4, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4, 2, 2, 2, 1, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 1, 2, 4, 4, 4, 1, 2, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 0, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 2, 1, 4, 2, 4, 2, 3, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 2, 2, 0, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 0, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 0, 4, 2, 2, 2, 2, 4, 2, 2, 3, 4, 0, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 3, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 1, 1, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 3, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 3, 2, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 1, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 4, 0, 2, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 3, 4, 2, 2, 0, 4, 0, 2, 4, 1, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 0, 1, 0, 2, 0, 2, 0, 4, 0, 0, 2, 0, 0, 0, 4, 4, 0, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 0, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 0, 2, 4, 4, 2, 4, 2, 4, 3, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 1, 4, 4, 4, 4, 0, 4, 3, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 3, 4, 4, 4, 4, 4, 1, 2, 4, 4, 2, 1, 1, 4, 4, 2, 2, 2, 2, 0, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 1, 2, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 4, 2, 2, 4, 1, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 1, 2, 2, 4, 4, 2, 4, 4, 2, 4, 2, 0, 4, 3, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 3, 4, 4, 2, 4, 2, 4, 4, 0, 1, 4, 4, 4, 2, 2, 4, 3, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 1, 2, 2, 4, 3, 4, 4, 3, 2, 4, 2, 2, 4, 2, 3, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 3, 4, 2, 2, 0, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 3, 2, 4, 2, 2, 3, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 1, 2, 4, 2, 1, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 2, 3, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 0, 4, 4, 2, 2, 2, 4, 4, 4, 0, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 4, 0, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 3, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 1, 2, 2, 3, 2, 4, 2, 2, 1, 4, 2, 2, 4, 3, 4, 1, 4, 4, 2, 2, 3, 0, 4, 0, 2, 0, 1, 2, 0, 0, 2, 0, 0, 4, 2, 2, 4, 2, 0, 4, 0, 0, 0, 0, 0, 2, 2, 4, 4, 2, 2, 4, 0, 2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 2, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 0, 4, 4, 4, 4, 0, 2, 4, 2, 2, 2, 4, 4, 3, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 3, 3, 2, 2, 3, 4, 4, 4, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 4, 0, 4, 4, 2, 2, 2, 0, 0, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 2, 4, 2, 1, 2, 4, 2, 4, 2, 4, 2, 2, 2, 0, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 4, 0, 4, 2, 4, 2, 4, 4, 2, 0, 2, 4, 4, 2, 4, 2, 0, 2, 0, 4, 3, 4, 2, 4, 4, 4, 2, 2, 2, 1, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 3, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 0, 4, 2, 2, 4, 2, 2, 4, 2, 2, 0, 0, 4, 0, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 1, 4, 2, 4, 4, 4, 2, 2, 2, 3, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 1, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 0, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 0, 4, 4, 4, 2, 4, 4, 2, 3, 0, 4, 4, 4, 2, 0, 4, 4, 0, 4, 1, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 3, 4, 2, 0, 2, 2, 1, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 1, 4, 2, 4, 2, 4, 2, 2, 4, 2, 2, 3, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 1, 2, 2, 4, 2, 4, 2, 1, 2, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 0, 1, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 1, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 3, 2, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 0, 2, 4, 2, 2, 3, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 0, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 0, 2, 2, 4, 4, 4, 4, 4, 1, 4, 1, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 0, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 3, 4, 4, 2, 4, 1, 4, 4, 2, 2, 0, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 4, 3, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 0, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 3, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 3, 2, 4, 2, 4, 2, 2, 2, 4, 2, 3, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 1, 2, 2, 1, 2, 2, 4, 1, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 0, 4, 3, 4, 2, 4, 2, 2, 4, 2, 3, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 3, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 0, 2, 0, 3, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 1, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 0, 3, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 0, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 3, 2, 2, 4, 2, 4, 4, 4, 4, 4, 0, 2, 2, 2, 4, 2, 3, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 3, 4, 2, 4, 4, 2, 0, 2, 4, 4, 4, 2, 0, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 4, 2, 4, 0, 2, 4, 4, 2, 4, 2, 2, 4, 0, 4, 4, 2, 2, 2, 2, 4, 0, 0, 2, 4, 4, 4, 2, 4, 4, 1, 0, 4, 4, 4, 4, 2, 1, 2, 4, 4, 2, 0, 2, 4, 4, 2, 3, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 0, 4, 3, 4, 4, 2, 4, 2, 4, 4, 3, 4, 2, 2, 4, 4, 2, 1, 0, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 0, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 2, 3, 4, 4, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 1, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 0, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 0, 4, 2, 1, 4, 2, 4, 2, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 1, 2, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 4, 0, 4, 2, 0, 4, 2, 2, 2, 4, 2, 4, 0, 1, 4, 4, 1, 0, 4, 2, 0, 2, 4, 4, 4, 2, 2, 1, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 2, 3, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 2, 4, 2, 3, 4, 4, 4, 4, 0, 2, 4, 0, 1, 4, 4, 4, 3, 4, 2, 2, 4, 2, 0, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 4, 4, 0, 4, 4, 4, 0, 1, 4, 3, 4, 2, 2, 2, 3, 3, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 0, 2, 2, 4, 4, 0, 0, 4, 0, 4, 2, 1, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 0, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 0, 4, 2, 4, 3, 3, 2, 4, 4, 2, 0, 2, 4, 2, 4, 4, 3, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 0, 4, 0, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 1, 2, 2, 4, 4, 4, 2, 4, 0, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 1, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 0, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 3, 2, 2, 4, 2, 2, 0, 2, 4, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 2, 4, 0, 4, 4, 2, 4, 4, 0, 0, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 1, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 0, 4, 2, 2, 4, 4, 4, 2, 0, 4, 4, 4, 2, 3, 4, 4, 2, 2, 2, 2, 4, 1, 2, 4, 2, 0, 4, 2, 4, 2, 2, 4, 2, 0, 4, 4, 4, 2, 4, 2, 0, 4, 2, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 1, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 0, 1, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 1, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 0, 0, 4, 4, 4, 2, 3, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 0, 0, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 0, 4, 4, 3, 0, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 4, 0, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 1, 4, 4, 2, 3, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 4, 0, 2, 0, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 0, 2, 4, 4, 4, 2, 2, 4, 4, 4, 3, 4, 4, 4, 0, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 1, 4, 2, 4, 4, 2, 0, 4, 2, 4, 2, 2, 2, 2, 4, 4, 0, 2, 4, 4, 1, 0, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 0, 2, 2, 2, 2, 2, 2, 4, 2, 3, 4, 2, 2, 0, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 3, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 2, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 3, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 0, 4, 4, 4, 4, 0, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 3, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 0, 2, 4, 0, 4, 4, 4, 2, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 2, 2, 2, 2, 4, 4, 4, 2, 0, 4, 0, 4, 2, 4, 2, 2, 3, 2, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 3, 2, 4, 4, 2, 0, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 0, 4, 0, 4, 0, 4, 1, 2, 2, 0, 4, 2, 2, 2, 4, 2, 1, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 0, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 3, 4, 4, 3, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 0, 2, 4, 2, 4, 4, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 3, 4, 4, 4, 0, 4, 4, 2, 4, 2, 0, 0, 1, 4, 4, 0, 3, 4, 2, 2, 4, 4, 4, 2, 3, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 0, 2, 4, 2, 2, 2, 3, 2, 4, 2, 0, 4, 4, 2, 4, 4, 3, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 0, 2, 4, 2, 0, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 2, 3, 4, 4, 3, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 0, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 3, 0, 4, 4, 0, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 3, 0, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 4, 3, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 0, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 0, 4, 4, 2, 2, 0, 4, 2, 4, 4, 4, 0, 0, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 3, 4, 4, 2, 2, 4, 1, 2, 2, 4, 4, 0, 4, 0, 4, 4, 4, 0, 1, 3, 4, 4, 4, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 0, 4, 4, 2, 2, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 1, 4, 4, 2, 4, 3, 4, 4, 2, 0, 0, 4, 4, 0, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 1, 4, 0, 4, 2, 1, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 1, 4, 2, 4, 3, 2, 4, 4, 4, 4, 2, 1, 2, 0, 4, 4, 0, 2, 4, 4, 2, 2, 4, 2, 4, 4, 0, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 0, 0, 4, 0, 0, 0, 4, 0, 0, 2, 4, 4, 1, 4, 4, 2, 0, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 3, 4, 4, 2, 4, 4, 2, 4, 4, 4, 0, 2, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 0, 0, 4, 4, 4, 1, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 1, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 0, 4, 0, 2, 0, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 0, 0, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 0, 2, 4, 3, 0, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 0, 4, 2, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 0, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 1, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 2, 2, 4, 0, 4, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 2, 0, 2, 2, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 0, 4, 3, 4, 4, 4, 4, 2, 4, 4, 4, 4, 0, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 0, 2, 1, 4, 4, 4, 4, 4, 4, 4, 2, 0, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 0, 4, 3, 2, 2, 4, 2, 2, 4, 2, 3, 4, 4, 4, 4, 2, 4, 4, 2, 3, 4, 3, 4, 2, 4, 2, 4, 4, 4, 4, 3, 4, 4, 0, 2, 4, 4, 4, 4, 0, 2, 0, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 0, 4, 2, 4, 2, 4, 0, 2, 4, 4, 4, 4, 4, 4, 0, 4, 3, 0, 4, 4, 4, 0, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 3, 4, 4, 4, 4, 0, 0, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 4, 3, 4, 4, 1, 0, 4, 2, 2, 0, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 0, 4, 3, 2, 0, 4, 4, 4, 4, 3, 4, 2, 2, 4, 3, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 3, 4, 2, 4, 3, 4, 4, 3, 4, 4, 0, 4, 4, 1, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 0, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 0, 2, 4, 4, 4, 4, 4, 0, 0, 4, 1, 4, 4, 4, 3, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 3, 2, 2, 0, 4, 4, 4, 0, 4, 2, 2, 2, 4, 4, 4, 2, 1, 4, 4, 4, 0, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 0, 4, 2, 2, 4]\n","10000\n","Average Length 512.0\n","Found 10000 texts.\n"]}],"source":["texts = []\n","labels = []\n","\n","count=0\n","\n","for record in train_data:\n","\n","        count=count+1\n","        new_sen = record['cleaned_data'].split()\n","\n","        if len(new_sen) >= 1536:\n","          new_sen = new_sen[1024:1536]\n","        \n","        elif len(new_sen) < 512:\n","          new_sen = new_sen[0:len(new_sen)]\n","        \n","        else:\n","          new_sen = new_sen[-512:]\n","          \n","        new_sen = ' '.join(new_sen)\n","\n","        texts.append(new_sen)\n","        labels.append(record['bias'])\n","   \n","len_list = [len(ele.split()) for ele in texts]\n","\n","print(labels)\n","print(len(labels))\n","\n","res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n","\n","print(\"Average Length %s\" % res) \n","print('Found %s texts.' % len(texts))"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T15:37:25.272275Z","iopub.status.busy":"2023-06-10T15:37:25.271905Z","iopub.status.idle":"2023-06-10T15:37:25.284204Z","shell.execute_reply":"2023-06-10T15:37:25.283080Z","shell.execute_reply.started":"2023-06-10T15:37:25.272241Z"},"id":"LprCHRM2aWb8","outputId":"3377113e-6825-4977-bf5d-ac9c08887b56","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                   text  label\n","0     95 56 93 Jan 1 11234 93 1858 104 1737 93 67 11...      0\n","1     91 101 126 Apr 1 11162 96 1600 104 1871 93 69 ...      0\n","2     96 56 114 Jul 1 9752 95 1863 112 1935 95 43 96...      0\n","3     96 47 92 Aug 1 9637 98 2113 103 1815 96 56 114...      0\n","4     Clinton So where is the investigation AG seanh...      4\n","...                                                 ...    ...\n","9995  to that is yes they check it as a rape and so ...      0\n","9996  will come as little surprise that the Maggi br...      4\n","9997  ballerina with having closed more than 2 billi...      2\n","9998  ballerina with having closed more than 2 billi...      2\n","9999  1 the day after he discovered that the Globe c...      4\n","\n","[10000 rows x 2 columns]\n"]}],"source":["summarized_data = pd.DataFrame(texts,\n","               columns =['text'])\n","summarized_data['label'] = labels\n","print(summarized_data)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T15:37:25.288101Z","iopub.status.busy":"2023-06-10T15:37:25.287737Z","iopub.status.idle":"2023-06-10T15:37:25.296948Z","shell.execute_reply":"2023-06-10T15:37:25.295866Z","shell.execute_reply.started":"2023-06-10T15:37:25.288067Z"},"id":"VoY1gHZoaZmG","trusted":true},"outputs":[],"source":["def create_model():\n","    inps = Input(shape = (max_len,), dtype='int64')\n","    masks= Input(shape = (max_len,), dtype='int64')\n","    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n","    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n","    dropout_0= Dropout(0.5)(dense_0)\n","    pred = Dense(5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n","    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n","    print(model.summary())\n","    return model   "]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T15:37:25.298941Z","iopub.status.busy":"2023-06-10T15:37:25.298600Z","iopub.status.idle":"2023-06-10T15:58:20.430213Z","shell.execute_reply":"2023-06-10T15:58:20.428980Z","shell.execute_reply.started":"2023-06-10T15:37:25.298910Z"},"id":"x9kO4eVwCHKg","outputId":"a3776971-f469-4ae7-dd89-06b3b2630cf1","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/home/ubuntu/miniconda/envs/nlp/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["Sun Jun 25 14:36:02 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A10          On   | 00000000:06:00.0 Off |                    0 |\n","|  0%   48C    P0    60W / 150W |  21548MiB / 23028MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A     72273      C   ...conda/envs/nlp/bin/python    21546MiB |\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/home/ubuntu/miniconda/envs/nlp/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","1125/1125 [==============================] - 349s 305ms/step - loss: 5.4210 - accuracy: 0.8666 - val_loss: 4.2172 - val_accuracy: 0.8900\n","Epoch 2/5\n","1125/1125 [==============================] - 344s 306ms/step - loss: 3.3766 - accuracy: 0.9171 - val_loss: 2.6725 - val_accuracy: 0.9240\n","Epoch 3/5\n","1125/1125 [==============================] - 344s 306ms/step - loss: 2.1027 - accuracy: 0.9403 - val_loss: 1.7666 - val_accuracy: 0.9250\n","Epoch 4/5\n","1125/1125 [==============================] - 344s 306ms/step - loss: 1.3217 - accuracy: 0.9596 - val_loss: 1.2884 - val_accuracy: 0.9260\n","Epoch 5/5\n","1125/1125 [==============================] - 344s 306ms/step - loss: 0.8130 - accuracy: 0.9800 - val_loss: 0.9848 - val_accuracy: 0.8990\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Accuracy: 0.926\n","Weighted F1: 0.9201472503405896\n","Micro F1: 0.926\n","Weighted Precision: 0.9236987079869406\n","Micro Precision: 0.926\n","Weighted Recall: 0.926\n","Micro Recall: 0.926\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","total_accuracy=0\n","total_weighted_f1=0\n","total_micro_f1=0\n","total_weighted_precision=0\n","total_micro_precision=0\n","total_weighted_recall=0\n","total_micro_recall=0\n","\n","for i in range(1):\n","  gc.collect()\n","  tf.keras.backend.clear_session()\n","  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n","  max_len=512\n","  sentences=summarized_data['text']\n","  labels=summarized_data['label']\n","  len(sentences),len(labels)\n","  model_0=create_model()\n","  input_ids=[]\n","  attention_masks=[]\n","\n","  for sent in sentences:\n","    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n","    input_ids.append(dbert_inps['input_ids'])\n","    attention_masks.append(dbert_inps['attention_mask'])\n","  input_ids=np.asarray(input_ids)\n","\n","  attention_masks=np.array(attention_masks)\n","  labels=np.array(labels)\n","  train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n","  log_dir='dbert_model'\n","\n","  model_save_path='/home/ubuntu/HyperPartisan_Classification_Using_BERT/Best512'+str(i)+'-5labels.h5'\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n","  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n","  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","  gpu_info = !nvidia-smi\n","  gpu_info = '\\n'.join(gpu_info)\n","  if gpu_info.find('failed') >= 0:\n","    print('Not connected to a GPU')\n","  else:\n","    print(gpu_info)\n","  history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n","  pred_labels=[]\n","\n","  model_saved= create_model()\n","  model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","  model_saved.load_weights('/home/ubuntu/HyperPartisan_Classification_Using_BERT/Best512'+str(i)+'-5labels.h5')\n","\n","  for i in range(0,len(val_inp)):\n","    pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n","    pred_label = pred.argmax(axis=1)\n","    pred_labels.append(pred_label)\n","  accuracy=accuracy_score(val_label, pred_labels)\n","  print(\"Accuracy: \"+str(accuracy))\n","  total_accuracy=total_accuracy+accuracy\n","  \n","  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n","  print(\"Weighted F1: \"+ str(weighted_f1))\n","  total_weighted_f1=total_weighted_f1+weighted_f1\n","  micro_f1=f1_score(val_label,pred_labels, average='micro')\n","  print(\"Micro F1: \"+ str(micro_f1))\n","  total_micro_f1=total_micro_f1+micro_f1\n","\n","  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n","  print(\"Weighted Precision: \" + str(weighted_precision))\n","  total_weighted_precision=total_weighted_precision+weighted_precision\n","  micro_precision=precision_score(val_label, pred_labels, average='micro')\n","  print(\"Micro Precision: \" + str(micro_precision))\n","  total_micro_precision=total_micro_precision+micro_precision\n","\n","  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n","  print(\"Weighted Recall: \" + str(weighted_recall))\n","  total_weighted_recall=total_weighted_recall+weighted_recall\n","  micro_recall=recall_score(val_label, pred_labels, average='micro')\n","  print(\"Micro Recall: \" + str(micro_recall))\n","  total_micro_recall=total_micro_recall+micro_recall"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Accuracy: 0.926\n","Average Weighted F1: 0.9201472503405896\n","Average Micro F1: 0.926\n","Average Weighted Precision: 0.9236987079869406\n","Average Micro Precision: 0.926\n","Average Weighted Recall: 0.926\n","Average Micro Recall: 0.926\n"]}],"source":["print(\"Average Accuracy: \"+str(total_accuracy/1))\n","print(\"Average Weighted F1: \"+str(total_weighted_f1/1))\n","print(\"Average Micro F1: \"+str(total_micro_f1/1))\n","print(\"Average Weighted Precision: \"+str(total_weighted_precision/1))\n","print(\"Average Micro Precision: \"+str(total_micro_precision/1))\n","print(\"Average Weighted Recall: \"+str(total_weighted_recall/1))\n","print(\"Average Micro Recall: \"+str(total_micro_recall/1))"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Best-512_0:512_15labels.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":4}
