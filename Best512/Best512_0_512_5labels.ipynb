{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install transformers\n",
        "# %pip install sentencepiece\n",
        "# %pip install tensorflow\n",
        "# %pip install stanza\n",
        "# %pip install tensorflow-addons\n",
        "# %pip install nltk\n",
        "# %pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0rs0NoritMk",
        "outputId": "92b77bac-3521-4e3b-cf37-6f33a0d5c9f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wYwcFK5gixXz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda/envs/nlp1/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "#from transformers import pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import regularizers\n",
        "#from transformers import *\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import numpy as np\n",
        "import gc\n",
        "import math\n",
        "import json\n",
        "import stanza\n",
        "from tensorflow.keras import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import TFRobertaModel,RobertaTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "\n",
        "from numpy.random import seed\n",
        "import random as python_random\n",
        "import os\n",
        "import sys\n",
        "\n",
        "np.random.seed(1)\n",
        "python_random.seed(1)\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# huggingface dataset access token\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset None/None to /home/ubuntu/.cache/huggingface/datasets/maneshkarun___parquet/maneshkarun--median3k_10000s-a12d2bed8c5e7733/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 5629.94it/s]\n",
            "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1276.42it/s]\n",
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset parquet downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/maneshkarun___parquet/maneshkarun--median3k_10000s-a12d2bed8c5e7733/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 583.11it/s]\n"
          ]
        }
      ],
      "source": [
        "# importing datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "# data = load_dataset(\"maneshkarun/median-3000\")\n",
        "data = load_dataset(\"maneshkarun/median3k_10000s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = data['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_text = train_data['cleaned_data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZinwFiui-A3",
        "outputId": "1a4d3851-73a3-444b-a286-ec608b7c3197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 4, 4, 0, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 3, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 1, 0, 4, 3, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 1, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 0, 4, 4, 2, 2, 4, 3, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 1, 4, 4, 4, 4, 2, 2, 2, 2, 3, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 3, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 0, 4, 0, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 1, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 0, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 0, 4, 0, 4, 2, 4, 4, 4, 0, 4, 2, 4, 3, 4, 4, 2, 4, 2, 2, 2, 3, 4, 2, 4, 2, 4, 4, 4, 4, 2, 0, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 0, 4, 4, 4, 0, 2, 0, 2, 4, 2, 2, 1, 2, 4, 2, 2, 2, 4, 3, 1, 3, 1, 2, 2, 4, 0, 4, 4, 4, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 0, 2, 2, 0, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 3, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 0, 4, 4, 2, 4, 3, 2, 4, 2, 4, 4, 3, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 0, 4, 1, 4, 2, 4, 0, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 2, 2, 2, 2, 4, 1, 4, 4, 2, 2, 4, 2, 0, 4, 3, 2, 3, 0, 2, 2, 2, 2, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 2, 2, 1, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 0, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 0, 4, 2, 4, 1, 2, 2, 4, 4, 4, 2, 4, 4, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 0, 4, 0, 1, 4, 2, 4, 2, 2, 0, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 2, 4, 4, 0, 2, 2, 2, 2, 4, 2, 4, 2, 4, 1, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 3, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 2, 3, 4, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 2, 2, 0, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 3, 3, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 3, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 0, 4, 1, 3, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 2, 1, 2, 4, 4, 4, 2, 3, 2, 4, 0, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 1, 4, 3, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 0, 2, 4, 4, 4, 2, 4, 2, 4, 0, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 1, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 3, 4, 4, 4, 4, 2, 2, 2, 4, 1, 2, 4, 4, 4, 2, 2, 4, 0, 3, 4, 4, 4, 2, 4, 4, 4, 3, 3, 4, 4, 4, 2, 4, 3, 2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 3, 2, 4, 4, 1, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 1, 4, 3, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 0, 4, 2, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 3, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 2, 0, 2, 4, 4, 4, 2, 4, 4, 2, 3, 4, 4, 2, 2, 4, 2, 2, 0, 2, 2, 2, 4, 1, 4, 4, 2, 2, 2, 4, 2, 3, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 3, 2, 2, 2, 4, 4, 4, 4, 2, 4, 3, 4, 4, 1, 4, 2, 2, 4, 2, 4, 4, 0, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 0, 4, 4, 4, 0, 2, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 1, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 0, 2, 2, 4, 4, 2, 2, 2, 3, 2, 2, 4, 4, 4, 4, 2, 4, 4, 0, 2, 4, 2, 4, 4, 2, 4, 1, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 3, 0, 4, 4, 2, 0, 2, 2, 3, 4, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 2, 2, 3, 3, 4, 4, 1, 4, 2, 4, 2, 4, 3, 4, 4, 4, 2, 2, 4, 4, 0, 3, 2, 2, 4, 4, 2, 4, 2, 1, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 1, 2, 2, 2, 4, 3, 2, 4, 2, 4, 4, 2, 4, 2, 3, 1, 4, 4, 2, 2, 0, 4, 4, 2, 4, 4, 3, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 0, 4, 2, 0, 2, 2, 4, 4, 0, 0, 4, 2, 2, 2, 1, 0, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 1, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 2, 2, 2, 4, 2, 4, 1, 4, 2, 4, 4, 4, 2, 3, 2, 2, 1, 2, 4, 4, 2, 3, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 3, 2, 2, 4, 2, 2, 4, 4, 2, 4, 3, 4, 2, 0, 4, 4, 4, 2, 2, 4, 4, 2, 2, 3, 3, 4, 4, 4, 4, 4, 3, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 0, 1, 4, 2, 4, 2, 2, 1, 2, 4, 2, 4, 2, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 2, 1, 2, 4, 4, 2, 0, 4, 4, 4, 2, 4, 2, 4, 2, 0, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 3, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 1, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 0, 0, 2, 4, 4, 4, 2, 2, 4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 1, 4, 2, 4, 2, 0, 2, 2, 2, 4, 2, 3, 4, 3, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 3, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 0, 4, 2, 2, 1, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 2, 2, 3, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 3, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 0, 2, 4, 3, 2, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 1, 2, 2, 3, 2, 2, 2, 2, 2, 1, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 0, 2, 4, 4, 2, 4, 4, 2, 4, 2, 3, 4, 2, 2, 2, 4, 0, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 3, 4, 4, 4, 2, 4, 3, 4, 4, 1, 4, 4, 0, 2, 2, 4, 2, 4, 2, 3, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 1, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 0, 0, 2, 0, 0, 0, 0, 4, 4, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 2, 4, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 3, 2, 2, 2, 2, 4, 4, 0, 4, 2, 2, 0, 3, 2, 2, 2, 2, 1, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 0, 0, 4, 4, 4, 4, 3, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 4, 3, 4, 4, 4, 4, 2, 2, 2, 2, 0, 2, 2, 2, 2, 4, 3, 4, 2, 2, 4, 2, 4, 2, 4, 1, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 1, 2, 0, 2, 2, 4, 2, 2, 2, 2, 2, 3, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 0, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 0, 4, 2, 0, 2, 4, 2, 4, 4, 4, 2, 0, 2, 2, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 2, 2, 4, 3, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 4, 3, 2, 2, 2, 2, 2, 4, 4, 2, 2, 0, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 3, 4, 3, 1, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 0, 2, 3, 4, 2, 3, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 1, 2, 2, 2, 4, 2, 4, 1, 3, 2, 4, 2, 4, 2, 4, 1, 2, 4, 4, 4, 3, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 3, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 0, 2, 2, 4, 4, 0, 4, 4, 4, 3, 2, 4, 4, 0, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 0, 4, 0, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4, 0, 2, 4, 2, 2, 2, 4, 4, 3, 4, 4, 2, 2, 2, 4, 4, 2, 4, 3, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 1, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 0, 1, 3, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 1, 1, 2, 1, 1, 2, 2, 0, 1, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 3, 4, 2, 4, 2, 2, 2, 3, 4, 4, 4, 0, 2, 4, 4, 0, 2, 4, 0, 4, 0, 2, 4, 2, 4, 4, 4, 0, 2, 0, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 0, 2, 1, 4, 2, 2, 4, 4, 1, 4, 4, 0, 2, 2, 4, 4, 2, 0, 2, 2, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 2, 4, 4, 4, 4, 0, 2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 1, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 0, 4, 4, 4, 2, 4, 2, 2, 3, 2, 4, 2, 4, 4, 4, 2, 4, 1, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 3, 4, 4, 3, 3, 4, 4, 4, 2, 4, 2, 4, 2, 0, 4, 2, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 3, 2, 4, 2, 2, 0, 1, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 3, 2, 2, 0, 4, 4, 0, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 1, 2, 4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 1, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 0, 2, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 2, 0, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 2, 3, 4, 2, 4, 4, 3, 4, 2, 2, 2, 4, 4, 4, 0, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 3, 4, 4, 2, 4, 2, 2, 3, 4, 4, 4, 3, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 0, 2, 2, 2, 2, 4, 4, 4, 0, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 0, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 0, 4, 0, 0, 4, 0, 0, 0, 0, 4, 4, 0, 4, 0, 2, 0, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 2, 2, 1, 4, 2, 4, 2, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 1, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 0, 4, 4, 2, 4, 2, 2, 2, 0, 2, 4, 3, 4, 4, 2, 4, 0, 4, 4, 2, 2, 4, 3, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 3, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 2, 3, 4, 2, 4, 2, 2, 1, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 0, 4, 2, 4, 2, 2, 4, 2, 2, 2, 1, 2, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 1, 4, 2, 4, 2, 2, 2, 4, 1, 4, 4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 3, 4, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 2, 0, 4, 4, 0, 3, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 3, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 3, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 4, 4, 0, 4, 4, 2, 0, 2, 0, 0, 0, 0, 0, 4, 0, 2, 2, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 0, 4, 4, 4, 2, 0, 1, 4, 2, 4, 1, 2, 4, 3, 4, 0, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 0, 4, 4, 4, 4, 2, 4, 2, 2, 0, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 0, 4, 2, 2, 4, 2, 4, 4, 3, 4, 2, 2, 3, 3, 4, 4, 2, 4, 4, 4, 4, 2, 2, 3, 2, 4, 4, 4, 2, 4, 2, 4, 4, 0, 2, 4, 4, 2, 4, 1, 4, 2, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 0, 2, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 0, 4, 1, 4, 4, 2, 4, 4, 4, 2, 2, 3, 2, 2, 2, 4, 2, 2, 2, 4, 3, 4, 3, 4, 2, 0, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 2, 3, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 1, 2, 2, 2, 2, 2, 2, 4, 2, 4, 3, 2, 2, 4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 2, 4, 1, 2, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 0, 4, 4, 0, 3, 2, 3, 4, 4, 4, 2, 2, 4, 2, 3, 2, 4, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 3, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 3, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 0, 2, 2, 4, 4, 4, 4, 4, 1, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 3, 4, 4, 0, 2, 4, 4, 2, 3, 2, 4, 2, 4, 4, 4, 0, 2, 2, 4, 2, 3, 4, 4, 4, 4, 3, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 3, 4, 4, 2, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 2, 4, 3, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 3, 4, 2, 4, 4, 2, 2, 4, 1, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 1, 2, 0, 4, 4, 4, 4, 4, 4, 4, 2, 0, 2, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 3, 2, 2, 2, 4, 4, 2, 4, 4, 4, 0, 2, 2, 4, 2, 4, 2, 2, 4, 0, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 3, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 4, 4, 1, 2, 4, 4, 4, 2, 2, 0, 2, 4, 4, 2, 0, 4, 2, 0, 2, 3, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 3, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 3, 2, 4, 1, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 3, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 0, 0, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 1, 0, 2, 4, 2, 3, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 0, 2, 2, 2, 0, 4, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 0, 2, 4, 2, 2, 2, 2, 2, 4, 3, 2, 4, 1, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 0, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 2, 0, 2, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 3, 1, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 0, 2, 4, 2, 2, 2, 2, 2, 2, 0, 4, 2, 2, 4, 2, 4, 1, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 3, 0, 0, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 2, 4, 2, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 0, 4, 2, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 0, 2, 4, 1, 4, 1, 4, 4, 2, 4, 0, 2, 4, 4, 4, 2, 4, 0, 1, 2, 2, 2, 4, 0, 2, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 2, 2, 0, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 1, 2, 4, 4, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4, 2, 2, 2, 1, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 1, 2, 4, 4, 4, 1, 2, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 0, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 2, 1, 4, 2, 4, 2, 3, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 2, 2, 0, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 0, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 0, 4, 2, 2, 2, 2, 4, 2, 2, 3, 4, 0, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 3, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 1, 1, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 3, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 3, 2, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 1, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 4, 0, 2, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 3, 4, 2, 2, 0, 4, 0, 2, 4, 1, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 0, 1, 0, 2, 0, 2, 0, 4, 0, 0, 2, 0, 0, 0, 4, 4, 0, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 0, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 0, 2, 4, 4, 2, 4, 2, 4, 3, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 1, 4, 4, 4, 4, 0, 4, 3, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 3, 4, 4, 4, 4, 4, 1, 2, 4, 4, 2, 1, 1, 4, 4, 2, 2, 2, 2, 0, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 1, 2, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 4, 2, 2, 4, 1, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 1, 2, 2, 4, 4, 2, 4, 4, 2, 4, 2, 0, 4, 3, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 3, 4, 4, 2, 4, 2, 4, 4, 0, 1, 4, 4, 4, 2, 2, 4, 3, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 1, 2, 2, 4, 3, 4, 4, 3, 2, 4, 2, 2, 4, 2, 3, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 3, 4, 2, 2, 0, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 3, 2, 4, 2, 2, 3, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 1, 2, 4, 2, 1, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 2, 3, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 0, 4, 4, 2, 2, 2, 4, 4, 4, 0, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 4, 0, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 3, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 1, 2, 2, 3, 2, 4, 2, 2, 1, 4, 2, 2, 4, 3, 4, 1, 4, 4, 2, 2, 3, 0, 4, 0, 2, 0, 1, 2, 0, 0, 2, 0, 0, 4, 2, 2, 4, 2, 0, 4, 0, 0, 0, 0, 0, 2, 2, 4, 4, 2, 2, 4, 0, 2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 2, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 0, 4, 4, 4, 4, 0, 2, 4, 2, 2, 2, 4, 4, 3, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 3, 3, 2, 2, 3, 4, 4, 4, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 4, 0, 4, 4, 2, 2, 2, 0, 0, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 2, 4, 2, 1, 2, 4, 2, 4, 2, 4, 2, 2, 2, 0, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 4, 0, 4, 2, 4, 2, 4, 4, 2, 0, 2, 4, 4, 2, 4, 2, 0, 2, 0, 4, 3, 4, 2, 4, 4, 4, 2, 2, 2, 1, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 3, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 0, 4, 2, 2, 4, 2, 2, 4, 2, 2, 0, 0, 4, 0, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 1, 4, 2, 4, 4, 4, 2, 2, 2, 3, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 1, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 0, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 0, 4, 4, 4, 2, 4, 4, 2, 3, 0, 4, 4, 4, 2, 0, 4, 4, 0, 4, 1, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 3, 4, 2, 0, 2, 2, 1, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 1, 4, 2, 4, 2, 4, 2, 2, 4, 2, 2, 3, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 1, 2, 2, 4, 2, 4, 2, 1, 2, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 0, 1, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 1, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 3, 2, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 0, 2, 4, 2, 2, 3, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 0, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 0, 2, 2, 4, 4, 4, 4, 4, 1, 4, 1, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 0, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 3, 4, 4, 2, 4, 1, 4, 4, 2, 2, 0, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 4, 3, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 0, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 3, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 3, 2, 4, 2, 4, 2, 2, 2, 4, 2, 3, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 1, 2, 2, 1, 2, 2, 4, 1, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 0, 4, 3, 4, 2, 4, 2, 2, 4, 2, 3, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 3, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 0, 2, 0, 3, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 1, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 0, 3, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 0, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 3, 2, 2, 4, 2, 4, 4, 4, 4, 4, 0, 2, 2, 2, 4, 2, 3, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 3, 4, 2, 4, 4, 2, 0, 2, 4, 4, 4, 2, 0, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 4, 2, 4, 0, 2, 4, 4, 2, 4, 2, 2, 4, 0, 4, 4, 2, 2, 2, 2, 4, 0, 0, 2, 4, 4, 4, 2, 4, 4, 1, 0, 4, 4, 4, 4, 2, 1, 2, 4, 4, 2, 0, 2, 4, 4, 2, 3, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 0, 4, 3, 4, 4, 2, 4, 2, 4, 4, 3, 4, 2, 2, 4, 4, 2, 1, 0, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 0, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 2, 3, 4, 4, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 1, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 0, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 0, 4, 2, 1, 4, 2, 4, 2, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 1, 2, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 4, 0, 4, 2, 0, 4, 2, 2, 2, 4, 2, 4, 0, 1, 4, 4, 1, 0, 4, 2, 0, 2, 4, 4, 4, 2, 2, 1, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 2, 3, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 2, 4, 2, 3, 4, 4, 4, 4, 0, 2, 4, 0, 1, 4, 4, 4, 3, 4, 2, 2, 4, 2, 0, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 4, 4, 0, 4, 4, 4, 0, 1, 4, 3, 4, 2, 2, 2, 3, 3, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 0, 2, 2, 4, 4, 0, 0, 4, 0, 4, 2, 1, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 0, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 0, 4, 2, 4, 3, 3, 2, 4, 4, 2, 0, 2, 4, 2, 4, 4, 3, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 0, 4, 0, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 1, 2, 2, 4, 4, 4, 2, 4, 0, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 1, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 0, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 3, 2, 2, 4, 2, 2, 0, 2, 4, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 2, 4, 0, 4, 4, 2, 4, 4, 0, 0, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 1, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 0, 4, 2, 2, 4, 4, 4, 2, 0, 4, 4, 4, 2, 3, 4, 4, 2, 2, 2, 2, 4, 1, 2, 4, 2, 0, 4, 2, 4, 2, 2, 4, 2, 0, 4, 4, 4, 2, 4, 2, 0, 4, 2, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 1, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 0, 1, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 1, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 0, 0, 4, 4, 4, 2, 3, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 0, 0, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 0, 4, 4, 3, 0, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 4, 0, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 1, 4, 4, 2, 3, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 4, 0, 2, 0, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 0, 2, 4, 4, 4, 2, 2, 4, 4, 4, 3, 4, 4, 4, 0, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 1, 4, 2, 4, 4, 2, 0, 4, 2, 4, 2, 2, 2, 2, 4, 4, 0, 2, 4, 4, 1, 0, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 0, 2, 2, 2, 2, 2, 2, 4, 2, 3, 4, 2, 2, 0, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 3, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 2, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 3, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 0, 4, 4, 4, 4, 0, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 3, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 0, 2, 4, 0, 4, 4, 4, 2, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 2, 2, 2, 2, 4, 4, 4, 2, 0, 4, 0, 4, 2, 4, 2, 2, 3, 2, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 3, 2, 4, 4, 2, 0, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 0, 4, 0, 4, 0, 4, 1, 2, 2, 0, 4, 2, 2, 2, 4, 2, 1, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 0, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 3, 4, 4, 3, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 0, 2, 4, 2, 4, 4, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 3, 4, 4, 4, 0, 4, 4, 2, 4, 2, 0, 0, 1, 4, 4, 0, 3, 4, 2, 2, 4, 4, 4, 2, 3, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 0, 2, 4, 2, 2, 2, 3, 2, 4, 2, 0, 4, 4, 2, 4, 4, 3, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 0, 2, 4, 2, 0, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 2, 3, 4, 4, 3, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 0, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 3, 0, 4, 4, 0, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 3, 0, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 4, 3, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 0, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 0, 4, 4, 2, 2, 0, 4, 2, 4, 4, 4, 0, 0, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 3, 4, 4, 2, 2, 4, 1, 2, 2, 4, 4, 0, 4, 0, 4, 4, 4, 0, 1, 3, 4, 4, 4, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 0, 4, 4, 2, 2, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 1, 4, 4, 2, 4, 3, 4, 4, 2, 0, 0, 4, 4, 0, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 1, 4, 0, 4, 2, 1, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 1, 4, 2, 4, 3, 2, 4, 4, 4, 4, 2, 1, 2, 0, 4, 4, 0, 2, 4, 4, 2, 2, 4, 2, 4, 4, 0, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 0, 0, 4, 0, 0, 0, 4, 0, 0, 2, 4, 4, 1, 4, 4, 2, 0, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 3, 4, 4, 2, 4, 4, 2, 4, 4, 4, 0, 2, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 0, 0, 4, 4, 4, 1, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 1, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 0, 4, 0, 2, 0, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 0, 0, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 0, 2, 4, 3, 0, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 0, 4, 2, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 0, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 1, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 2, 2, 4, 0, 4, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 2, 0, 2, 2, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 0, 4, 3, 4, 4, 4, 4, 2, 4, 4, 4, 4, 0, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 0, 2, 1, 4, 4, 4, 4, 4, 4, 4, 2, 0, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 0, 4, 3, 2, 2, 4, 2, 2, 4, 2, 3, 4, 4, 4, 4, 2, 4, 4, 2, 3, 4, 3, 4, 2, 4, 2, 4, 4, 4, 4, 3, 4, 4, 0, 2, 4, 4, 4, 4, 0, 2, 0, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 0, 4, 2, 4, 2, 4, 0, 2, 4, 4, 4, 4, 4, 4, 0, 4, 3, 0, 4, 4, 4, 0, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 3, 4, 4, 4, 4, 0, 0, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 4, 3, 4, 4, 1, 0, 4, 2, 2, 0, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 0, 4, 3, 2, 0, 4, 4, 4, 4, 3, 4, 2, 2, 4, 3, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 3, 4, 2, 4, 3, 4, 4, 3, 4, 4, 0, 4, 4, 1, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 0, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 0, 2, 4, 4, 4, 4, 4, 0, 0, 4, 1, 4, 4, 4, 3, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 3, 2, 2, 0, 4, 4, 4, 0, 4, 2, 2, 2, 4, 4, 4, 2, 1, 4, 4, 4, 0, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 0, 4, 2, 2, 4]\n",
            "10000\n",
            "Average Length 512.0\n",
            "Found 10000 texts.\n"
          ]
        }
      ],
      "source": [
        "texts = []\n",
        "labels = []\n",
        "\n",
        "count=0\n",
        "\n",
        "for record in train_data:\n",
        "\n",
        "        count=count+1\n",
        "        new_sen = record['cleaned_data'].split()\n",
        "\n",
        "        if len(new_sen) >= 512:\n",
        "          new_sen = new_sen[0:512]\n",
        "        \n",
        "        elif len(new_sen) < 512:\n",
        "          new_sen = new_sen[0:len(new_sen)]\n",
        "        \n",
        "        else:\n",
        "          new_sen = new_sen[-512:]\n",
        "          \n",
        "        new_sen = ' '.join(new_sen)\n",
        "\n",
        "        texts.append(new_sen)\n",
        "        labels.append(record['bias'])\n",
        "   \n",
        "len_list = [len(ele.split()) for ele in texts]\n",
        "\n",
        "print(labels)\n",
        "print(len(labels))\n",
        "\n",
        "res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n",
        "\n",
        "print(\"Average Length %s\" % res) \n",
        "print('Found %s texts.' % len(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LprCHRM2aWb8",
        "outputId": "3377113e-6825-4977-bf5d-ac9c08887b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   text  label\n",
            "0     Source USDA TOTAL STATES Continue Reading Belo...      0\n",
            "1     Source USDA TOTAL STATES Continue Reading Belo...      0\n",
            "2     Source USDA TOTAL STATES Continue Reading Belo...      0\n",
            "3     Source USDA TOTAL STATES Continue Reading Belo...      0\n",
            "4     Republican presidential nominee Donald Trump l...      4\n",
            "...                                                 ...    ...\n",
            "9995  The Campus Rape Frenzy The Attack On Due Proce...      0\n",
            "9996  Ever since its opening to the double edged swo...      4\n",
            "9997  MONTECITO Calif AP 8212 A young mother asleep ...      2\n",
            "9998  MONTECITO Calif AP 8212 A young mother asleep ...      2\n",
            "9999  8220A disgusting sinkhole of racism and vulgar...      4\n",
            "\n",
            "[10000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "summarized_data = pd.DataFrame(texts,\n",
        "               columns =['text'])\n",
        "summarized_data['label'] = labels\n",
        "print(summarized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VoY1gHZoaZmG"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout_0= Dropout(0.5)(dense_0)\n",
        "    pred = Dense(5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9kO4eVwCHKg",
        "outputId": "a3776971-f469-4ae7-dd89-06b3b2630cf1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,878,533\n",
            "Trainable params: 109,878,533\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_76781/3713393565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdbert_inps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madd_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpad_to_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbert_inps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mattention_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbert_inps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda/envs/nlp1/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2756\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2757\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2758\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2759\u001b[0m         )\n\u001b[1;32m   2760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda/envs/nlp1/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             )\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda/envs/nlp1/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda/envs/nlp1/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda/envs/nlp1/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda/envs/nlp1/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morig_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_accents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "total_accuracy=0\n",
        "total_weighted_f1=0\n",
        "total_micro_f1=0\n",
        "total_weighted_precision=0\n",
        "total_micro_precision=0\n",
        "total_weighted_recall=0\n",
        "total_micro_recall=0\n",
        "\n",
        "for i in range(5):\n",
        "  gc.collect()\n",
        "  tf.keras.backend.clear_session()\n",
        "  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "  max_len=512\n",
        "  sentences=summarized_data['text']\n",
        "  labels=summarized_data['label']\n",
        "  len(sentences),len(labels)\n",
        "  model_0=create_model()\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "  input_ids=np.asarray(input_ids)\n",
        "\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n",
        "  log_dir='dbert_model'\n",
        "\n",
        "  model_save_path='/home/ubuntu/HyperPartisan_Classification_Using_BERT/Model Weights/Best512'+str(i)+'-5labels.h5'\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "  history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n",
        "  pred_labels=[]\n",
        "\n",
        "  model_saved= create_model()\n",
        "  model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
        "  model_saved.load_weights('/home/ubuntu/HyperPartisan_Classification_Using_BERT/Model Weights/Best512'+str(i)+'-15labels.h5')\n",
        "\n",
        "  for i in range(0,len(val_inp)):\n",
        "    pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n",
        "    pred_label = pred.argmax(axis=1)\n",
        "    pred_labels.append(pred_label)\n",
        "  accuracy=accuracy_score(val_label, pred_labels)\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  total_accuracy=total_accuracy+accuracy\n",
        "  \n",
        "  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
        "  print(\"Weighted F1: \"+ str(weighted_f1))\n",
        "  total_weighted_f1=total_weighted_f1+weighted_f1\n",
        "  micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
        "  print(\"Micro F1: \"+ str(micro_f1))\n",
        "  total_micro_f1=total_micro_f1+micro_f1\n",
        "\n",
        "  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Precision: \" + str(weighted_precision))\n",
        "  total_weighted_precision=total_weighted_precision+weighted_precision\n",
        "  micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Precision: \" + str(micro_precision))\n",
        "  total_micro_precision=total_micro_precision+micro_precision\n",
        "\n",
        "  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
        "  print(\"Weighted Recall: \" + str(weighted_recall))\n",
        "  total_weighted_recall=total_weighted_recall+weighted_recall\n",
        "  micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
        "  print(\"Micro Recall: \" + str(micro_recall))\n",
        "  total_micro_recall=total_micro_recall+micro_recall\n",
        "\n",
        "\n",
        "print(\"Average Accuracy: \"+str(total_accuracy/5))\n",
        "print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n",
        "print(\"Average Micro F1: \"+str(total_micro_f1/5))\n",
        "print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n",
        "print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n",
        "print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n",
        "print(\"Average Micro Recall: \"+str(total_micro_recall/5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Best-512_0:512_15labels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
