{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%pip install transformers\n","%pip install sentencepiece\n","%pip install tensorflow\n","%pip install stanza\n","%pip install tensorflow-addons\n","%pip install nltk\n","%pip install datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T16:32:55.727574Z","iopub.status.busy":"2023-06-10T16:32:55.727223Z","iopub.status.idle":"2023-06-10T16:32:55.733741Z","shell.execute_reply":"2023-06-10T16:32:55.732166Z","shell.execute_reply.started":"2023-06-10T16:32:55.727546Z"},"id":"K0rs0NoritMk","outputId":"92b77bac-3521-4e3b-cf37-6f33a0d5c9f1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.12.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:33:56.196950Z","iopub.status.busy":"2023-06-10T16:33:56.196621Z","iopub.status.idle":"2023-06-10T16:34:00.862809Z","shell.execute_reply":"2023-06-10T16:34:00.861709Z","shell.execute_reply.started":"2023-06-10T16:33:56.196919Z"},"id":"wYwcFK5gixXz","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import re\n","import unicodedata\n","import nltk\n","#from transformers import pipeline\n","from nltk.corpus import stopwords\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n","from tqdm import tqdm\n","import pickle\n","from sklearn.metrics import confusion_matrix,f1_score,classification_report\n","import matplotlib.pyplot as plt\n","import itertools\n","from sklearn.utils import shuffle\n","from tensorflow.keras import regularizers\n","#from transformers import *\n","from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n","import pandas as pd\n","from transformers import AutoTokenizer, TFAutoModel\n","import numpy as np\n","import gc\n","import math\n","import json\n","import stanza\n","from tensorflow.keras import *\n","import tensorflow as tf\n","from tensorflow.keras import *\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import classification_report\n","from transformers import TFRobertaModel,RobertaTokenizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.initializers import RandomUniform\n","\n","from numpy.random import seed\n","import random as python_random\n","import os\n","import sys\n","\n","np.random.seed(1)\n","python_random.seed(1)\n","tf.random.set_seed(1)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:34:00.865328Z","iopub.status.busy":"2023-06-10T16:34:00.864345Z","iopub.status.idle":"2023-06-10T16:34:01.018735Z","shell.execute_reply":"2023-06-10T16:34:01.017444Z","shell.execute_reply.started":"2023-06-10T16:34:00.865284Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["# huggingface dataset access token\n","\n","from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:34:01.022375Z","iopub.status.busy":"2023-06-10T16:34:01.021954Z","iopub.status.idle":"2023-06-10T16:34:03.631005Z","shell.execute_reply":"2023-06-10T16:34:03.629872Z","shell.execute_reply.started":"2023-06-10T16:34:01.022325Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset parquet/maneshkarun--median-3000 to /root/.cache/huggingface/datasets/parquet/maneshkarun--median-3000-d9224ad77edfd979/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d0ca1eaa5f24505a976a005b784b90e","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"111035f50916426cb9522b03f1b6b03b","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d73003b3ab364ec6b1b550ea756ac49d","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/maneshkarun--median-3000-d9224ad77edfd979/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c38d1874406f4ea39cece8b1fe1a17b1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# importing datasets\n","\n","from datasets import load_dataset\n","# data = load_dataset(\"maneshkarun/median-3000\")\n","data = load_dataset(\"maneshkarun/median3k_10000s\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:34:03.633820Z","iopub.status.busy":"2023-06-10T16:34:03.632695Z","iopub.status.idle":"2023-06-10T16:34:03.643138Z","shell.execute_reply":"2023-06-10T16:34:03.642212Z","shell.execute_reply.started":"2023-06-10T16:34:03.633782Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'title', 'hyperpartisan', 'url', 'published_at', 'bias', 'word_count', 'cleaned_data', 'pos_tagged'],\n","        num_rows: 500\n","    })\n","})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:34:03.645409Z","iopub.status.busy":"2023-06-10T16:34:03.644761Z","iopub.status.idle":"2023-06-10T16:34:03.654823Z","shell.execute_reply":"2023-06-10T16:34:03.653814Z","shell.execute_reply.started":"2023-06-10T16:34:03.645374Z"},"trusted":true},"outputs":[],"source":["train_data = data['train']"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:34:03.657037Z","iopub.status.busy":"2023-06-10T16:34:03.656383Z","iopub.status.idle":"2023-06-10T16:34:03.672843Z","shell.execute_reply":"2023-06-10T16:34:03.671735Z","shell.execute_reply.started":"2023-06-10T16:34:03.657004Z"},"trusted":true},"outputs":[],"source":["train_text = train_data['cleaned_data']"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T16:34:03.675388Z","iopub.status.busy":"2023-06-10T16:34:03.674697Z","iopub.status.idle":"2023-06-10T16:34:03.892540Z","shell.execute_reply":"2023-06-10T16:34:03.891564Z","shell.execute_reply.started":"2023-06-10T16:34:03.675353Z"},"id":"2ZinwFiui-A3","outputId":"1a4d3851-73a3-444b-a286-ec608b7c3197","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 0, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 1, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 2, 4, 2, 2, 0, 2, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 0, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 0, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 4, 1, 1, 2, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 3, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 0, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 3, 2, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 2, 2, 2, 4, 2, 1, 2, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 1, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 1, 4, 2, 4, 4, 0, 2, 2, 2, 4, 4, 4, 2, 2, 2, 3, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 4, 1, 4, 4, 0, 4, 4, 4, 4, 4, 4, 3, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 1, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 3, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 4, 4, 4, 3]\n","500\n","Average Length 512.0\n","Found 500 texts.\n"]}],"source":["texts = []\n","labels = []\n","\n","count=0\n","\n","for record in train_data:\n","    count=count+1\n","    new_sen = record['cleaned_data'].split()\n","    if len(new_sen) >= 2560:\n","        new_sen = new_sen[2048:2560]\n","        \n","    elif len(new_sen) < 512:\n","        new_sen = new_sen[0:len(new_sen)]\n","        \n","    else:\n","        new_sen = new_sen[-512:]\n","          \n","    new_sen = ' '.join(new_sen)\n","\n","    texts.append(new_sen)\n","    labels.append(record['bias'])\n","    \n","len_list = [len(ele.split()) for ele in texts]\n","print(labels)\n","print(len(labels))\n","\n","res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n","\n","print(\"Average Length %s\" % res) \n","print('Found %s texts.' % len(texts))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T16:34:03.894372Z","iopub.status.busy":"2023-06-10T16:34:03.894044Z","iopub.status.idle":"2023-06-10T16:34:03.914344Z","shell.execute_reply":"2023-06-10T16:34:03.913122Z","shell.execute_reply.started":"2023-06-10T16:34:03.894340Z"},"id":"LprCHRM2aWb8","outputId":"3377113e-6825-4977-bf5d-ac9c08887b56","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                  text  label\n","0    thats quite true And on top of that you know h...      4\n","1    theyve actually enacted laws which have been v...      4\n","2    Friedman Emily 8220 Romney Warns of Obama8217s...      2\n","3    have repeatedly been playing the losing hands ...      4\n","4    is still playing catchup An analysis by DBL In...      4\n","..                                                 ...    ...\n","495  8216Where I8217m gonna go with six kids8217822...      3\n","496  soldiers have been demobilized 55 million stud...      4\n","497  A nowin situation resulted Teachers wee malign...      4\n","498  increasingly French presidents of both parties...      4\n","499  Bell said After the initial report is filed an...      3\n","\n","[500 rows x 2 columns]\n"]}],"source":["summarized_data = pd.DataFrame(texts,\n","               columns =['text'])\n","summarized_data['label'] = labels\n","print(summarized_data)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T16:34:03.918153Z","iopub.status.busy":"2023-06-10T16:34:03.917717Z","iopub.status.idle":"2023-06-10T16:34:03.924638Z","shell.execute_reply":"2023-06-10T16:34:03.923729Z","shell.execute_reply.started":"2023-06-10T16:34:03.918120Z"},"id":"VoY1gHZoaZmG","trusted":true},"outputs":[],"source":["def create_model():\n","    inps = Input(shape = (max_len,), dtype='int64')\n","    masks= Input(shape = (max_len,), dtype='int64')\n","    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n","    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n","    dropout_0= Dropout(0.5)(dense_0)\n","    pred = Dense(5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n","    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n","    print(model.summary())\n","    return model   "]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T16:34:03.926796Z","iopub.status.busy":"2023-06-10T16:34:03.926180Z","iopub.status.idle":"2023-06-10T16:55:12.223342Z","shell.execute_reply":"2023-06-10T16:55:12.222346Z","shell.execute_reply.started":"2023-06-10T16:34:03.926762Z"},"id":"x9kO4eVwCHKg","outputId":"a3776971-f469-4ae7-dd89-06b3b2630cf1","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23814c100a01450e83661b594d990797","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7175975dbbac447e9a7fcf0aafca046a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82fc7acfb5994856a024aac7edb9c921","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66dd966cd8ec45eba5c5933b12b8293d","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 16:34:34 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    32W / 250W |  15857MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 87s 658ms/step - loss: 7.1413 - accuracy: 0.6467 - val_loss: 7.0550 - val_accuracy: 0.6200\n","Epoch 2/5\n","57/57 [==============================] - 35s 609ms/step - loss: 6.4844 - accuracy: 0.8733 - val_loss: 6.6363 - val_accuracy: 0.8200\n","Epoch 3/5\n","57/57 [==============================] - 36s 633ms/step - loss: 6.2387 - accuracy: 0.9067 - val_loss: 6.4342 - val_accuracy: 0.8400\n","Epoch 4/5\n","57/57 [==============================] - 33s 583ms/step - loss: 5.9921 - accuracy: 0.9244 - val_loss: 6.5825 - val_accuracy: 0.8200\n","Epoch 5/5\n","57/57 [==============================] - 33s 586ms/step - loss: 5.7875 - accuracy: 0.9378 - val_loss: 6.4682 - val_accuracy: 0.8000\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 129ms/step\n","1/1 [==============================] - 0s 80ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 96ms/step\n","1/1 [==============================] - 0s 95ms/step\n","1/1 [==============================] - 0s 89ms/step\n","1/1 [==============================] - 0s 98ms/step\n","1/1 [==============================] - 0s 86ms/step\n","1/1 [==============================] - 0s 86ms/step\n","1/1 [==============================] - 0s 86ms/step\n","1/1 [==============================] - 0s 86ms/step\n","1/1 [==============================] - 0s 84ms/step\n","Accuracy: 0.84\n","Weighted F1: 0.7985964912280703\n","Micro F1: 0.8399999999999999\n","Weighted Precision: 0.7712987012987014\n","Micro Precision: 0.84\n","Weighted Recall: 0.84\n","Micro Recall: 0.84\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 16:38:48 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P0    38W / 250W |  15905MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 84s 656ms/step - loss: 7.0454 - accuracy: 0.6844 - val_loss: 6.8299 - val_accuracy: 0.7800\n","Epoch 2/5\n","57/57 [==============================] - 34s 604ms/step - loss: 6.4625 - accuracy: 0.8867 - val_loss: 6.4624 - val_accuracy: 0.8600\n","Epoch 3/5\n","57/57 [==============================] - 34s 606ms/step - loss: 6.1969 - accuracy: 0.9156 - val_loss: 6.3728 - val_accuracy: 0.8200\n","Epoch 4/5\n","57/57 [==============================] - 33s 584ms/step - loss: 5.9761 - accuracy: 0.9289 - val_loss: 6.6581 - val_accuracy: 0.8200\n","Epoch 5/5\n","57/57 [==============================] - 34s 598ms/step - loss: 5.7525 - accuracy: 0.9533 - val_loss: 6.0942 - val_accuracy: 0.9000\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 70ms/step\n","1/1 [==============================] - 0s 69ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 86ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 69ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","Accuracy: 0.9\n","Weighted F1: 0.8902992776057792\n","Micro F1: 0.9\n","Weighted Precision: 0.883015873015873\n","Micro Precision: 0.9\n","Weighted Recall: 0.9\n","Micro Recall: 0.9\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 16:42:56 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P0    38W / 250W |  15909MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 85s 691ms/step - loss: 6.9801 - accuracy: 0.7244 - val_loss: 6.8899 - val_accuracy: 0.7800\n","Epoch 2/5\n","57/57 [==============================] - 36s 627ms/step - loss: 6.4616 - accuracy: 0.8933 - val_loss: 6.5460 - val_accuracy: 0.8400\n","Epoch 3/5\n","57/57 [==============================] - 33s 582ms/step - loss: 6.1638 - accuracy: 0.9267 - val_loss: 6.4863 - val_accuracy: 0.8400\n","Epoch 4/5\n","57/57 [==============================] - 34s 604ms/step - loss: 5.9825 - accuracy: 0.9444 - val_loss: 6.2981 - val_accuracy: 0.8600\n","Epoch 5/5\n","57/57 [==============================] - 33s 582ms/step - loss: 5.7604 - accuracy: 0.9689 - val_loss: 6.2692 - val_accuracy: 0.8400\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 82ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 83ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 65ms/step\n","Accuracy: 0.86\n","Weighted F1: 0.8343517138599107\n","Micro F1: 0.8599999999999999\n","Weighted Precision: 0.8317279411764705\n","Micro Precision: 0.86\n","Weighted Recall: 0.86\n","Micro Recall: 0.86\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 16:47:05 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P0    38W / 250W |  15913MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 85s 684ms/step - loss: 7.0163 - accuracy: 0.6844 - val_loss: 6.8055 - val_accuracy: 0.7600\n","Epoch 2/5\n","57/57 [==============================] - 34s 604ms/step - loss: 6.4161 - accuracy: 0.8911 - val_loss: 6.5586 - val_accuracy: 0.8200\n","Epoch 3/5\n","57/57 [==============================] - 36s 625ms/step - loss: 6.0979 - accuracy: 0.9289 - val_loss: 6.5256 - val_accuracy: 0.8600\n","Epoch 4/5\n","57/57 [==============================] - 35s 622ms/step - loss: 5.8229 - accuracy: 0.9667 - val_loss: 6.3071 - val_accuracy: 0.8800\n","Epoch 5/5\n","57/57 [==============================] - 34s 598ms/step - loss: 5.6811 - accuracy: 0.9689 - val_loss: 6.0276 - val_accuracy: 0.9000\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 71ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 66ms/step\n","Accuracy: 0.9\n","Weighted F1: 0.8902992776057792\n","Micro F1: 0.9\n","Weighted Precision: 0.883015873015873\n","Micro Precision: 0.9\n","Weighted Recall: 0.9\n","Micro Recall: 0.9\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 16:51:19 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P0    38W / 250W |  15917MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 85s 686ms/step - loss: 6.9260 - accuracy: 0.7467 - val_loss: 6.9318 - val_accuracy: 0.8000\n","Epoch 2/5\n","57/57 [==============================] - 34s 605ms/step - loss: 6.4226 - accuracy: 0.9044 - val_loss: 6.5254 - val_accuracy: 0.8400\n","Epoch 3/5\n","57/57 [==============================] - 36s 627ms/step - loss: 6.1532 - accuracy: 0.9289 - val_loss: 6.4010 - val_accuracy: 0.8600\n","Epoch 4/5\n","57/57 [==============================] - 33s 581ms/step - loss: 5.9380 - accuracy: 0.9578 - val_loss: 6.5600 - val_accuracy: 0.8000\n","Epoch 5/5\n","57/57 [==============================] - 33s 582ms/step - loss: 5.8174 - accuracy: 0.9622 - val_loss: 6.2520 - val_accuracy: 0.8200\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 108ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 60ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","Accuracy: 0.86\n","Weighted F1: 0.8343529411764706\n","Micro F1: 0.8599999999999999\n","Weighted Precision: 0.8318279569892474\n","Micro Precision: 0.86\n","Weighted Recall: 0.86\n","Micro Recall: 0.86\n","Average Accuracy: 0.8720000000000001\n","Average Weighted F1: 0.849579940295202\n","Average Micro F1: 0.8719999999999999\n","Average Weighted Precision: 0.8401772690992331\n","Average Micro Precision: 0.8720000000000001\n","Average Weighted Recall: 0.8720000000000001\n","Average Micro Recall: 0.8720000000000001\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","total_accuracy=0\n","total_weighted_f1=0\n","total_micro_f1=0\n","total_weighted_precision=0\n","total_micro_precision=0\n","total_weighted_recall=0\n","total_micro_recall=0\n","\n","for i in range(5):\n","    gc.collect()\n","    tf.keras.backend.clear_session()\n","    dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n","    max_len=512\n","    sentences=summarized_data['text']\n","    labels=summarized_data['label']\n","    len(sentences),len(labels)\n","    model_0=create_model()\n","    input_ids=[]\n","    attention_masks=[]\n","\n","    for sent in sentences:\n","        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n","        input_ids.append(dbert_inps['input_ids'])\n","        attention_masks.append(dbert_inps['attention_mask'])\n","    input_ids=np.asarray(input_ids)\n","\n","    attention_masks=np.array(attention_masks)\n","    labels=np.array(labels)\n","    train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n","    log_dir='dbert_model'\n","\n","    model_save_path='./kaggle/working/roberta-best-512-0-512-' + str(i) + '-4labels.h5'\n","\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n","    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n","    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","    gpu_info = !nvidia-smi\n","    gpu_info = '\\n'.join(gpu_info)\n","    if gpu_info.find('failed') >= 0:\n","        print('Not connected to a GPU')\n","    else:\n","        print(gpu_info)\n","    history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n","    pred_labels=[]\n","\n","    model_saved= create_model()\n","    model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","    model_saved.load_weights('./kaggle/working/roberta-best-512-0-512-' + str(i) + '-4labels.h5')\n","\n","    for i in range(0,len(val_inp)):\n","        pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n","        pred_label = pred.argmax(axis=1)\n","        pred_labels.append(pred_label)\n","    accuracy=accuracy_score(val_label, pred_labels)\n","    print(\"Accuracy: \"+str(accuracy))\n","    total_accuracy=total_accuracy+accuracy\n","  \n","    weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n","    print(\"Weighted F1: \"+ str(weighted_f1))\n","    total_weighted_f1=total_weighted_f1+weighted_f1\n","    micro_f1=f1_score(val_label,pred_labels, average='micro')\n","    print(\"Micro F1: \"+ str(micro_f1))\n","    total_micro_f1=total_micro_f1+micro_f1\n","\n","    weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n","    print(\"Weighted Precision: \" + str(weighted_precision))\n","    total_weighted_precision=total_weighted_precision+weighted_precision\n","    micro_precision=precision_score(val_label, pred_labels, average='micro')\n","    print(\"Micro Precision: \" + str(micro_precision))\n","    total_micro_precision=total_micro_precision+micro_precision\n","\n","    weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n","    print(\"Weighted Recall: \" + str(weighted_recall))\n","    total_weighted_recall=total_weighted_recall+weighted_recall\n","    micro_recall=recall_score(val_label, pred_labels, average='micro')\n","    print(\"Micro Recall: \" + str(micro_recall))\n","    total_micro_recall=total_micro_recall+micro_recall\n","\n","\n","print(\"Average Accuracy: \"+str(total_accuracy/5))\n","print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n","print(\"Average Micro F1: \"+str(total_micro_f1/5))\n","print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n","print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n","print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n","print(\"Average Micro Recall: \"+str(total_micro_recall/5))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Best-512_0:512_15labels.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
