{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:44:47.363390Z","iopub.status.busy":"2023-06-10T14:44:47.362511Z","iopub.status.idle":"2023-06-10T14:45:46.654890Z","shell.execute_reply":"2023-06-10T14:45:46.653435Z","shell.execute_reply.started":"2023-06-10T14:44:47.363355Z"},"trusted":true},"outputs":[],"source":["# %pip install transformers\n","# %pip install sentencepiece\n","# %pip install tensorflow\n","# %pip install stanza\n","# %pip install tensorflow-addons\n","# %pip install nltk\n","# %pip install datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T14:44:47.353253Z","iopub.status.busy":"2023-06-10T14:44:47.352539Z","iopub.status.idle":"2023-06-10T14:44:47.358418Z","shell.execute_reply":"2023-06-10T14:44:47.357493Z","shell.execute_reply.started":"2023-06-10T14:44:47.353215Z"},"id":"K0rs0NoritMk","outputId":"92b77bac-3521-4e3b-cf37-6f33a0d5c9f1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.7.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:46.658978Z","iopub.status.busy":"2023-06-10T14:45:46.658654Z","iopub.status.idle":"2023-06-10T14:45:46.816548Z","shell.execute_reply":"2023-06-10T14:45:46.815500Z","shell.execute_reply.started":"2023-06-10T14:45:46.658949Z"},"id":"wYwcFK5gixXz","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ubuntu/miniconda/envs/nlp/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import re\n","import unicodedata\n","import nltk\n","#from transformers import pipeline\n","from nltk.corpus import stopwords\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n","from tqdm import tqdm\n","import pickle\n","from sklearn.metrics import confusion_matrix,f1_score,classification_report\n","import matplotlib.pyplot as plt\n","import itertools\n","from sklearn.utils import shuffle\n","from tensorflow.keras import regularizers\n","#from transformers import *\n","from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n","import pandas as pd\n","from transformers import AutoTokenizer, TFAutoModel\n","import numpy as np\n","import gc\n","import math\n","import json\n","import stanza\n","from tensorflow.keras import *\n","import tensorflow as tf\n","from tensorflow.keras import *\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import classification_report\n","from transformers import TFRobertaModel,RobertaTokenizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.initializers import RandomUniform\n","\n","from numpy.random import seed\n","import random as python_random\n","import os\n","import sys\n","\n","np.random.seed(1)\n","python_random.seed(1)\n","tf.random.set_seed(1)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:46.821786Z","iopub.status.busy":"2023-06-10T14:45:46.821439Z","iopub.status.idle":"2023-06-10T14:45:47.103906Z","shell.execute_reply":"2023-06-10T14:45:47.102874Z","shell.execute_reply.started":"2023-06-10T14:45:46.821759Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/ubuntu/.cache/huggingface/token\n","Login successful\n"]}],"source":["# huggingface dataset access token\n","\n","from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:47.107502Z","iopub.status.busy":"2023-06-10T14:45:47.106653Z","iopub.status.idle":"2023-06-10T14:45:47.780766Z","shell.execute_reply":"2023-06-10T14:45:47.779821Z","shell.execute_reply.started":"2023-06-10T14:45:47.107462Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/maneshkarun___parquet/maneshkarun--median3k_10000s-a12d2bed8c5e7733/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n","100%|██████████| 1/1 [00:00<00:00, 650.68it/s]\n"]}],"source":["# importing datasets\n","\n","from datasets import load_dataset\n","# data = load_dataset(\"maneshkarun/median-3000\")\n","data = load_dataset(\"maneshkarun/median3k_10000s\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:47.783231Z","iopub.status.busy":"2023-06-10T14:45:47.782181Z","iopub.status.idle":"2023-06-10T14:45:47.790208Z","shell.execute_reply":"2023-06-10T14:45:47.789201Z","shell.execute_reply.started":"2023-06-10T14:45:47.783195Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'title', 'hyperpartisan', 'url', 'published_at', 'bias', 'word_count', 'cleaned_data', 'pos_tagged'],\n","        num_rows: 10000\n","    })\n","})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:47.793104Z","iopub.status.busy":"2023-06-10T14:45:47.791925Z","iopub.status.idle":"2023-06-10T14:45:47.799579Z","shell.execute_reply":"2023-06-10T14:45:47.798586Z","shell.execute_reply.started":"2023-06-10T14:45:47.793070Z"},"trusted":true},"outputs":[],"source":["train_data = data['train']"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:47.802589Z","iopub.status.busy":"2023-06-10T14:45:47.801101Z","iopub.status.idle":"2023-06-10T14:45:47.820355Z","shell.execute_reply":"2023-06-10T14:45:47.819505Z","shell.execute_reply.started":"2023-06-10T14:45:47.802563Z"},"trusted":true},"outputs":[],"source":["train_text = train_data['cleaned_data']"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T14:45:47.822679Z","iopub.status.busy":"2023-06-10T14:45:47.822314Z","iopub.status.idle":"2023-06-10T14:45:48.142529Z","shell.execute_reply":"2023-06-10T14:45:48.140589Z","shell.execute_reply.started":"2023-06-10T14:45:47.822646Z"},"id":"2ZinwFiui-A3","outputId":"1a4d3851-73a3-444b-a286-ec608b7c3197","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 0, 0, 0, 4, 4, 0, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 3, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 1, 0, 4, 3, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 1, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 0, 4, 4, 2, 2, 4, 3, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 1, 4, 4, 4, 4, 2, 2, 2, 2, 3, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 3, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 0, 4, 0, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 1, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 0, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 0, 4, 0, 4, 2, 4, 4, 4, 0, 4, 2, 4, 3, 4, 4, 2, 4, 2, 2, 2, 3, 4, 2, 4, 2, 4, 4, 4, 4, 2, 0, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 0, 4, 4, 4, 0, 2, 0, 2, 4, 2, 2, 1, 2, 4, 2, 2, 2, 4, 3, 1, 3, 1, 2, 2, 4, 0, 4, 4, 4, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 0, 2, 2, 0, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 3, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 0, 4, 4, 2, 4, 3, 2, 4, 2, 4, 4, 3, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 0, 4, 1, 4, 2, 4, 0, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 2, 2, 2, 2, 4, 1, 4, 4, 2, 2, 4, 2, 0, 4, 3, 2, 3, 0, 2, 2, 2, 2, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 2, 2, 1, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 0, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 0, 4, 2, 4, 1, 2, 2, 4, 4, 4, 2, 4, 4, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 0, 4, 0, 1, 4, 2, 4, 2, 2, 0, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 2, 4, 4, 0, 2, 2, 2, 2, 4, 2, 4, 2, 4, 1, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 3, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 2, 3, 4, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 2, 2, 0, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 3, 3, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 3, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 0, 4, 1, 3, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 2, 1, 2, 4, 4, 4, 2, 3, 2, 4, 0, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 1, 4, 3, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 0, 2, 4, 4, 4, 2, 4, 2, 4, 0, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 1, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 3, 4, 4, 4, 4, 2, 2, 2, 4, 1, 2, 4, 4, 4, 2, 2, 4, 0, 3, 4, 4, 4, 2, 4, 4, 4, 3, 3, 4, 4, 4, 2, 4, 3, 2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 3, 2, 4, 4, 1, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 1, 4, 3, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 0, 4, 2, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 3, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 2, 0, 2, 4, 4, 4, 2, 4, 4, 2, 3, 4, 4, 2, 2, 4, 2, 2, 0, 2, 2, 2, 4, 1, 4, 4, 2, 2, 2, 4, 2, 3, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 3, 2, 2, 2, 4, 4, 4, 4, 2, 4, 3, 4, 4, 1, 4, 2, 2, 4, 2, 4, 4, 0, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 0, 4, 4, 4, 0, 2, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 1, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 0, 2, 2, 4, 4, 2, 2, 2, 3, 2, 2, 4, 4, 4, 4, 2, 4, 4, 0, 2, 4, 2, 4, 4, 2, 4, 1, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 3, 0, 4, 4, 2, 0, 2, 2, 3, 4, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 2, 2, 3, 3, 4, 4, 1, 4, 2, 4, 2, 4, 3, 4, 4, 4, 2, 2, 4, 4, 0, 3, 2, 2, 4, 4, 2, 4, 2, 1, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 1, 2, 2, 2, 4, 3, 2, 4, 2, 4, 4, 2, 4, 2, 3, 1, 4, 4, 2, 2, 0, 4, 4, 2, 4, 4, 3, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 0, 4, 2, 0, 2, 2, 4, 4, 0, 0, 4, 2, 2, 2, 1, 0, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 1, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 2, 2, 2, 4, 2, 4, 1, 4, 2, 4, 4, 4, 2, 3, 2, 2, 1, 2, 4, 4, 2, 3, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 3, 2, 2, 4, 2, 2, 4, 4, 2, 4, 3, 4, 2, 0, 4, 4, 4, 2, 2, 4, 4, 2, 2, 3, 3, 4, 4, 4, 4, 4, 3, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 0, 1, 4, 2, 4, 2, 2, 1, 2, 4, 2, 4, 2, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 2, 1, 2, 4, 4, 2, 0, 4, 4, 4, 2, 4, 2, 4, 2, 0, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 3, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 1, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 0, 0, 2, 4, 4, 4, 2, 2, 4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 1, 4, 2, 4, 2, 0, 2, 2, 2, 4, 2, 3, 4, 3, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 3, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 0, 4, 2, 2, 1, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 2, 2, 3, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 3, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 0, 2, 4, 3, 2, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 1, 2, 2, 3, 2, 2, 2, 2, 2, 1, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 0, 2, 4, 4, 2, 4, 4, 2, 4, 2, 3, 4, 2, 2, 2, 4, 0, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 3, 4, 4, 4, 2, 4, 3, 4, 4, 1, 4, 4, 0, 2, 2, 4, 2, 4, 2, 3, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 1, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 0, 0, 2, 0, 0, 0, 0, 4, 4, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 2, 4, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 3, 2, 2, 2, 2, 4, 4, 0, 4, 2, 2, 0, 3, 2, 2, 2, 2, 1, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 0, 0, 4, 4, 4, 4, 3, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 4, 3, 4, 4, 4, 4, 2, 2, 2, 2, 0, 2, 2, 2, 2, 4, 3, 4, 2, 2, 4, 2, 4, 2, 4, 1, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 1, 2, 0, 2, 2, 4, 2, 2, 2, 2, 2, 3, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 0, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 0, 4, 2, 0, 2, 4, 2, 4, 4, 4, 2, 0, 2, 2, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 2, 2, 4, 3, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 4, 3, 2, 2, 2, 2, 2, 4, 4, 2, 2, 0, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 3, 4, 3, 1, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 0, 2, 3, 4, 2, 3, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 1, 2, 2, 2, 4, 2, 4, 1, 3, 2, 4, 2, 4, 2, 4, 1, 2, 4, 4, 4, 3, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 3, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 0, 2, 2, 4, 4, 0, 4, 4, 4, 3, 2, 4, 4, 0, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 0, 4, 0, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4, 0, 2, 4, 2, 2, 2, 4, 4, 3, 4, 4, 2, 2, 2, 4, 4, 2, 4, 3, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 1, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 0, 1, 3, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 1, 1, 2, 1, 1, 2, 2, 0, 1, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 3, 4, 2, 4, 2, 2, 2, 3, 4, 4, 4, 0, 2, 4, 4, 0, 2, 4, 0, 4, 0, 2, 4, 2, 4, 4, 4, 0, 2, 0, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 0, 2, 1, 4, 2, 2, 4, 4, 1, 4, 4, 0, 2, 2, 4, 4, 2, 0, 2, 2, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 2, 4, 4, 4, 4, 0, 2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 1, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 0, 4, 4, 4, 2, 4, 2, 2, 3, 2, 4, 2, 4, 4, 4, 2, 4, 1, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 3, 4, 4, 3, 3, 4, 4, 4, 2, 4, 2, 4, 2, 0, 4, 2, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 3, 2, 4, 2, 2, 0, 1, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 3, 2, 2, 0, 4, 4, 0, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 1, 2, 4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 1, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 0, 2, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 2, 0, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 2, 3, 4, 2, 4, 4, 3, 4, 2, 2, 2, 4, 4, 4, 0, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 3, 4, 4, 2, 4, 2, 2, 3, 4, 4, 4, 3, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 0, 2, 2, 2, 2, 4, 4, 4, 0, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 0, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 0, 4, 0, 0, 4, 0, 0, 0, 0, 4, 4, 0, 4, 0, 2, 0, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 2, 2, 1, 4, 2, 4, 2, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 1, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 0, 4, 4, 2, 4, 2, 2, 2, 0, 2, 4, 3, 4, 4, 2, 4, 0, 4, 4, 2, 2, 4, 3, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 3, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 2, 3, 4, 2, 4, 2, 2, 1, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 0, 4, 2, 4, 2, 2, 4, 2, 2, 2, 1, 2, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 1, 4, 2, 4, 2, 2, 2, 4, 1, 4, 4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 3, 4, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 2, 0, 4, 4, 0, 3, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 3, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 3, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 4, 4, 0, 4, 4, 2, 0, 2, 0, 0, 0, 0, 0, 4, 0, 2, 2, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 0, 4, 4, 4, 2, 0, 1, 4, 2, 4, 1, 2, 4, 3, 4, 0, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 0, 4, 4, 4, 4, 2, 4, 2, 2, 0, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 0, 4, 2, 2, 4, 2, 4, 4, 3, 4, 2, 2, 3, 3, 4, 4, 2, 4, 4, 4, 4, 2, 2, 3, 2, 4, 4, 4, 2, 4, 2, 4, 4, 0, 2, 4, 4, 2, 4, 1, 4, 2, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 0, 2, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 0, 4, 1, 4, 4, 2, 4, 4, 4, 2, 2, 3, 2, 2, 2, 4, 2, 2, 2, 4, 3, 4, 3, 4, 2, 0, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 2, 3, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 1, 2, 2, 2, 2, 2, 2, 4, 2, 4, 3, 2, 2, 4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 2, 4, 1, 2, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 0, 4, 4, 0, 3, 2, 3, 4, 4, 4, 2, 2, 4, 2, 3, 2, 4, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 3, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 3, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 0, 2, 2, 4, 4, 4, 4, 4, 1, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 3, 4, 4, 0, 2, 4, 4, 2, 3, 2, 4, 2, 4, 4, 4, 0, 2, 2, 4, 2, 3, 4, 4, 4, 4, 3, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 3, 4, 4, 2, 2, 4, 4, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 2, 4, 3, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 3, 4, 2, 4, 4, 2, 2, 4, 1, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 1, 2, 0, 4, 4, 4, 4, 4, 4, 4, 2, 0, 2, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 3, 2, 2, 2, 4, 4, 2, 4, 4, 4, 0, 2, 2, 4, 2, 4, 2, 2, 4, 0, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 3, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 4, 4, 1, 2, 4, 4, 4, 2, 2, 0, 2, 4, 4, 2, 0, 4, 2, 0, 2, 3, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 3, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 3, 2, 4, 1, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 3, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 0, 0, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 1, 0, 2, 4, 2, 3, 4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 0, 2, 2, 2, 0, 4, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 0, 2, 4, 2, 2, 2, 2, 2, 4, 3, 2, 4, 1, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 0, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 2, 0, 2, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 3, 1, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 0, 2, 4, 2, 2, 2, 2, 2, 2, 0, 4, 2, 2, 4, 2, 4, 1, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 3, 0, 0, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 2, 4, 2, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 0, 4, 2, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 0, 2, 4, 1, 4, 1, 4, 4, 2, 4, 0, 2, 4, 4, 4, 2, 4, 0, 1, 2, 2, 2, 4, 0, 2, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 2, 2, 0, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 1, 2, 4, 4, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4, 2, 2, 2, 1, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 1, 2, 4, 4, 4, 1, 2, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 0, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 2, 1, 4, 2, 4, 2, 3, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 2, 2, 2, 0, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 0, 4, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 0, 4, 2, 2, 2, 2, 4, 2, 2, 3, 4, 0, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 3, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 1, 1, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 3, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 3, 2, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 1, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 4, 0, 2, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 3, 4, 2, 2, 0, 4, 0, 2, 4, 1, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 0, 1, 0, 2, 0, 2, 0, 4, 0, 0, 2, 0, 0, 0, 4, 4, 0, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 0, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 0, 2, 4, 4, 2, 4, 2, 4, 3, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 4, 1, 4, 4, 4, 4, 0, 4, 3, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 3, 4, 4, 4, 4, 4, 1, 2, 4, 4, 2, 1, 1, 4, 4, 2, 2, 2, 2, 0, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 1, 2, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 4, 2, 2, 4, 1, 4, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 1, 2, 2, 4, 4, 2, 4, 4, 2, 4, 2, 0, 4, 3, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 3, 4, 4, 2, 4, 2, 4, 4, 0, 1, 4, 4, 4, 2, 2, 4, 3, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 1, 2, 2, 4, 3, 4, 4, 3, 2, 4, 2, 2, 4, 2, 3, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 3, 4, 2, 2, 0, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 3, 2, 4, 2, 2, 3, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 1, 2, 4, 2, 1, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 2, 2, 3, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 0, 4, 4, 2, 2, 2, 4, 4, 4, 0, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 4, 0, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 3, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 1, 2, 2, 3, 2, 4, 2, 2, 1, 4, 2, 2, 4, 3, 4, 1, 4, 4, 2, 2, 3, 0, 4, 0, 2, 0, 1, 2, 0, 0, 2, 0, 0, 4, 2, 2, 4, 2, 0, 4, 0, 0, 0, 0, 0, 2, 2, 4, 4, 2, 2, 4, 0, 2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 4, 2, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 0, 4, 4, 4, 4, 0, 2, 4, 2, 2, 2, 4, 4, 3, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 3, 3, 2, 2, 3, 4, 4, 4, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 4, 0, 4, 4, 2, 2, 2, 0, 0, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 2, 4, 2, 1, 2, 4, 2, 4, 2, 4, 2, 2, 2, 0, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 4, 0, 4, 2, 4, 2, 4, 4, 2, 0, 2, 4, 4, 2, 4, 2, 0, 2, 0, 4, 3, 4, 2, 4, 4, 4, 2, 2, 2, 1, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 3, 4, 4, 4, 2, 2, 2, 2, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 0, 4, 2, 2, 4, 2, 2, 4, 2, 2, 0, 0, 4, 0, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 1, 4, 2, 4, 4, 4, 2, 2, 2, 3, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 1, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 0, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 0, 4, 4, 4, 2, 4, 4, 2, 3, 0, 4, 4, 4, 2, 0, 4, 4, 0, 4, 1, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 3, 4, 2, 0, 2, 2, 1, 2, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 1, 4, 2, 4, 2, 4, 2, 2, 4, 2, 2, 3, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 1, 2, 2, 4, 2, 4, 2, 1, 2, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 0, 1, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 1, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 3, 2, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 0, 2, 4, 2, 2, 3, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 0, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 0, 2, 2, 4, 4, 4, 4, 4, 1, 4, 1, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 0, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 3, 4, 4, 2, 4, 1, 4, 4, 2, 2, 0, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 4, 3, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 0, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 3, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 3, 2, 4, 2, 4, 2, 2, 2, 4, 2, 3, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 1, 2, 2, 1, 2, 2, 4, 1, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 0, 4, 3, 4, 2, 4, 2, 2, 4, 2, 3, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 3, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 0, 2, 0, 3, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 1, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 0, 3, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 0, 2, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 3, 2, 2, 4, 2, 4, 4, 4, 4, 4, 0, 2, 2, 2, 4, 2, 3, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 3, 4, 2, 4, 4, 2, 0, 2, 4, 4, 4, 2, 0, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 4, 2, 4, 0, 2, 4, 4, 2, 4, 2, 2, 4, 0, 4, 4, 2, 2, 2, 2, 4, 0, 0, 2, 4, 4, 4, 2, 4, 4, 1, 0, 4, 4, 4, 4, 2, 1, 2, 4, 4, 2, 0, 2, 4, 4, 2, 3, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 0, 4, 3, 4, 4, 2, 4, 2, 4, 4, 3, 4, 2, 2, 4, 4, 2, 1, 0, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 0, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 2, 3, 4, 4, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 1, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 0, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 2, 2, 4, 0, 4, 2, 1, 4, 2, 4, 2, 4, 4, 4, 0, 2, 4, 4, 4, 4, 4, 1, 2, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 4, 0, 4, 2, 0, 4, 2, 2, 2, 4, 2, 4, 0, 1, 4, 4, 1, 0, 4, 2, 0, 2, 4, 4, 4, 2, 2, 1, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 2, 3, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 2, 4, 2, 3, 4, 4, 4, 4, 0, 2, 4, 0, 1, 4, 4, 4, 3, 4, 2, 2, 4, 2, 0, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 4, 4, 0, 4, 4, 4, 0, 1, 4, 3, 4, 2, 2, 2, 3, 3, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 0, 2, 2, 4, 4, 0, 0, 4, 0, 4, 2, 1, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 0, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 0, 4, 2, 4, 3, 3, 2, 4, 4, 2, 0, 2, 4, 2, 4, 4, 3, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 0, 4, 0, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 1, 2, 2, 4, 4, 4, 2, 4, 0, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 1, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 0, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 3, 2, 2, 4, 2, 2, 0, 2, 4, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 2, 4, 0, 4, 4, 2, 4, 4, 0, 0, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 1, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 0, 4, 2, 2, 4, 4, 4, 2, 0, 4, 4, 4, 2, 3, 4, 4, 2, 2, 2, 2, 4, 1, 2, 4, 2, 0, 4, 2, 4, 2, 2, 4, 2, 0, 4, 4, 4, 2, 4, 2, 0, 4, 2, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 1, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 0, 1, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 1, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 0, 0, 4, 4, 4, 2, 3, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 0, 0, 4, 0, 4, 4, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 0, 4, 4, 3, 0, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 4, 0, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 1, 4, 4, 2, 3, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 4, 0, 2, 0, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 0, 2, 4, 4, 4, 2, 2, 4, 4, 4, 3, 4, 4, 4, 0, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 1, 4, 2, 4, 4, 2, 0, 4, 2, 4, 2, 2, 2, 2, 4, 4, 0, 2, 4, 4, 1, 0, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 0, 2, 2, 2, 2, 2, 2, 4, 2, 3, 4, 2, 2, 0, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 3, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 2, 4, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 3, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 0, 4, 4, 4, 4, 0, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 3, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 0, 2, 4, 0, 4, 4, 4, 2, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 2, 2, 2, 2, 4, 4, 4, 2, 0, 4, 0, 4, 2, 4, 2, 2, 3, 2, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 3, 2, 4, 4, 2, 0, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 0, 4, 0, 4, 0, 4, 1, 2, 2, 0, 4, 2, 2, 2, 4, 2, 1, 2, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 0, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 3, 4, 4, 3, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 0, 2, 4, 2, 4, 4, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 3, 4, 4, 4, 0, 4, 4, 2, 4, 2, 0, 0, 1, 4, 4, 0, 3, 4, 2, 2, 4, 4, 4, 2, 3, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 0, 2, 4, 2, 2, 2, 3, 2, 4, 2, 0, 4, 4, 2, 4, 4, 3, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 0, 2, 4, 2, 0, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 2, 3, 4, 4, 3, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 0, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 3, 0, 4, 4, 0, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 3, 0, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 4, 3, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 0, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 0, 4, 4, 2, 2, 0, 4, 2, 4, 4, 4, 0, 0, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 3, 4, 4, 2, 2, 4, 1, 2, 2, 4, 4, 0, 4, 0, 4, 4, 4, 0, 1, 3, 4, 4, 4, 2, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 0, 4, 4, 2, 2, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 1, 4, 4, 2, 4, 3, 4, 4, 2, 0, 0, 4, 4, 0, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 1, 4, 0, 4, 2, 1, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 1, 4, 2, 4, 3, 2, 4, 4, 4, 4, 2, 1, 2, 0, 4, 4, 0, 2, 4, 4, 2, 2, 4, 2, 4, 4, 0, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 0, 0, 4, 0, 0, 0, 4, 0, 0, 2, 4, 4, 1, 4, 4, 2, 0, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 3, 4, 4, 2, 4, 4, 2, 4, 4, 4, 0, 2, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 0, 0, 4, 4, 4, 1, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 1, 4, 2, 4, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 0, 4, 0, 2, 0, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 0, 0, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 0, 2, 4, 3, 0, 4, 4, 2, 2, 4, 4, 4, 2, 4, 4, 4, 0, 4, 2, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 0, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 1, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2, 4, 4, 2, 4, 2, 2, 4, 4, 0, 2, 2, 4, 0, 4, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 2, 0, 2, 2, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 0, 4, 3, 4, 4, 4, 4, 2, 4, 4, 4, 4, 0, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 0, 2, 1, 4, 4, 4, 4, 4, 4, 4, 2, 0, 2, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 0, 4, 3, 2, 2, 4, 2, 2, 4, 2, 3, 4, 4, 4, 4, 2, 4, 4, 2, 3, 4, 3, 4, 2, 4, 2, 4, 4, 4, 4, 3, 4, 4, 0, 2, 4, 4, 4, 4, 0, 2, 0, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 0, 4, 2, 4, 2, 4, 0, 2, 4, 4, 4, 4, 4, 4, 0, 4, 3, 0, 4, 4, 4, 0, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 3, 4, 4, 4, 4, 0, 0, 4, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 4, 3, 4, 4, 1, 0, 4, 2, 2, 0, 2, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 0, 4, 3, 2, 0, 4, 4, 4, 4, 3, 4, 2, 2, 4, 3, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 3, 4, 2, 4, 3, 4, 4, 3, 4, 4, 0, 4, 4, 1, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 0, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 0, 2, 4, 4, 4, 4, 4, 0, 0, 4, 1, 4, 4, 4, 3, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 3, 2, 2, 0, 4, 4, 4, 0, 4, 2, 2, 2, 4, 4, 4, 2, 1, 4, 4, 4, 0, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 0, 4, 2, 2, 4]\n","10000\n","Average Length 512.0\n","Found 10000 texts.\n"]}],"source":["texts = []\n","labels = []\n","\n","count=0\n","\n","for record in train_data:\n","\n","        count=count+1\n","        new_sen = record['cleaned_data'].split()\n","\n","        if len(new_sen) >= 1024:\n","          new_sen = new_sen[512:1024]\n","        \n","        elif len(new_sen) < 512:\n","          new_sen = new_sen[0:len(new_sen)]\n","        \n","        else:\n","          new_sen = new_sen[-512:]\n","          \n","        new_sen = ' '.join(new_sen)\n","\n","        texts.append(new_sen)\n","        labels.append(record['bias'])\n","   \n","len_list = [len(ele.split()) for ele in texts]\n","\n","print(labels)\n","print(len(labels))\n","\n","res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n","\n","print(\"Average Length %s\" % res) \n","print('Found %s texts.' % len(texts))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T14:45:48.144508Z","iopub.status.busy":"2023-06-10T14:45:48.144111Z","iopub.status.idle":"2023-06-10T14:45:48.156413Z","shell.execute_reply":"2023-06-10T14:45:48.155254Z","shell.execute_reply.started":"2023-06-10T14:45:48.144474Z"},"id":"LprCHRM2aWb8","outputId":"3377113e-6825-4977-bf5d-ac9c08887b56","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                   text  label\n","0     100 Apr 1 10924 95 1720 113 1815 100 69 88 Mar...      0\n","1     90 Jul 1 10368 97 1722 90 2000 105 64 102 Jun ...      0\n","2     83 Oct 1 10144 92 2388 110 1859 101 76 97 Sep ...      0\n","3     104 Nov 1 10597 94 1882 97 1681 95 73 83 Oct 1...      0\n","4     help and Obama Administration knowledge is the...      4\n","...                                                 ...    ...\n","9995  hard and they not only issued these commands t...      0\n","9996  funds coming from developed countries reaching...      4\n","9997  need money to replace his home and possessions...      2\n","9998  need money to replace his home and possessions...      2\n","9999  other friend was also dropped He had claimed h...      4\n","\n","[10000 rows x 2 columns]\n"]}],"source":["summarized_data = pd.DataFrame(texts,\n","               columns =['text'])\n","summarized_data['label'] = labels\n","print(summarized_data)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:48.161428Z","iopub.status.busy":"2023-06-10T14:45:48.160419Z","iopub.status.idle":"2023-06-10T14:45:48.168958Z","shell.execute_reply":"2023-06-10T14:45:48.168223Z","shell.execute_reply.started":"2023-06-10T14:45:48.161377Z"},"id":"VoY1gHZoaZmG","trusted":true},"outputs":[],"source":["def create_model():\n","    inps = Input(shape = (max_len,), dtype='int64')\n","    masks= Input(shape = (max_len,), dtype='int64')\n","    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n","    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n","    dropout_0= Dropout(0.5)(dense_0)\n","    pred = Dense(5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n","    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n","    print(model.summary())\n","    return model   "]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T14:45:48.171490Z","iopub.status.busy":"2023-06-10T14:45:48.170746Z","iopub.status.idle":"2023-06-10T15:06:53.723210Z","shell.execute_reply":"2023-06-10T15:06:53.722007Z","shell.execute_reply.started":"2023-06-10T14:45:48.171440Z"},"id":"x9kO4eVwCHKg","outputId":"a3776971-f469-4ae7-dd89-06b3b2630cf1","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-06-23 09:40:28.527578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-06-23 09:40:28.532412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-06-23 09:40:28.532534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-06-23 09:40:28.533253: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-23 09:40:28.534707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-06-23 09:40:28.534838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-06-23 09:40:28.534923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-06-23 09:40:28.950624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-06-23 09:40:28.950764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-06-23 09:40:28.950857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-06-23 09:40:28.950949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20788 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:06:00.0, compute capability: 8.6\n","2023-06-23 09:40:29.968289: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/home/ubuntu/miniconda/envs/nlp/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["Fri Jun 23 09:41:58 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A10          On   | 00000000:06:00.0 Off |                    0 |\n","|  0%   47C    P0    61W / 150W |  21548MiB / 23028MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A    114465      C   ...conda/envs/nlp/bin/python    21546MiB |\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/home/ubuntu/miniconda/envs/nlp/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"," 671/1125 [================>.............] - ETA: 2:14 - loss: 6.0067 - accuracy: 0.8513"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_114465/1761591668.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0mpred_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda/envs/nlp/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda/envs/nlp/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda/envs/nlp/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda/envs/nlp/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda/envs/nlp/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda/envs/nlp/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda/envs/nlp/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda/envs/nlp/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m~/miniconda/envs/nlp/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","total_accuracy=0\n","total_weighted_f1=0\n","total_micro_f1=0\n","total_weighted_precision=0\n","total_micro_precision=0\n","total_weighted_recall=0\n","total_micro_recall=0\n","\n","for i in range(1):\n","  gc.collect()\n","  tf.keras.backend.clear_session()\n","  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n","  max_len=512\n","  sentences=summarized_data['text']\n","  labels=summarized_data['label']\n","  len(sentences),len(labels)\n","  model_0=create_model()\n","  input_ids=[]\n","  attention_masks=[]\n","\n","  for sent in sentences:\n","    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n","    input_ids.append(dbert_inps['input_ids'])\n","    attention_masks.append(dbert_inps['attention_mask'])\n","  input_ids=np.asarray(input_ids)\n","\n","  attention_masks=np.array(attention_masks)\n","  labels=np.array(labels)\n","  train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n","  log_dir='dbert_model'\n","\n","  model_save_path='/home/ubuntu/HyperPartisan_Classification_Using_BERT/Best512'+str(i)+'-5labels.h5'\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n","  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n","  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","  gpu_info = !nvidia-smi\n","  gpu_info = '\\n'.join(gpu_info)\n","  if gpu_info.find('failed') >= 0:\n","    print('Not connected to a GPU')\n","  else:\n","    print(gpu_info)\n","  history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n","  pred_labels=[]\n","\n","  model_saved= create_model()\n","  model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","  model_saved.load_weights('/home/ubuntu/HyperPartisan_Classification_Using_BERT/Best512'+str(i)+'-5labels.h5')\n","\n","  for i in range(0,len(val_inp)):\n","    pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n","    pred_label = pred.argmax(axis=1)\n","    pred_labels.append(pred_label)\n","  accuracy=accuracy_score(val_label, pred_labels)\n","  print(\"Accuracy: \"+str(accuracy))\n","  total_accuracy=total_accuracy+accuracy\n","  \n","  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n","  print(\"Weighted F1: \"+ str(weighted_f1))\n","  total_weighted_f1=total_weighted_f1+weighted_f1\n","  micro_f1=f1_score(val_label,pred_labels, average='micro')\n","  print(\"Micro F1: \"+ str(micro_f1))\n","  total_micro_f1=total_micro_f1+micro_f1\n","\n","  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n","  print(\"Weighted Precision: \" + str(weighted_precision))\n","  total_weighted_precision=total_weighted_precision+weighted_precision\n","  micro_precision=precision_score(val_label, pred_labels, average='micro')\n","  print(\"Micro Precision: \" + str(micro_precision))\n","  total_micro_precision=total_micro_precision+micro_precision\n","\n","  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n","  print(\"Weighted Recall: \" + str(weighted_recall))\n","  total_weighted_recall=total_weighted_recall+weighted_recall\n","  micro_recall=recall_score(val_label, pred_labels, average='micro')\n","  print(\"Micro Recall: \" + str(micro_recall))\n","  total_micro_recall=total_micro_recall+micro_recall\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Average Accuracy: \"+str(total_accuracy/1))\n","print(\"Average Weighted F1: \"+str(total_weighted_f1/1))\n","print(\"Average Micro F1: \"+str(total_micro_f1/1))\n","print(\"Average Weighted Precision: \"+str(total_weighted_precision/1))\n","print(\"Average Micro Precision: \"+str(total_micro_precision/1))\n","print(\"Average Weighted Recall: \"+str(total_weighted_recall/1))\n","print(\"Average Micro Recall: \"+str(total_micro_recall/1))"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Best-512_0:512_15labels.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":4}
