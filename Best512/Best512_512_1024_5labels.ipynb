{"cells":[{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:44:47.363390Z","iopub.status.busy":"2023-06-10T14:44:47.362511Z","iopub.status.idle":"2023-06-10T14:45:46.654890Z","shell.execute_reply":"2023-06-10T14:45:46.653435Z","shell.execute_reply.started":"2023-06-10T14:44:47.363355Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.29.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.7.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.13.0rc0, 2.13.0rc1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.7.0\u001b[0m\u001b[31m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: stanza in /opt/conda/lib/python3.10/site-packages (1.5.0)\n","Requirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.2.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.23.5)\n","Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.28.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from stanza) (1.16.0)\n","Requirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.0.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.64.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.12.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.5.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2023.5.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (0.20.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow-addons) (3.0.9)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install transformers\n","%pip install sentencepiece\n","%pip install tensorflow\n","%pip install stanza\n","%pip install tensorflow-addons\n","%pip install nltk\n","%pip install datasets"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T14:44:47.353253Z","iopub.status.busy":"2023-06-10T14:44:47.352539Z","iopub.status.idle":"2023-06-10T14:44:47.358418Z","shell.execute_reply":"2023-06-10T14:44:47.357493Z","shell.execute_reply.started":"2023-06-10T14:44:47.353215Z"},"id":"K0rs0NoritMk","outputId":"92b77bac-3521-4e3b-cf37-6f33a0d5c9f1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.12.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:46.658978Z","iopub.status.busy":"2023-06-10T14:45:46.658654Z","iopub.status.idle":"2023-06-10T14:45:46.816548Z","shell.execute_reply":"2023-06-10T14:45:46.815500Z","shell.execute_reply.started":"2023-06-10T14:45:46.658949Z"},"id":"wYwcFK5gixXz","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import re\n","import unicodedata\n","import nltk\n","#from transformers import pipeline\n","from nltk.corpus import stopwords\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\n","from tqdm import tqdm\n","import pickle\n","from sklearn.metrics import confusion_matrix,f1_score,classification_report\n","import matplotlib.pyplot as plt\n","import itertools\n","from sklearn.utils import shuffle\n","from tensorflow.keras import regularizers\n","#from transformers import *\n","from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n","import pandas as pd\n","from transformers import AutoTokenizer, TFAutoModel\n","import numpy as np\n","import gc\n","import math\n","import json\n","import stanza\n","from tensorflow.keras import *\n","import tensorflow as tf\n","from tensorflow.keras import *\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import classification_report\n","from transformers import TFRobertaModel,RobertaTokenizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.initializers import RandomUniform\n","\n","from numpy.random import seed\n","import random as python_random\n","import os\n","import sys\n","\n","np.random.seed(1)\n","python_random.seed(1)\n","tf.random.set_seed(1)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:46.821786Z","iopub.status.busy":"2023-06-10T14:45:46.821439Z","iopub.status.idle":"2023-06-10T14:45:47.103906Z","shell.execute_reply":"2023-06-10T14:45:47.102874Z","shell.execute_reply.started":"2023-06-10T14:45:46.821759Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["# huggingface dataset access token\n","\n","from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:47.107502Z","iopub.status.busy":"2023-06-10T14:45:47.106653Z","iopub.status.idle":"2023-06-10T14:45:47.780766Z","shell.execute_reply":"2023-06-10T14:45:47.779821Z","shell.execute_reply.started":"2023-06-10T14:45:47.107462Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0b5609052cc4ad9a4dae732fbb91457","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# importing datasets\n","\n","from datasets import load_dataset\n","# data = load_dataset(\"maneshkarun/median-3000\")\n","data = load_dataset(\"maneshkarun/median3k_10000s\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:47.783231Z","iopub.status.busy":"2023-06-10T14:45:47.782181Z","iopub.status.idle":"2023-06-10T14:45:47.790208Z","shell.execute_reply":"2023-06-10T14:45:47.789201Z","shell.execute_reply.started":"2023-06-10T14:45:47.783195Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'title', 'hyperpartisan', 'url', 'published_at', 'bias', 'word_count', 'cleaned_data', 'pos_tagged'],\n","        num_rows: 500\n","    })\n","})"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:47.793104Z","iopub.status.busy":"2023-06-10T14:45:47.791925Z","iopub.status.idle":"2023-06-10T14:45:47.799579Z","shell.execute_reply":"2023-06-10T14:45:47.798586Z","shell.execute_reply.started":"2023-06-10T14:45:47.793070Z"},"trusted":true},"outputs":[],"source":["train_data = data['train']"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:47.802589Z","iopub.status.busy":"2023-06-10T14:45:47.801101Z","iopub.status.idle":"2023-06-10T14:45:47.820355Z","shell.execute_reply":"2023-06-10T14:45:47.819505Z","shell.execute_reply.started":"2023-06-10T14:45:47.802563Z"},"trusted":true},"outputs":[],"source":["train_text = train_data['cleaned_data']"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T14:45:47.822679Z","iopub.status.busy":"2023-06-10T14:45:47.822314Z","iopub.status.idle":"2023-06-10T14:45:48.142529Z","shell.execute_reply":"2023-06-10T14:45:48.140589Z","shell.execute_reply.started":"2023-06-10T14:45:47.822646Z"},"id":"2ZinwFiui-A3","outputId":"1a4d3851-73a3-444b-a286-ec608b7c3197","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 0, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 1, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 2, 4, 2, 2, 0, 2, 2, 2, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 0, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 0, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 4, 1, 1, 2, 2, 4, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 2, 4, 2, 3, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 0, 2, 2, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 3, 2, 4, 4, 2, 4, 4, 2, 4, 2, 2, 4, 2, 4, 2, 4, 4, 0, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 2, 2, 2, 4, 2, 1, 2, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 1, 4, 4, 2, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 1, 4, 2, 4, 4, 0, 2, 2, 2, 4, 4, 4, 2, 2, 2, 3, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 4, 1, 4, 4, 0, 4, 4, 4, 4, 4, 4, 3, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 1, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 3, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 4, 4, 4, 3]\n","500\n","Average Length 512.0\n","Found 500 texts.\n"]}],"source":["texts = []\n","labels = []\n","\n","count=0\n","\n","for record in train_data:\n","\n","        count=count+1\n","        new_sen = record['cleaned_data'].split()\n","\n","        if len(new_sen) >= 1024:\n","          new_sen = new_sen[512:1024]\n","        \n","        elif len(new_sen) < 512:\n","          new_sen = new_sen[0:len(new_sen)]\n","        \n","        else:\n","          new_sen = new_sen[-512:]\n","          \n","        new_sen = ' '.join(new_sen)\n","\n","        texts.append(new_sen)\n","        labels.append(record['bias'])\n","   \n","len_list = [len(ele.split()) for ele in texts]\n","\n","print(labels)\n","print(len(labels))\n","\n","res = 0 if len(len_list) == 0 else (float(sum(len_list)) / len(len_list))\n","\n","print(\"Average Length %s\" % res) \n","print('Found %s texts.' % len(texts))"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T14:45:48.144508Z","iopub.status.busy":"2023-06-10T14:45:48.144111Z","iopub.status.idle":"2023-06-10T14:45:48.156413Z","shell.execute_reply":"2023-06-10T14:45:48.155254Z","shell.execute_reply.started":"2023-06-10T14:45:48.144474Z"},"id":"LprCHRM2aWb8","outputId":"3377113e-6825-4977-bf5d-ac9c08887b56","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                  text  label\n","0    it in a fire having dinner with FDR When I was...      4\n","1    they will have to suppress them through violen...      4\n","2    to Bush and fiscal 2010 would be the first yea...      2\n","3    the imposition of modest sanctions and various...      4\n","4    oil industry connections spent 235 million to ...      4\n","..                                                 ...    ...\n","495  Chicago Housing Authority has an obligation to...      3\n","496  the daily slurs insults and humiliations dealt...      4\n","497  they retire By reducing the overall amount pai...      4\n","498  Pen represents For the moment many people say ...      4\n","499  fell mostly into two divergent camps One side ...      3\n","\n","[500 rows x 2 columns]\n"]}],"source":["summarized_data = pd.DataFrame(texts,\n","               columns =['text'])\n","summarized_data['label'] = labels\n","print(summarized_data)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T14:45:48.161428Z","iopub.status.busy":"2023-06-10T14:45:48.160419Z","iopub.status.idle":"2023-06-10T14:45:48.168958Z","shell.execute_reply":"2023-06-10T14:45:48.168223Z","shell.execute_reply.started":"2023-06-10T14:45:48.161377Z"},"id":"VoY1gHZoaZmG","trusted":true},"outputs":[],"source":["def create_model():\n","    inps = Input(shape = (max_len,), dtype='int64')\n","    masks= Input(shape = (max_len,), dtype='int64')\n","    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n","    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n","    dropout_0= Dropout(0.5)(dense_0)\n","    pred = Dense(5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n","    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n","    print(model.summary())\n","    return model   "]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-10T14:45:48.171490Z","iopub.status.busy":"2023-06-10T14:45:48.170746Z","iopub.status.idle":"2023-06-10T15:06:53.723210Z","shell.execute_reply":"2023-06-10T15:06:53.722007Z","shell.execute_reply.started":"2023-06-10T14:45:48.171440Z"},"id":"x9kO4eVwCHKg","outputId":"a3776971-f469-4ae7-dd89-06b3b2630cf1","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 14:46:04 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    33W / 250W |  15919MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 85s 661ms/step - loss: 7.1209 - accuracy: 0.6533 - val_loss: 6.9267 - val_accuracy: 0.7000\n","Epoch 2/5\n","57/57 [==============================] - 34s 605ms/step - loss: 6.4584 - accuracy: 0.8889 - val_loss: 6.3980 - val_accuracy: 0.8800\n","Epoch 3/5\n","57/57 [==============================] - 35s 608ms/step - loss: 6.1658 - accuracy: 0.9178 - val_loss: 6.2717 - val_accuracy: 0.8800\n","Epoch 4/5\n","57/57 [==============================] - 34s 605ms/step - loss: 5.9421 - accuracy: 0.9356 - val_loss: 6.2029 - val_accuracy: 0.9000\n","Epoch 5/5\n","57/57 [==============================] - 33s 583ms/step - loss: 5.7540 - accuracy: 0.9667 - val_loss: 6.0386 - val_accuracy: 0.9000\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 70ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 70ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 62ms/step\n","Accuracy: 0.9\n","Weighted F1: 0.8725098039215686\n","Micro F1: 0.9\n","Weighted Precision: 0.86831541218638\n","Micro Precision: 0.9\n","Weighted Recall: 0.9\n","Micro Recall: 0.9\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 14:50:18 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P0    39W / 250W |  15921MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 86s 694ms/step - loss: 7.0521 - accuracy: 0.6756 - val_loss: 6.8047 - val_accuracy: 0.8000\n","Epoch 2/5\n","57/57 [==============================] - 33s 583ms/step - loss: 6.5277 - accuracy: 0.8578 - val_loss: 6.6096 - val_accuracy: 0.8000\n","Epoch 3/5\n","57/57 [==============================] - 34s 603ms/step - loss: 6.2263 - accuracy: 0.9089 - val_loss: 6.3039 - val_accuracy: 0.8800\n","Epoch 4/5\n","57/57 [==============================] - 33s 583ms/step - loss: 5.9692 - accuracy: 0.9422 - val_loss: 6.1935 - val_accuracy: 0.8800\n","Epoch 5/5\n","57/57 [==============================] - 33s 583ms/step - loss: 5.7900 - accuracy: 0.9600 - val_loss: 6.1998 - val_accuracy: 0.8800\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 69ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 70ms/step\n","1/1 [==============================] - 0s 69ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 105ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","Accuracy: 0.88\n","Weighted F1: 0.8560239282153539\n","Micro F1: 0.88\n","Weighted Precision: 0.8364444444444444\n","Micro Precision: 0.88\n","Weighted Recall: 0.88\n","Micro Recall: 0.88\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 14:54:25 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   60C    P0    39W / 250W |  15925MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 86s 696ms/step - loss: 6.9483 - accuracy: 0.7400 - val_loss: 6.7810 - val_accuracy: 0.7800\n","Epoch 2/5\n","57/57 [==============================] - 34s 599ms/step - loss: 6.4545 - accuracy: 0.8889 - val_loss: 6.4025 - val_accuracy: 0.8400\n","Epoch 3/5\n","57/57 [==============================] - 34s 601ms/step - loss: 6.1796 - accuracy: 0.9267 - val_loss: 6.2801 - val_accuracy: 0.8800\n","Epoch 4/5\n","57/57 [==============================] - 33s 582ms/step - loss: 5.9678 - accuracy: 0.9556 - val_loss: 6.2371 - val_accuracy: 0.8600\n","Epoch 5/5\n","57/57 [==============================] - 34s 605ms/step - loss: 5.7681 - accuracy: 0.9733 - val_loss: 6.1847 - val_accuracy: 0.8800\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 82ms/step\n","1/1 [==============================] - 0s 79ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 69ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 70ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","Accuracy: 0.88\n","Weighted F1: 0.85300051203277\n","Micro F1: 0.88\n","Weighted Precision: 0.8533725490196079\n","Micro Precision: 0.88\n","Weighted Recall: 0.88\n","Micro Recall: 0.88\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 14:58:39 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P0    39W / 250W |  15931MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 86s 696ms/step - loss: 6.9699 - accuracy: 0.7133 - val_loss: 6.8487 - val_accuracy: 0.7000\n","Epoch 2/5\n","57/57 [==============================] - 34s 603ms/step - loss: 6.3921 - accuracy: 0.8933 - val_loss: 6.3571 - val_accuracy: 0.8600\n","Epoch 3/5\n","57/57 [==============================] - 34s 603ms/step - loss: 6.0982 - accuracy: 0.9356 - val_loss: 6.2518 - val_accuracy: 0.9000\n","Epoch 4/5\n","57/57 [==============================] - 34s 605ms/step - loss: 5.8426 - accuracy: 0.9733 - val_loss: 6.3564 - val_accuracy: 0.8600\n","Epoch 5/5\n","57/57 [==============================] - 34s 604ms/step - loss: 5.7018 - accuracy: 0.9667 - val_loss: 5.9750 - val_accuracy: 0.8800\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 120ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 69ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 62ms/step\n","Accuracy: 0.9\n","Weighted F1: 0.8725806451612903\n","Micro F1: 0.9\n","Weighted Precision: 0.8696969696969696\n","Micro Precision: 0.9\n","Weighted Recall: 0.9\n","Micro Recall: 0.9\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n","                                thPoolingAndCrossAt               'input_2[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            2565        ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sat Jun 10 15:02:55 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P0    39W / 250W |  15933MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 86s 693ms/step - loss: 6.9688 - accuracy: 0.7289 - val_loss: 6.8988 - val_accuracy: 0.7000\n","Epoch 2/5\n","57/57 [==============================] - 34s 602ms/step - loss: 6.4230 - accuracy: 0.8867 - val_loss: 6.4702 - val_accuracy: 0.8400\n","Epoch 3/5\n","57/57 [==============================] - 36s 627ms/step - loss: 6.1592 - accuracy: 0.9222 - val_loss: 6.2685 - val_accuracy: 0.8600\n","Epoch 4/5\n","57/57 [==============================] - 36s 625ms/step - loss: 5.9462 - accuracy: 0.9578 - val_loss: 6.1895 - val_accuracy: 0.9000\n","Epoch 5/5\n","57/57 [==============================] - 35s 609ms/step - loss: 5.7385 - accuracy: 0.9711 - val_loss: 6.2216 - val_accuracy: 0.8600\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n","                                thPoolingAndCrossAt               'input_4[0][0]']                \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 5)            2565        ['dropout_38[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,878,533\n","Trainable params: 109,878,533\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 61ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 65ms/step\n","1/1 [==============================] - 0s 63ms/step\n","1/1 [==============================] - 0s 64ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 62ms/step\n","1/1 [==============================] - 0s 64ms/step\n","Accuracy: 0.9\n","Weighted F1: 0.8736456733230926\n","Micro F1: 0.9\n","Weighted Precision: 0.8747058823529411\n","Micro Precision: 0.9\n","Weighted Recall: 0.9\n","Micro Recall: 0.9\n","Average Accuracy: 0.892\n","Average Weighted F1: 0.8655521125308152\n","Average Micro F1: 0.892\n","Average Weighted Precision: 0.8605070515400687\n","Average Micro Precision: 0.892\n","Average Weighted Recall: 0.892\n","Average Micro Recall: 0.892\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","total_accuracy=0\n","total_weighted_f1=0\n","total_micro_f1=0\n","total_weighted_precision=0\n","total_micro_precision=0\n","total_weighted_recall=0\n","total_micro_recall=0\n","\n","for i in range(5):\n","  gc.collect()\n","  tf.keras.backend.clear_session()\n","  dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","  dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n","  max_len=512\n","  sentences=summarized_data['text']\n","  labels=summarized_data['label']\n","  len(sentences),len(labels)\n","  model_0=create_model()\n","  input_ids=[]\n","  attention_masks=[]\n","\n","  for sent in sentences:\n","    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n","    input_ids.append(dbert_inps['input_ids'])\n","    attention_masks.append(dbert_inps['attention_mask'])\n","  input_ids=np.asarray(input_ids)\n","\n","  attention_masks=np.array(attention_masks)\n","  labels=np.array(labels)\n","  train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.1,random_state=42)\n","  log_dir='dbert_model'\n","\n","  model_save_path='./drive/MyDrive/Best-512/best-512-0-512'+str(i)+'-15labels.h5'\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","  accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n","  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n","  model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","  gpu_info = !nvidia-smi\n","  gpu_info = '\\n'.join(gpu_info)\n","  if gpu_info.find('failed') >= 0:\n","    print('Not connected to a GPU')\n","  else:\n","    print(gpu_info)\n","  history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n","  pred_labels=[]\n","\n","  model_saved= create_model()\n","  model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n","  model_saved.load_weights('./drive/MyDrive/Best-512/best-512-0-512'+str(i)+'-15labels.h5')\n","\n","  for i in range(0,len(val_inp)):\n","    pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n","    pred_label = pred.argmax(axis=1)\n","    pred_labels.append(pred_label)\n","  accuracy=accuracy_score(val_label, pred_labels)\n","  print(\"Accuracy: \"+str(accuracy))\n","  total_accuracy=total_accuracy+accuracy\n","  \n","  weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n","  print(\"Weighted F1: \"+ str(weighted_f1))\n","  total_weighted_f1=total_weighted_f1+weighted_f1\n","  micro_f1=f1_score(val_label,pred_labels, average='micro')\n","  print(\"Micro F1: \"+ str(micro_f1))\n","  total_micro_f1=total_micro_f1+micro_f1\n","\n","  weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n","  print(\"Weighted Precision: \" + str(weighted_precision))\n","  total_weighted_precision=total_weighted_precision+weighted_precision\n","  micro_precision=precision_score(val_label, pred_labels, average='micro')\n","  print(\"Micro Precision: \" + str(micro_precision))\n","  total_micro_precision=total_micro_precision+micro_precision\n","\n","  weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n","  print(\"Weighted Recall: \" + str(weighted_recall))\n","  total_weighted_recall=total_weighted_recall+weighted_recall\n","  micro_recall=recall_score(val_label, pred_labels, average='micro')\n","  print(\"Micro Recall: \" + str(micro_recall))\n","  total_micro_recall=total_micro_recall+micro_recall\n","\n","\n","print(\"Average Accuracy: \"+str(total_accuracy/5))\n","print(\"Average Weighted F1: \"+str(total_weighted_f1/5))\n","print(\"Average Micro F1: \"+str(total_micro_f1/5))\n","print(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\n","print(\"Average Micro Precision: \"+str(total_micro_precision/5))\n","print(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\n","print(\"Average Micro Recall: \"+str(total_micro_recall/5))"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Best-512_0:512_15labels.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
