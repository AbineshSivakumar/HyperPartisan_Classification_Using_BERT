{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers sentencepiece tensorflow stanza tensorflow-addons nltk textacy datasets ipywidgets","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:43:34.438055Z","iopub.execute_input":"2023-06-11T07:43:34.438847Z","iopub.status.idle":"2023-06-11T07:43:50.171928Z","shell.execute_reply.started":"2023-06-11T07:43:34.438808Z","shell.execute_reply":"2023-06-11T07:43:50.170745Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.29.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\nRequirement already satisfied: stanza in /opt/conda/lib/python3.10/site-packages (1.5.0)\nRequirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (0.20.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: textacy in /opt/conda/lib/python3.10/site-packages (0.13.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.3.3)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.10)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.5.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.31.0)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.2.0)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.0.0)\nRequirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\nRequirement already satisfied: cachetools>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (4.2.4)\nRequirement already satisfied: catalogue~=2.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (2.0.8)\nRequirement already satisfied: cytoolz>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from textacy) (0.12.0)\nRequirement already satisfied: floret~=0.10.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (0.10.3)\nRequirement already satisfied: jellyfish>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (0.11.2)\nRequirement already satisfied: joblib>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.2.0)\nRequirement already satisfied: networkx>=2.7 in /opt/conda/lib/python3.10/site-packages (from textacy) (3.1)\nRequirement already satisfied: pyphen>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (0.14.0)\nRequirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.10.1)\nRequirement already satisfied: scikit-learn>=1.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.2.2)\nRequirement already satisfied: spacy~=3.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (3.5.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (10.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (6.23.0)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.6.4)\nRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.13.2)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.0.7)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\nRequirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.3)\nRequirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.1)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.2)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.38)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (2.15.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.6.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0->textacy) (3.1.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (8.1.10)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.1.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (2.4.6)\nRequirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (0.7.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (0.10.1)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (6.3.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.10.7)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.1.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.3.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.12)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.5.0)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.8.0)\nRequirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.4.5)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.17.1)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.16.0)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.0.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=4.0.0->ipywidgets) (0.2.6)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.0.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy~=3.0->textacy) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.5.0)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.3)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.0.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.13)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.16.3)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.17.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.19.3)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.6.2)\nRequirement already satisfied: jupyter-events>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.3)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4.4)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\nRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.3.2.post1)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.4.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.7)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.4.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.4.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"### LIBRARIES","metadata":{}},{"cell_type":"code","source":"! pip install stanza\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport re\nimport unicodedata\nimport nltk\n\n#from transformers import pipeline\nfrom nltk.corpus import stopwords\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization\nfrom tqdm import tqdm\nimport pickle\nfrom sklearn.metrics import confusion_matrix,f1_score,classification_report\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras import regularizers\n\n#from transformers import *\nfrom transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig, TFRobertaModel,RobertaTokenizer\nimport pandas as pd\nfrom transformers import AutoTokenizer, TFAutoModel\nimport numpy as np\nimport gc\nimport math\nimport json\nimport stanza\nfrom tensorflow.keras import *\nimport tensorflow as tf\nfrom tensorflow.keras import *\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom transformers import TFRobertaModel,RobertaTokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.initializers import RandomUniform\nfrom transformers import LongformerTokenizer, TFLongformerModel\nfrom numpy.random import seed\nimport random as python_random\nimport os\nimport sys\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(1)\npython_random.seed(1)\ntf.random.set_seed(1)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:43:50.181936Z","iopub.execute_input":"2023-06-11T07:43:50.182631Z","iopub.status.idle":"2023-06-11T07:44:14.501982Z","shell.execute_reply.started":"2023-06-11T07:43:50.182590Z","shell.execute_reply":"2023-06-11T07:44:14.500960Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: stanza in /opt/conda/lib/python3.10/site-packages (1.5.0)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.23.5)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.28.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from stanza) (1.16.0)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### DATASET","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom huggingface_hub import login","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:44:14.515534Z","iopub.execute_input":"2023-06-11T07:44:14.516129Z","iopub.status.idle":"2023-06-11T07:44:14.908373Z","shell.execute_reply.started":"2023-06-11T07:44:14.516093Z","shell.execute_reply":"2023-06-11T07:44:14.907430Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Access token: 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","metadata":{}},{"cell_type":"code","source":"login()","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:44:14.914155Z","iopub.execute_input":"2023-06-11T07:44:14.914860Z","iopub.status.idle":"2023-06-11T07:44:14.944261Z","shell.execute_reply.started":"2023-06-11T07:44:14.914809Z","shell.execute_reply":"2023-06-11T07:44:14.943386Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e44de5f5f3124ddca2ca2bf553eea677"}},"metadata":{}}]},{"cell_type":"code","source":"data = load_dataset(\"maneshkarun/median-3000\")\ndata","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:44:14.952393Z","iopub.execute_input":"2023-06-11T07:44:14.953089Z","iopub.status.idle":"2023-06-11T07:44:15.902016Z","shell.execute_reply.started":"2023-06-11T07:44:14.953050Z","shell.execute_reply":"2023-06-11T07:44:15.901052Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f250987ed504635bcdee1e2aace7603"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'title', 'hyperpartisan', 'url', 'published_at', 'bias', 'word_count', 'cleaned_data', 'pos_tagged'],\n        num_rows: 500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"texts_train = data['train']['cleaned_data']\nlabels_train = data['train']['hyperpartisan']\npos_tagged_train = data['train']['pos_tagged']","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:44:15.906786Z","iopub.execute_input":"2023-06-11T07:44:15.907525Z","iopub.status.idle":"2023-06-11T07:44:16.054630Z","shell.execute_reply.started":"2023-06-11T07:44:15.907490Z","shell.execute_reply":"2023-06-11T07:44:16.053471Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"texts = []  # list of text samples\nlabels = []  # list of label ids","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:44:16.059787Z","iopub.execute_input":"2023-06-11T07:44:16.060518Z","iopub.status.idle":"2023-06-11T07:44:16.066036Z","shell.execute_reply.started":"2023-06-11T07:44:16.060483Z","shell.execute_reply":"2023-06-11T07:44:16.064759Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"summarized_data = pd.DataFrame(pos_tagged_train,\n               columns =['text'])\nsummarized_data['label'] = labels_train\nprint(summarized_data)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:44:16.071591Z","iopub.execute_input":"2023-06-11T07:44:16.073051Z","iopub.status.idle":"2023-06-11T07:44:16.095827Z","shell.execute_reply.started":"2023-06-11T07:44:16.073017Z","shell.execute_reply":"2023-06-11T07:44:16.094846Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"                                                  text  label\n0    David/PROPN Cay/PROPN Johnston/PROPN was/AUX c...   True\n1    Max/PROPN Blumenthal/PROPN is/AUX an/DET award...   True\n2    Is/AUX President/PROPN Obama8217s/PROPN spendi...  False\n3    A/DET group/NOUN of/ADP Malaysian/ADJ people/N...   True\n4    Advertise/VERB on/ADP MotherJonescom/PROPN Ove...   True\n..                                                 ...    ...\n495  Jessica/PROPN Moore/PROPN showed/VERB up/ADP a...  False\n496  This/PRON is/AUX the/DET second/ADJ part/NOUN ...   True\n497  Like/ADP many/ADJ countries/NOUN trying/VERB t...   True\n498  Whether/SCONJ the/DET fascist/ADJ Marine/PROPN...   True\n499  Standing/VERB at/ADP the/DET front/ADJ door/NO...  False\n\n[500 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"cleaned_data = data['train']['cleaned_data']","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:44:16.100985Z","iopub.execute_input":"2023-06-11T07:44:16.101687Z","iopub.status.idle":"2023-06-11T07:44:16.121138Z","shell.execute_reply.started":"2023-06-11T07:44:16.101652Z","shell.execute_reply":"2023-06-11T07:44:16.120017Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"summarized_data2 = pd.DataFrame(cleaned_data,\n               columns =['text'])\nsummarized_data2['label'] = labels_train\nprint(summarized_data)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:44:16.127057Z","iopub.execute_input":"2023-06-11T07:44:16.127938Z","iopub.status.idle":"2023-06-11T07:44:16.140370Z","shell.execute_reply.started":"2023-06-11T07:44:16.127903Z","shell.execute_reply":"2023-06-11T07:44:16.139287Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"                                                  text  label\n0    David/PROPN Cay/PROPN Johnston/PROPN was/AUX c...   True\n1    Max/PROPN Blumenthal/PROPN is/AUX an/DET award...   True\n2    Is/AUX President/PROPN Obama8217s/PROPN spendi...  False\n3    A/DET group/NOUN of/ADP Malaysian/ADJ people/N...   True\n4    Advertise/VERB on/ADP MotherJonescom/PROPN Ove...   True\n..                                                 ...    ...\n495  Jessica/PROPN Moore/PROPN showed/VERB up/ADP a...  False\n496  This/PRON is/AUX the/DET second/ADJ part/NOUN ...   True\n497  Like/ADP many/ADJ countries/NOUN trying/VERB t...   True\n498  Whether/SCONJ the/DET fascist/ADJ Marine/PROPN...   True\n499  Standing/VERB at/ADP the/DET front/ADJ door/NO...  False\n\n[500 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import Concatenate\n\ndef create_model():\n    inps = Input(shape = (max_len,), dtype='int64')\n    masks= Input(shape = (max_len,), dtype='int64')\n    longformer_output = longformer_model(inps, attention_mask=masks)\n    sequence_output = longformer_output[0][:,0,:]\n    pooler_output = longformer_output[1]\n\n    # You can combine the sequence_output and pooler_output in various ways depending\n    # on your use case. Here's an example where we concatenate them:\n    combined_output = Concatenate()([sequence_output, pooler_output])\n\n    # ... then use the combined_output in your model\n    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(combined_output)\n    dropout_0= Dropout(0.5)(dense_0)\n\n    pred = Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n    print(model.summary())\n    return model \n\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\ntotal_accuracy=0\ntotal_weighted_f1=0\ntotal_micro_f1=0\ntotal_weighted_precision=0\ntotal_micro_precision=0\ntotal_weighted_recall=0\ntotal_micro_recall=0\n\nfor i in range(5):\n    gc.collect()\n    tf.keras.backend.clear_session()\n    longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n    longformer_model = TFLongformerModel.from_pretrained('allenai/longformer-base-4096')\n    max_len=512\n    sentences=summarized_data['text']\n    labels=summarized_data['label']\n    len(sentences),len(labels)\n    model_0=create_model()\n    input_ids=[]\n    attention_masks=[]\n    \n    for sent in sentences:\n        longformer_inps=longformer_tokenizer.encode_plus(sent,add_special_tokens = True,  max_length =max_len,pad_to_max_length = True,return_attention_mask = True, truncation=True)\n        input_ids.append(longformer_inps['input_ids'])\n        attention_masks.append(longformer_inps['attention_mask'])\n    input_ids=np.asarray(input_ids)\n\n    attention_masks=np.array(attention_masks)\n    labels=np.array(labels)\n    \n\n    train_val_inp, test_inp, train_val_label, test_label, train_val_mask, test_mask = train_test_split(input_ids, labels, attention_masks, test_size=0.2, random_state=42)\n    train_inp, val_inp, train_label, val_label, train_mask, val_mask = train_test_split(train_val_inp, train_val_label, train_val_mask, test_size=0.25, random_state=42)\n    log_dir='dbert_model'\n\n    model_save_path = './kaggle/working/roberta-best-512-0-512-' + str(i) + '-4labels.h5'\n    \n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n    accuracy = tf.keras.metrics.BinaryAccuracy()\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,  monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),    keras.callbacks.TensorBoard(log_dir=log_dir)]\n    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n#     gpu_info = !nvidia-smi\n#     gpu_info = '\\n'.join(gpu_info)\n#     if gpu_info.find('failed') >= 0:\n#           print('Not connected to a GPU')\n#     else:\n#           print(gpu_info)\n\n    pred_labels=[]\n    # Fit the model\n    history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5, validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n    # Save the weights of the trained model\n    model_0.save_weights(model_save_path)\n\n    # Create a new model with the same structure\n    model_saved = create_model()\n\n    # Compile the new model\n    model_saved.compile(loss=loss, optimizer=optimizer, metrics=[accuracy])\n\n    # Load the weights of the trained model into the new model\n    model_saved.load_weights(model_save_path)\n\n    # for loop starts here\n    for i in range(0,len(val_inp)):\n        pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n        pred_label = (pred > 0.5).astype(\"int32\")\n        pred_labels.extend(pred_label.flatten())  # Flatten the array and append it to the list\n\n    pred_labels = np.array(pred_labels)  # Convert list to numpy array\n\n    accuracy=accuracy_score(val_label, pred_labels)\n    print(\"Accuracy: \"+str(accuracy))\n    total_accuracy=total_accuracy+accuracy\n    \n    weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n    print(\"Weighted F1: \"+ str(weighted_f1))\n    total_weighted_f1=total_weighted_f1+weighted_f1\n    micro_f1=f1_score(val_label,pred_labels, average='micro')\n    print(\"Micro F1: \"+ str(micro_f1))\n    total_micro_f1=total_micro_f1+micro_f1\n\n    weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n    print(\"Weighted Precision: \" + str(weighted_precision))\n    total_weighted_precision=total_weighted_precision+weighted_precision\n    micro_precision=precision_score(val_label, pred_labels, average='micro')\n    print(\"Micro Precision: \" + str(micro_precision))\n    total_micro_precision=total_micro_precision+micro_precision\n\n    weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n    print(\"Weighted Recall: \" + str(weighted_recall))\n    total_weighted_recall=total_weighted_recall+weighted_recall\n    micro_recall=recall_score(val_label, pred_labels, average='micro')\n    print(\"Micro Recall: \" + str(micro_recall))\n    total_micro_recall=total_micro_recall+micro_recall\n\n\nprint(\"Average Accuracy: \"+str(total_accuracy/5))\nprint(\"Average Weighted F1: \"+str(total_weighted_f1/5))\nprint(\"Average Micro F1: \"+str(total_micro_f1/5))\nprint(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\nprint(\"Average Micro Precision: \"+str(total_micro_precision/5))\nprint(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\nprint(\"Average Micro Recall: \"+str(total_micro_recall/5))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Concatenate\n\ndef create_model():\n    inps = Input(shape = (max_len,), dtype='int64')\n    masks= Input(shape = (max_len,), dtype='int64')\n    longformer_output = longformer_model(inps, attention_mask=masks)\n    sequence_output = longformer_output[0][:,0,:]\n    pooler_output = longformer_output[1]\n\n    # You can combine the sequence_output and pooler_output in various ways depending\n    # on your use case. Here's an example where we concatenate them:\n    combined_output = Concatenate()([sequence_output, pooler_output])\n\n    # ... then use the combined_output in your model\n    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(combined_output)\n    dropout_0= Dropout(0.5)(dense_0)\n\n    pred = Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n    print(model.summary())\n    return model \n\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\ntotal_accuracy=0\ntotal_weighted_f1=0\ntotal_micro_f1=0\ntotal_weighted_precision=0\ntotal_micro_precision=0\ntotal_weighted_recall=0\ntotal_micro_recall=0\n\nfor i in range(5):\n    gc.collect()\n    tf.keras.backend.clear_session()\n    longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n    longformer_model = TFLongformerModel.from_pretrained('allenai/longformer-base-4096')\n    max_len=512\n    sentences=summarized_data2['text']\n    labels=summarized_data2['label']\n    len(sentences),len(labels)\n    model_0=create_model()\n    input_ids=[]\n    attention_masks=[]\n    \n    for sent in sentences:\n        longformer_inps=longformer_tokenizer.encode_plus(sent,add_special_tokens = True,  max_length =max_len,pad_to_max_length = True,return_attention_mask = True, truncation=True)\n        input_ids.append(longformer_inps['input_ids'])\n        attention_masks.append(longformer_inps['attention_mask'])\n    input_ids=np.asarray(input_ids)\n\n    attention_masks=np.array(attention_masks)\n    labels=np.array(labels)\n    \n\n    train_val_inp, test_inp, train_val_label, test_label, train_val_mask, test_mask = train_test_split(input_ids, labels, attention_masks, test_size=0.2, random_state=42)\n    train_inp, val_inp, train_label, val_label, train_mask, val_mask = train_test_split(train_val_inp, train_val_label, train_val_mask, test_size=0.25, random_state=42)\n    log_dir='dbert_model'\n\n    model_save_path = './kaggle/working/roberta-best-512-0-512-' + str(i) + '-4labels.h5'\n    \n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n    accuracy = tf.keras.metrics.BinaryAccuracy()\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,  monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),    keras.callbacks.TensorBoard(log_dir=log_dir)]\n    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n    gpu_info = !nvidia-smi\n    gpu_info = '\\n'.join(gpu_info)\n    if gpu_info.find('failed') >= 0:\n          print('Not connected to a GPU')\n    else:\n          print(gpu_info)\n\n    pred_labels=[]\n    # Fit the model\n    history=model_0.fit([train_inp,train_mask],train_label,batch_size=8,epochs=5, validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n    # Save the weights of the trained model\n    model_0.save_weights(model_save_path)\n\n    # Create a new model with the same structure\n    model_saved = create_model()\n\n    # Compile the new model\n    model_saved.compile(loss=loss, optimizer=optimizer, metrics=[accuracy])\n\n    # Load the weights of the trained model into the new model\n    model_saved.load_weights(model_save_path)\n\n    # for loop starts here\n    for i in range(0,len(val_inp)):\n        pred=model_saved.predict([val_inp[i].reshape(1,512),val_mask[i].reshape(1,512)])\n        pred_label = (pred > 0.5).astype(\"int32\")\n        pred_labels.extend(pred_label.flatten())  # Flatten the array and append it to the list\n\n    pred_labels = np.array(pred_labels)  # Convert list to numpy array\n\n    accuracy=accuracy_score(val_label, pred_labels)\n    print(\"Accuracy: \"+str(accuracy))\n    total_accuracy=total_accuracy+accuracy\n    \n    weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n    print(\"Weighted F1: \"+ str(weighted_f1))\n    total_weighted_f1=total_weighted_f1+weighted_f1\n    micro_f1=f1_score(val_label,pred_labels, average='micro')\n    print(\"Micro F1: \"+ str(micro_f1))\n    total_micro_f1=total_micro_f1+micro_f1\n\n    weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n    print(\"Weighted Precision: \" + str(weighted_precision))\n    total_weighted_precision=total_weighted_precision+weighted_precision\n    micro_precision=precision_score(val_label, pred_labels, average='micro')\n    print(\"Micro Precision: \" + str(micro_precision))\n    total_micro_precision=total_micro_precision+micro_precision\n\n    weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n    print(\"Weighted Recall: \" + str(weighted_recall))\n    total_weighted_recall=total_weighted_recall+weighted_recall\n    micro_recall=recall_score(val_label, pred_labels, average='micro')\n    print(\"Micro Recall: \" + str(micro_recall))\n    total_micro_recall=total_micro_recall+micro_recall\n\n\nprint(\"Average Accuracy: \"+str(total_accuracy/5))\nprint(\"Average Weighted F1: \"+str(total_weighted_f1/5))\nprint(\"Average Micro F1: \"+str(total_micro_f1/5))\nprint(\"Average Weighted Precision: \"+str(total_weighted_precision/5))\nprint(\"Average Micro Precision: \"+str(total_micro_precision/5))\nprint(\"Average Weighted Recall: \"+str(total_weighted_recall/5))\nprint(\"Average Micro Recall: \"+str(total_micro_recall/5))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-11T07:44:16.142012Z","iopub.execute_input":"2023-06-11T07:44:16.142671Z","iopub.status.idle":"2023-06-11T08:27:32.486653Z","shell.execute_reply.started":"2023-06-11T07:44:16.142637Z","shell.execute_reply":"2023-06-11T08:27:32.485474Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_1[0][0]',                \n merModel)                      elOutputWithPooling               'input_2[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_longformer_model[0][0]']    \n ingOpLambda)                                                                                     \n                                                                                                  \n concatenate (Concatenate)      (None, 1536)         0           ['tf.__operators__.getitem[0][0]'\n                                                                 , 'tf_longformer_model[0][1]']   \n                                                                                                  \n dense (Dense)                  (None, 512)          786944      ['concatenate[0][0]']            \n                                                                                                  \n dropout_49 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 1)            513         ['dropout_49[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nSun Jun 11 07:45:11 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   45C    P0    38W / 250W |  15857MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n38/38 [==============================] - 208s 2s/step - loss: 8.2697 - binary_accuracy: 0.6400 - val_loss: 7.8585 - val_binary_accuracy: 0.9200\nEpoch 2/5\n38/38 [==============================] - 43s 1s/step - loss: 7.7527 - binary_accuracy: 0.9433 - val_loss: 7.7123 - val_binary_accuracy: 0.9200\nEpoch 3/5\n38/38 [==============================] - 43s 1s/step - loss: 7.6015 - binary_accuracy: 0.9467 - val_loss: 7.6415 - val_binary_accuracy: 0.9200\nEpoch 4/5\n38/38 [==============================] - 48s 1s/step - loss: 7.4433 - binary_accuracy: 0.9633 - val_loss: 7.4552 - val_binary_accuracy: 0.9500\nEpoch 5/5\n38/38 [==============================] - 43s 1s/step - loss: 7.2996 - binary_accuracy: 0.9900 - val_loss: 7.4432 - val_binary_accuracy: 0.9300\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n merModel)                      elOutputWithPooling               'input_4[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[1][0]']    \n icingOpLambda)                                                                                   \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 1536)         0           ['tf.__operators__.getitem_1[0][0\n                                                                 ]',                              \n                                                                  'tf_longformer_model[1][1]']    \n                                                                                                  \n dense_2 (Dense)                (None, 512)          786944      ['concatenate_1[0][0]']          \n                                                                                                  \n dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 1)            513         ['dropout_50[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 25s 25s/step\n1/1 [==============================] - 0s 185ms/step\n1/1 [==============================] - 0s 163ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 197ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 125ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 127ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 109ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 155ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 109ms/step\nAccuracy: 0.93\nWeighted F1: 0.9298947368421053\nMicro F1: 0.93\nWeighted Precision: 0.9314734299516908\nMicro Precision: 0.93\nWeighted Recall: 0.93\nMicro Recall: 0.93\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_1[0][0]',                \n merModel)                      elOutputWithPooling               'input_2[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_longformer_model[0][0]']    \n ingOpLambda)                                                                                     \n                                                                                                  \n concatenate (Concatenate)      (None, 1536)         0           ['tf.__operators__.getitem[0][0]'\n                                                                 , 'tf_longformer_model[0][1]']   \n                                                                                                  \n dense (Dense)                  (None, 512)          786944      ['concatenate[0][0]']            \n                                                                                                  \n dropout_49 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 1)            513         ['dropout_49[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nSun Jun 11 07:53:18 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   53C    P0    41W / 250W |  15903MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n38/38 [==============================] - 201s 2s/step - loss: 8.2761 - binary_accuracy: 0.6300 - val_loss: 8.0855 - val_binary_accuracy: 0.8900\nEpoch 2/5\n38/38 [==============================] - 42s 1s/step - loss: 7.8870 - binary_accuracy: 0.8900 - val_loss: 7.6840 - val_binary_accuracy: 0.9300\nEpoch 3/5\n38/38 [==============================] - 43s 1s/step - loss: 7.6234 - binary_accuracy: 0.9433 - val_loss: 7.5254 - val_binary_accuracy: 0.9400\nEpoch 4/5\n38/38 [==============================] - 43s 1s/step - loss: 7.4892 - binary_accuracy: 0.9367 - val_loss: 7.4651 - val_binary_accuracy: 0.9400\nEpoch 5/5\n38/38 [==============================] - 42s 1s/step - loss: 7.3340 - binary_accuracy: 0.9633 - val_loss: 7.3151 - val_binary_accuracy: 0.9400\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n merModel)                      elOutputWithPooling               'input_4[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[1][0]']    \n icingOpLambda)                                                                                   \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 1536)         0           ['tf.__operators__.getitem_1[0][0\n                                                                 ]',                              \n                                                                  'tf_longformer_model[1][1]']    \n                                                                                                  \n dense_2 (Dense)                (None, 512)          786944      ['concatenate_1[0][0]']          \n                                                                                                  \n dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 1)            513         ['dropout_50[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 26s 26s/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 124ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 186ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 109ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 194ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 108ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 117ms/step\nAccuracy: 0.94\nWeighted F1: 0.9399519230769232\nMicro F1: 0.94\nWeighted Precision: 0.9406583701324769\nMicro Precision: 0.94\nWeighted Recall: 0.94\nMicro Recall: 0.94\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_1[0][0]',                \n merModel)                      elOutputWithPooling               'input_2[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_longformer_model[0][0]']    \n ingOpLambda)                                                                                     \n                                                                                                  \n concatenate (Concatenate)      (None, 1536)         0           ['tf.__operators__.getitem[0][0]'\n                                                                 , 'tf_longformer_model[0][1]']   \n                                                                                                  \n dense (Dense)                  (None, 512)          786944      ['concatenate[0][0]']            \n                                                                                                  \n dropout_49 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 1)            513         ['dropout_49[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nSun Jun 11 08:01:26 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   52C    P0    40W / 250W |  15909MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n38/38 [==============================] - 206s 2s/step - loss: 8.2590 - binary_accuracy: 0.6467 - val_loss: 7.8888 - val_binary_accuracy: 0.8900\nEpoch 2/5\n38/38 [==============================] - 43s 1s/step - loss: 7.7765 - binary_accuracy: 0.9267 - val_loss: 7.6516 - val_binary_accuracy: 0.9300\nEpoch 3/5\n38/38 [==============================] - 42s 1s/step - loss: 7.5789 - binary_accuracy: 0.9567 - val_loss: 7.5495 - val_binary_accuracy: 0.9300\nEpoch 4/5\n38/38 [==============================] - 42s 1s/step - loss: 7.4531 - binary_accuracy: 0.9600 - val_loss: 7.4864 - val_binary_accuracy: 0.9400\nEpoch 5/5\n38/38 [==============================] - 43s 1s/step - loss: 7.2809 - binary_accuracy: 0.9800 - val_loss: 7.3505 - val_binary_accuracy: 0.9500\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n merModel)                      elOutputWithPooling               'input_4[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[1][0]']    \n icingOpLambda)                                                                                   \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 1536)         0           ['tf.__operators__.getitem_1[0][0\n                                                                 ]',                              \n                                                                  'tf_longformer_model[1][1]']    \n                                                                                                  \n dense_2 (Dense)                (None, 512)          786944      ['concatenate_1[0][0]']          \n                                                                                                  \n dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 1)            513         ['dropout_50[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 25s 25s/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 109ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 148ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 127ms/step\n1/1 [==============================] - 0s 124ms/step\n1/1 [==============================] - 0s 124ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 108ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 108ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 111ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 213ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 109ms/step\n1/1 [==============================] - 0s 111ms/step\nAccuracy: 0.95\nWeighted F1: 0.9499849864878391\nMicro F1: 0.9500000000000001\nWeighted Precision: 0.9501602564102564\nMicro Precision: 0.95\nWeighted Recall: 0.95\nMicro Recall: 0.95\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_1[0][0]',                \n merModel)                      elOutputWithPooling               'input_2[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_longformer_model[0][0]']    \n ingOpLambda)                                                                                     \n                                                                                                  \n concatenate (Concatenate)      (None, 1536)         0           ['tf.__operators__.getitem[0][0]'\n                                                                 , 'tf_longformer_model[0][1]']   \n                                                                                                  \n dense (Dense)                  (None, 512)          786944      ['concatenate[0][0]']            \n                                                                                                  \n dropout_49 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 1)            513         ['dropout_49[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nSun Jun 11 08:10:19 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   48C    P0    39W / 250W |  15913MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n38/38 [==============================] - 203s 2s/step - loss: 8.3340 - binary_accuracy: 0.5900 - val_loss: 8.0301 - val_binary_accuracy: 0.9000\nEpoch 2/5\n38/38 [==============================] - 43s 1s/step - loss: 7.8427 - binary_accuracy: 0.9233 - val_loss: 7.7447 - val_binary_accuracy: 0.9100\nEpoch 3/5\n38/38 [==============================] - 43s 1s/step - loss: 7.6412 - binary_accuracy: 0.9400 - val_loss: 7.6143 - val_binary_accuracy: 0.9100\nEpoch 4/5\n38/38 [==============================] - 42s 1s/step - loss: 7.4836 - binary_accuracy: 0.9600 - val_loss: 7.5690 - val_binary_accuracy: 0.9000\nEpoch 5/5\n38/38 [==============================] - 48s 1s/step - loss: 7.3236 - binary_accuracy: 0.9733 - val_loss: 7.4706 - val_binary_accuracy: 0.9100\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n merModel)                      elOutputWithPooling               'input_4[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[1][0]']    \n icingOpLambda)                                                                                   \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 1536)         0           ['tf.__operators__.getitem_1[0][0\n                                                                 ]',                              \n                                                                  'tf_longformer_model[1][1]']    \n                                                                                                  \n dense_2 (Dense)                (None, 512)          786944      ['concatenate_1[0][0]']          \n                                                                                                  \n dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 1)            513         ['dropout_50[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 26s 26s/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 125ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 124ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 124ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 124ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 211ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 128ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 127ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 211ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 123ms/step\nAccuracy: 0.91\nWeighted F1: 0.9099729756781102\nMicro F1: 0.91\nWeighted Precision: 0.9101282051282051\nMicro Precision: 0.91\nWeighted Recall: 0.91\nMicro Recall: 0.91\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_1[0][0]',                \n merModel)                      elOutputWithPooling               'input_2[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_longformer_model[0][0]']    \n ingOpLambda)                                                                                     \n                                                                                                  \n concatenate (Concatenate)      (None, 1536)         0           ['tf.__operators__.getitem[0][0]'\n                                                                 , 'tf_longformer_model[0][1]']   \n                                                                                                  \n dense (Dense)                  (None, 512)          786944      ['concatenate[0][0]']            \n                                                                                                  \n dropout_49 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 1)            513         ['dropout_49[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nSun Jun 11 08:19:20 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   44C    P0    37W / 250W |  15915MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nEpoch 1/5\n38/38 [==============================] - 208s 2s/step - loss: 8.2181 - binary_accuracy: 0.6867 - val_loss: 7.8339 - val_binary_accuracy: 0.9300\nEpoch 2/5\n38/38 [==============================] - 43s 1s/step - loss: 7.7616 - binary_accuracy: 0.9467 - val_loss: 7.8371 - val_binary_accuracy: 0.8600\nEpoch 3/5\n38/38 [==============================] - 43s 1s/step - loss: 7.6220 - binary_accuracy: 0.9400 - val_loss: 7.5973 - val_binary_accuracy: 0.9200\nEpoch 4/5\n38/38 [==============================] - 42s 1s/step - loss: 7.4629 - binary_accuracy: 0.9567 - val_loss: 7.5359 - val_binary_accuracy: 0.9300\nEpoch 5/5\n38/38 [==============================] - 43s 1s/step - loss: 7.3133 - binary_accuracy: 0.9833 - val_loss: 7.4927 - val_binary_accuracy: 0.9300\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_3 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n input_4 (InputLayer)           [(None, 512)]        0           []                               \n                                                                                                  \n tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['input_3[0][0]',                \n merModel)                      elOutputWithPooling               'input_4[0][0]']                \n                                (last_hidden_state=                                               \n                                (None, 512, 768),                                                 \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 hidden_states=None                                               \n                                , attentions=None,                                                \n                                global_attentions=N                                               \n                                one)                                                              \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_longformer_model[1][0]']    \n icingOpLambda)                                                                                   \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 1536)         0           ['tf.__operators__.getitem_1[0][0\n                                                                 ]',                              \n                                                                  'tf_longformer_model[1][1]']    \n                                                                                                  \n dense_2 (Dense)                (None, 512)          786944      ['concatenate_1[0][0]']          \n                                                                                                  \n dropout_50 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 1)            513         ['dropout_50[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 149,446,913\nTrainable params: 149,446,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n1/1 [==============================] - 26s 26s/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 129ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 127ms/step\n1/1 [==============================] - 0s 130ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 175ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 127ms/step\n1/1 [==============================] - 0s 126ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 124ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 129ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 131ms/step\n1/1 [==============================] - 0s 119ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 112ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 125ms/step\n1/1 [==============================] - 0s 122ms/step\n1/1 [==============================] - 0s 123ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 116ms/step\n1/1 [==============================] - 0s 155ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 121ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 129ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 115ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 117ms/step\n1/1 [==============================] - 0s 118ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 114ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 120ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 113ms/step\n1/1 [==============================] - 0s 115ms/step\nAccuracy: 0.93\nWeighted F1: 0.9299789810829747\nMicro F1: 0.93\nWeighted Precision: 0.9301442307692308\nMicro Precision: 0.93\nWeighted Recall: 0.93\nMicro Recall: 0.93\nAverage Accuracy: 0.932\nAverage Weighted F1: 0.9319567206335906\nAverage Micro F1: 0.932\nAverage Weighted Precision: 0.932512898478372\nAverage Micro Precision: 0.932\nAverage Weighted Recall: 0.932\nAverage Micro Recall: 0.932\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### POS-TAGGED LONGFORMER\n* Average Accuracy: 0.924\n* Average Weighted F1: 0.9239799941578791\n* Average Micro F1: 0.924\n* Average Weighted Precision: 0.9253759552874665\n* Average Micro Precision: 0.924\n* Average Weighted Recall: 0.924\n* Average Micro Recall: 0.924\n\n\n### CLEANED DATA LONGFORMER\n\n* Average Accuracy: 0.932\n* Average Weighted F1: 0.9319567206335906\n* Average Micro F1: 0.932\n* Average Weighted Precision: 0.932512898478372\n* Average Micro Precision: 0.932\n* Average Weighted Recall: 0.932\n* Average Micro Recall: 0.932","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}