{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj3gUkvvEZW2"
   },
   "source": [
    "### <font color='blue'>Import all packages</font> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AwA6C92yJyaP",
    "outputId": "7977b2e6-8e43-40c5-e06e-7b01120112b5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install sentencepiece\n",
    "# %pip install tensorflow\n",
    "# %pip install stanza\n",
    "# %pip install tensorflow-addons\n",
    "# %pip install nltk\n",
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jV9qKCQ3qpl",
    "outputId": "cf68b4eb-84a2-44cd-913b-8e666212d3e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DC59AD4KEZW7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/envs/nlp/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import statistics\n",
    "import nltk\n",
    "#from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout, Input, BatchNormalization, Concatenate, Flatten\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import regularizers\n",
    "#from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import stanza\n",
    "from tensorflow.keras import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TFRobertaModel,RobertaTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from numpy.random import seed\n",
    "import random as python_random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "np.random.seed(1)\n",
    "python_random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vccKXcLYEZW-"
   },
   "source": [
    "### <font color='blue'> Preprocessing and cleaning functions </font> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 15:23:25.638921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 15:23:25.653263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 15:23:25.653389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 15:23:25.654072: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 15:23:25.655677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 15:23:25.655808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 15:23:25.655900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 15:23:26.051075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 15:23:26.051213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 15:23:26.051305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-24 15:23:26.051396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20788 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    inps_first_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_second_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_third_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_fourth_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_fifth_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_sixth_512 = Input(shape = (max_len,), dtype='int64')\n",
    "    inps_seventh_512 = Input(shape = (max_len,), dtype='int64')\n",
    "\n",
    "    masks_first_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_second_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_third_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_fourth_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_fifth_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_sixth_512= Input(shape = (max_len,), dtype='int64')\n",
    "    masks_seventh_512= Input(shape = (max_len,), dtype='int64')\n",
    "\n",
    "    dbert_layer_first_512 = dbert_model(inps_first_512, attention_mask=masks_first_512)[0][:,0,:]\n",
    "    dbert_layer_second_512 = dbert_model(inps_second_512, attention_mask=masks_second_512)[0][:,0,:]\n",
    "    dbert_layer_third_512 = dbert_model(inps_third_512, attention_mask=masks_third_512)[0][:,0,:]\n",
    "    dbert_layer_fourth_512 = dbert_model(inps_fourth_512, attention_mask=masks_fourth_512)[0][:,0,:]\n",
    "    dbert_layer_fifth_512 = dbert_model(inps_fifth_512, attention_mask=masks_fifth_512)[0][:,0,:]\n",
    "    dbert_layer_sixth_512 = dbert_model(inps_sixth_512, attention_mask=masks_sixth_512)[0][:,0,:]\n",
    "    dbert_layer_seventh_512 = dbert_model(inps_seventh_512, attention_mask=masks_seventh_512)[0][:,0,:]\n",
    "\n",
    "    concat=Concatenate()([dbert_layer_first_512, dbert_layer_second_512,dbert_layer_third_512, dbert_layer_fourth_512,dbert_layer_fifth_512, dbert_layer_sixth_512,dbert_layer_seventh_512])\n",
    "    dense_0 = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.001))(concat)\n",
    "    dropout_0= Dropout(0.5)(dense_0)\n",
    "    pred = Dense(5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
    "    #pred = Dense(279, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_0)\n",
    "    model = tf.keras.Model(inputs=[inps_first_512,masks_first_512,inps_second_512,masks_second_512,inps_third_512,masks_third_512,inps_fourth_512,masks_fourth_512,inps_fifth_512,masks_fifth_512,inps_sixth_512,masks_sixth_512,inps_seventh_512,masks_seventh_512], outputs=pred)\n",
    " \n",
    "    \n",
    "    print(model.summary())\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 654/654 [00:00<00:00, 7.35MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/ubuntu/.cache/huggingface/datasets/maneshkarun___parquet/maneshkarun--median3k_10000s-a12d2bed8c5e7733/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 161M/161M [00:03<00:00, 47.9MB/s]\n",
      "Downloading data: 100%|██████████| 194M/194M [00:03<00:00, 53.9MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:08<00:00,  8.63s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1198.37it/s]\n",
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/maneshkarun___parquet/maneshkarun--median3k_10000s-a12d2bed8c5e7733/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 679.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# data = load_dataset(\"maneshkarun/median-3000\")\n",
    "data = load_dataset(\"maneshkarun/median3k_10000s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'title', 'hyperpartisan', 'url', 'published_at', 'bias', 'word_count', 'cleaned_data', 'pos_tagged'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 0, 2, 2, 4, 4, 4, 4, 4, 1, 4, 1, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 2, 2, 0, 4, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 3, 4, 4, 2, 4, 1, 4, 4, 2, 2, 0, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 3, 2, 2, 2, 4, 4, 4, 3, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 2, 2, 2, 4, 4, 0, 4, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 4, 4, 2, 4, 4, 4, 2, 2, 2, 4, 0, 4, 4, 2, 4, 4, 4, 2, 2, 2, 2, 4, 3, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 4, 2, 2, 2, 4, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2, 3, 2, 4, 2, 4, 2, 2, 2, 4, 2, 3, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 1, 2, 2, 1, 2, 2, 4, 1, 4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 1, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 2, 0, 4, 3, 4, 2, 4, 2, 2, 4, 2, 3, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 4, 3, 4, 2, 2, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 4, 2, 2, 2, 0, 2, 0, 3, 4, 0, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 4, 4, 2, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 1, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 0, 3, 4, 4, 4, 4, 2, 4, 4]\n",
      "499\n",
      "Average Length of complete text 3235.1963927855713\n",
      "Found 499 texts in complete\n",
      "Median Length: 3234 \n",
      "Max length of complete text 3273\n",
      "Min length of complete text 3201\n",
      "Average Length of First 512 512.0\n",
      "Average Length of Second 512 512.0\n",
      "Found 499 texts in First 512.\n",
      "Found 499 texts in Second 512.\n",
      "Average Length of Third 512 512.0\n",
      "Average Length of Fourth 512 512.0\n",
      "Found 499 texts in Third 512.\n",
      "Found 499 texts in Fourth 512.\n",
      "Average Length of Fifth 512 512.0\n",
      "Average Length of Sixth 512 512.0\n",
      "Found 499 texts in Fifth 512.\n",
      "Found 499 texts in Sixth 512.\n",
      "Average Length of Seventh 512 512.0\n",
      "Found 499 texts in Seventh 512.\n",
      "Found 5 labels.\n",
      "                                        First_512_text  \\\n",
      "0    You received years of specialized training in ...   \n",
      "1    SEOUL Reuters POSCO 005490KS South Korea8217s ...   \n",
      "2    Photograph by Benjamin Sklar The night he won ...   \n",
      "3    You received years of specialized training in ...   \n",
      "4    Onamp160October 29 The Subcommittee On The Con...   \n",
      "..                                                 ...   \n",
      "494  Occupy Providence was a pivotal event in my li...   \n",
      "495  Veterans and family members of soldiers now in...   \n",
      "496  BEIJING Jan 23 Reuters Volkswagen on Tuesday s...   \n",
      "497  The top US government official in charge of ar...   \n",
      "498  amplta hrefhttpswwwflickrcomphotossoldiersmedi...   \n",
      "\n",
      "                                       Second_512_text  \\\n",
      "0    can8217t forget the things that have happened8...   \n",
      "1    Trade Organization WTO attends an interview wi...   \n",
      "2    of Kentucky and Mike Lee of Utah who are bent ...   \n",
      "3    can8217t forget the things that have happened8...   \n",
      "4    Cooper 360 71513 Zimmerman Trial Jury Instruct...   \n",
      "..                                                 ...   \n",
      "494  from Foucault quick to heart All these and so ...   \n",
      "495  about the campaign his chapter is launching on...   \n",
      "496  plans to announce retaliatory tariffs against ...   \n",
      "497  established the Rumsfeld missile defense commi...   \n",
      "498  Islamic government In a tape that was soon sho...   \n",
      "\n",
      "                                        Third_512_text  \\\n",
      "0    is essentially a superfluous partnership with ...   \n",
      "1    check restrict firearms sales for buyers under...   \n",
      "2    as if you wished Ted might be sick one year so...   \n",
      "3    is essentially a superfluous partnership with ...   \n",
      "4    reasonable means within his power and consiste...   \n",
      "..                                                 ...   \n",
      "494  the opening salvo in a wave of negotiations ov...   \n",
      "495  If they8217re serious about wanting to do some...   \n",
      "496  utility vehicles it also chose to scale back o...   \n",
      "497  War world 8220The dynamics of deterrents are m...   \n",
      "498  weapons of mass destruction or facilities for ...   \n",
      "\n",
      "                                       Fourth_512_text  \\\n",
      "0    before now that state legislatures are effecti...   \n",
      "1    disagree 8220Our eyes are wide open about what...   \n",
      "2    that the 14th Amendment should grant full citi...   \n",
      "3    before now that state legislatures are effecti...   \n",
      "4    600 Homicides Per Yearamp160In a June 2012 stu...   \n",
      "..                                                 ...   \n",
      "494  Middle East In Syria and IsraelPalestine solid...   \n",
      "495  the military a month ago When I ask why Christ...   \n",
      "496  of the Arizona accident are not yet known and ...   \n",
      "497  almost exclusively on proposals for hightech h...   \n",
      "498  Bush administration withdrew from the Kyoto Pr...   \n",
      "\n",
      "                                        Fifth_512_text  \\\n",
      "0    so I don8217t get fined or charged with a misd...   \n",
      "1    stocks sold off earlier on Thursday on the exp...   \n",
      "2    returned to Texas where in 2003 the state8217s...   \n",
      "3    so I don8217t get fined or charged with a misd...   \n",
      "4    Ground laws affect homicides and firearm injur...   \n",
      "..                                                 ...   \n",
      "494  groups like the Libertarian Party can stand as...   \n",
      "495  going on any missions But they continue to kee...   \n",
      "496  to company statements At Uber8217s September 2...   \n",
      "497  and use the weapons it sees fiteven nuclear wa...   \n",
      "498  In the words of the 8220National Security Stra...   \n",
      "\n",
      "                                        Sixth_512_text  \\\n",
      "0    shrinking pool of people who can help women sa...   \n",
      "1    global supply chains that feed companies such ...   \n",
      "2    224 la Ron Paul to eliminate the departments o...   \n",
      "3    shrinking pool of people who can help women sa...   \n",
      "4    of the country Mayors Against Illegal Guns 916...   \n",
      "..                                                 ...   \n",
      "494  it follows as a matter of course that strategy...   \n",
      "495  have many members who actually supported the i...   \n",
      "496  Un has pledged to denuclearize and meet US off...   \n",
      "497  disarmament and arms control to the conviction...   \n",
      "498  to the use of weapons of mass destruction in t...   \n",
      "\n",
      "                                      Seventh_512_text  label  \n",
      "0    and not have it hidden away like something we ...      4  \n",
      "1    businesses like Alphabet Inc Amazoncom Inc and...      2  \n",
      "2    freedom8221 and again gave a shoutout to his d...      4  \n",
      "3    and not have it hidden away like something we ...      4  \n",
      "4    seemed to show that permissive guncarrying law...      4  \n",
      "..                                                 ...    ...  \n",
      "494  the downfalls of Occupy Providence was our fai...      4  \n",
      "495  makers to bully the public is that antiwar pro...      4  \n",
      "496  of Kim Jong Un8217s visit soon became the thir...      2  \n",
      "497  role in the incident that went to the heart of...      4  \n",
      "498  between 1914 and 2001 like all measurements of...      4  \n",
      "\n",
      "[499 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_36 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_30 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_37 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_31 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_38 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_39 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_33 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_40 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_41 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_35 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_42 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_29[0][0]',               \n",
      "                                thPoolingAndCrossAt               'input_36[0][0]',               \n",
      "                                tentions(last_hidde               'input_30[0][0]',               \n",
      "                                n_state=(None, 512,               'input_37[0][0]',               \n",
      "                                 768),                            'input_31[0][0]',               \n",
      "                                 pooler_output=(Non               'input_38[0][0]',               \n",
      "                                e, 768),                          'input_32[0][0]',               \n",
      "                                 past_key_values=No               'input_39[0][0]',               \n",
      "                                ne, hidden_states=N               'input_33[0][0]',               \n",
      "                                one, attentions=Non               'input_40[0][0]',               \n",
      "                                e, cross_attentions               'input_34[0][0]',               \n",
      "                                =None)                            'input_41[0][0]',               \n",
      "                                                                  'input_35[0][0]',               \n",
      "                                                                  'input_42[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_13 (S  (None, 768)         0           ['tf_bert_model_2[0][0]']        \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_14 (S  (None, 768)         0           ['tf_bert_model_2[1][0]']        \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_15 (S  (None, 768)         0           ['tf_bert_model_2[2][0]']        \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_16 (S  (None, 768)         0           ['tf_bert_model_2[3][0]']        \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_17 (S  (None, 768)         0           ['tf_bert_model_2[4][0]']        \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_18 (S  (None, 768)         0           ['tf_bert_model_2[5][0]']        \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_19 (S  (None, 768)         0           ['tf_bert_model_2[6][0]']        \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 5376)         0           ['tf.__operators__.getitem_13[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_14[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_15[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_16[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_17[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_18[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          2753024     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_112 (Dropout)          (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            2565        ['dropout_112[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 112,237,829\n",
      "Trainable params: 112,237,829\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/envs/nlp/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    texts_complete=[]\n",
    "\n",
    "    texts_0_512 = []  # list of text samples\n",
    "    texts_448_960 = []  # list of text samples\n",
    "    texts_896_1408 = []  # list of text samples\n",
    "    texts_1344_1856 = []  # list of text samples\n",
    "    texts_1792_2304 = []  # list of text samples\n",
    "    texts_2240_2752 = []  # list of text samples\n",
    "    texts_2688_3200 = []  # list of text samples\n",
    "\n",
    "    label_list = []  # list of label ids\n",
    "\n",
    "    count=0\n",
    "\n",
    "    for record in train_data:\n",
    "\n",
    "            #print(record)\n",
    "            new_sen = record['cleaned_data']\n",
    "            new_new_sen = new_sen.split()\n",
    "            \n",
    "            if len(new_new_sen) > 3200:\n",
    "                count=count+1\n",
    "                # print(count)\n",
    "                if count == 500:\n",
    "                    break\n",
    "\n",
    "                label_list.append(record['bias'])\n",
    "\n",
    "                first_512=new_new_sen[0:512]\n",
    "                first_448_960=new_new_sen[448:960]\n",
    "                first_512=' '.join(first_512)\n",
    "                first_448_960=' '.join(first_448_960)\n",
    "\n",
    "                first_896_1408=new_new_sen[896:1408]\n",
    "                first_1344_1856=new_new_sen[1344:1856]\n",
    "                first_896_1408=' '.join(first_896_1408)\n",
    "                first_1344_1856=' '.join(first_1344_1856)\n",
    "\n",
    "                first_1792_2304=new_new_sen[1792:2304]\n",
    "                first_2240_2752=new_new_sen[2240:2752]\n",
    "                first_1792_2304=' '.join(first_1792_2304)\n",
    "                first_2240_2752=' '.join(first_2240_2752)\n",
    "\n",
    "                first_2688_3200=new_new_sen[2688:3200]\n",
    "                first_2688_3200=' '.join(first_2688_3200)\n",
    "\n",
    "                texts_complete.append(new_sen)\n",
    "                texts_0_512.append(first_512)\n",
    "                texts_448_960.append(first_448_960)\n",
    "                texts_896_1408.append(first_896_1408) \n",
    "                texts_1344_1856.append(first_1344_1856) \n",
    "                texts_1792_2304.append(first_1792_2304)  \n",
    "                texts_2240_2752.append(first_2240_2752)\n",
    "                texts_2688_3200.append(first_2688_3200) \n",
    "\n",
    "    len_list_complete = [len(ele.split()) for ele in texts_complete]\n",
    "    len_list_0_512 = [len(ele.split()) for ele in texts_0_512]\n",
    "    len_list_448_960 = [len(ele.split()) for ele in texts_448_960]\n",
    "    len_list_896_1408 = [len(ele.split()) for ele in texts_896_1408]\n",
    "    len_list_1344_1856 = [len(ele.split()) for ele in texts_1344_1856]\n",
    "    len_list_1792_2304 = [len(ele.split()) for ele in texts_1792_2304]\n",
    "    len_list_2240_2752 = [len(ele.split()) for ele in texts_2240_2752]\n",
    "    len_list_2688_3200 = [len(ele.split()) for ele in texts_2688_3200]\n",
    "\n",
    "    print(label_list)\n",
    "    print(len(label_list))\n",
    "\n",
    "\n",
    "    res_complete = 0 if len(len_list_complete) == 0 else (float(sum(len_list_complete)) / len(len_list_complete))\n",
    "    print(\"Average Length of complete text %s\" % res_complete) \n",
    "    print('Found %s texts in complete' % len(texts_complete))\n",
    "    print(\"Median Length: %s \" % statistics.median(len_list_complete))\n",
    "    print(\"Max length of complete text %s\" % max(len_list_complete))\n",
    "    print(\"Min length of complete text %s\" % min(len_list_complete))\n",
    "\n",
    "\n",
    "    res_0_512 = 0 if len(len_list_0_512) == 0 else (float(sum(len_list_0_512)) / len(len_list_0_512))\n",
    "    res_448_960 = 0 if len(len_list_448_960) == 0 else (float(sum(len_list_448_960)) / len(len_list_448_960))\n",
    "    print(\"Average Length of First 512 %s\" % res_0_512) \n",
    "    print(\"Average Length of Second 512 %s\" % res_448_960) \n",
    "    print('Found %s texts in First 512.' % len(texts_0_512))\n",
    "    print('Found %s texts in Second 512.' % len(texts_448_960))\n",
    "\n",
    "    res_896_1408 = 0 if len(len_list_896_1408) == 0 else (float(sum(len_list_896_1408)) / len(len_list_896_1408))\n",
    "    res_1344_1856 = 0 if len(len_list_1344_1856) == 0 else (float(sum(len_list_1344_1856)) / len(len_list_1344_1856))\n",
    "    print(\"Average Length of Third 512 %s\" % res_896_1408) \n",
    "    print(\"Average Length of Fourth 512 %s\" % res_1344_1856) \n",
    "    print('Found %s texts in Third 512.' % len(texts_896_1408))\n",
    "    print('Found %s texts in Fourth 512.' % len(texts_1344_1856))\n",
    "\n",
    "    res_1792_2304 = 0 if len(len_list_1792_2304) == 0 else (float(sum(len_list_1792_2304)) / len(len_list_1792_2304))\n",
    "    res_2240_2752 = 0 if len(len_list_2240_2752) == 0 else (float(sum(len_list_2240_2752)) / len(len_list_2240_2752))\n",
    "    print(\"Average Length of Fifth 512 %s\" % res_1792_2304) \n",
    "    print(\"Average Length of Sixth 512 %s\" % res_2240_2752) \n",
    "    print('Found %s texts in Fifth 512.' % len(texts_1792_2304))\n",
    "    print('Found %s texts in Sixth 512.' % len(texts_2240_2752))\n",
    "\n",
    "    res_2688_3200 = 0 if len(len_list_2688_3200) == 0 else (float(sum(len_list_2688_3200)) / len(len_list_2688_3200))\n",
    "    print(\"Average Length of Seventh 512 %s\" % res_2688_3200)\n",
    "    print('Found %s texts in Seventh 512.' % len(texts_2688_3200))\n",
    "\n",
    "\n",
    "    print('Found %s labels.' % len(set(label_list)))\n",
    "    summarized_data = pd.DataFrame(list(zip(texts_0_512,texts_448_960,texts_896_1408,texts_1344_1856,texts_1792_2304,texts_2240_2752,texts_2688_3200)),\n",
    "               columns =['First_512_text','Second_512_text','Third_512_text','Fourth_512_text','Fifth_512_text','Sixth_512_text', 'Seventh_512_text'])\n",
    "    summarized_data['label'] = label_list\n",
    "    print(summarized_data)\n",
    "    \n",
    "    dbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    dbert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    max_len=512\n",
    "    first_512_sentences=summarized_data['First_512_text']\n",
    "    second_512_sentences=summarized_data['Second_512_text']\n",
    "    third_512_sentences=summarized_data['Third_512_text']\n",
    "    fourth_512_sentences=summarized_data['Fourth_512_text']\n",
    "    fifth_512_sentences=summarized_data['Fifth_512_text']\n",
    "    sixth_512_sentences=summarized_data['Sixth_512_text']\n",
    "    seventh_512_sentences=summarized_data['Seventh_512_text']\n",
    "    labels=summarized_data['label']\n",
    "    len(first_512_sentences),len(labels),len(second_512_sentences),len(third_512_sentences),len(fourth_512_sentences),len(fifth_512_sentences),len(sixth_512_sentences),len(seventh_512_sentences)\n",
    "\n",
    "    model_0=create_model()\n",
    "\n",
    "    input_ids_first_512=[]\n",
    "    attention_masks_first_512=[]\n",
    "    for sent in summarized_data['First_512_text']:\n",
    "        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "        input_ids_first_512.append(dbert_inps['input_ids'])\n",
    "        attention_masks_first_512.append(dbert_inps['attention_mask'])\n",
    "    input_ids_first_512=np.asarray(input_ids_first_512)\n",
    "    attention_masks_first_512=np.array(attention_masks_first_512)\n",
    "    labels=np.array(labels)\n",
    "\n",
    "    input_ids_second_512=[]\n",
    "    attention_masks_second_512=[]\n",
    "    for sent in summarized_data['Second_512_text']:\n",
    "        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "        input_ids_second_512.append(dbert_inps['input_ids'])\n",
    "        attention_masks_second_512.append(dbert_inps['attention_mask'])\n",
    "    input_ids_second_512=np.asarray(input_ids_second_512)\n",
    "    attention_masks_second_512=np.array(attention_masks_second_512)\n",
    "\n",
    "    input_ids_third_512=[]\n",
    "    attention_masks_third_512=[]\n",
    "    for sent in summarized_data['Third_512_text']:\n",
    "        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "        input_ids_third_512.append(dbert_inps['input_ids'])\n",
    "        attention_masks_third_512.append(dbert_inps['attention_mask'])\n",
    "    input_ids_third_512=np.asarray(input_ids_third_512)\n",
    "    attention_masks_third_512=np.array(attention_masks_third_512)\n",
    "\n",
    "    input_ids_fourth_512=[]\n",
    "    attention_masks_fourth_512=[]\n",
    "    for sent in summarized_data['Fourth_512_text']:\n",
    "        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "        input_ids_fourth_512.append(dbert_inps['input_ids'])\n",
    "        attention_masks_fourth_512.append(dbert_inps['attention_mask'])\n",
    "    input_ids_fourth_512=np.asarray(input_ids_fourth_512)\n",
    "    attention_masks_fourth_512=np.array(attention_masks_fourth_512)\n",
    "\n",
    "    input_ids_fifth_512=[]\n",
    "    attention_masks_fifth_512=[]\n",
    "    for sent in summarized_data['Fifth_512_text']:\n",
    "        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "        input_ids_fifth_512.append(dbert_inps['input_ids'])\n",
    "        attention_masks_fifth_512.append(dbert_inps['attention_mask'])\n",
    "    input_ids_fifth_512=np.asarray(input_ids_fifth_512)\n",
    "    attention_masks_fifth_512=np.array(attention_masks_fifth_512)\n",
    "\n",
    "    input_ids_sixth_512=[]\n",
    "    attention_masks_sixth_512=[]\n",
    "    for sent in summarized_data['Sixth_512_text']:\n",
    "        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "        input_ids_sixth_512.append(dbert_inps['input_ids'])\n",
    "        attention_masks_sixth_512.append(dbert_inps['attention_mask'])\n",
    "    input_ids_sixth_512=np.asarray(input_ids_sixth_512)\n",
    "    attention_masks_sixth_512=np.array(attention_masks_sixth_512)\n",
    "    \n",
    "    input_ids_seventh_512=[]\n",
    "    attention_masks_seventh_512=[]\n",
    "    for sent in summarized_data['Seventh_512_text']:\n",
    "        dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "        input_ids_seventh_512.append(dbert_inps['input_ids'])\n",
    "        attention_masks_seventh_512.append(dbert_inps['attention_mask'])\n",
    "    input_ids_seventh_512=np.asarray(input_ids_seventh_512)\n",
    "    attention_masks_seventh_512=np.array(attention_masks_seventh_512)\n",
    "\n",
    "    train_inp_first_512,val_inp_first_512,train_label,val_label,train_mask_first_512,val_mask_first_512,train_inp_second_512,val_inp_second_512,train_mask_second_512,val_mask_second_512,train_inp_third_512,val_inp_third_512,train_mask_third_512,val_mask_third_512,train_inp_fourth_512,val_inp_fourth_512,train_mask_fourth_512,val_mask_fourth_512,train_inp_fifth_512,val_inp_fifth_512,train_mask_fifth_512,val_mask_fifth_512,train_inp_sixth_512,val_inp_sixth_512,train_mask_sixth_512,val_mask_sixth_512,train_inp_seventh_512,val_inp_seventh_512,train_mask_seventh_512,val_mask_seventh_512=train_test_split(input_ids_first_512,labels,attention_masks_first_512,input_ids_second_512,attention_masks_second_512,input_ids_third_512,attention_masks_third_512,input_ids_fourth_512,attention_masks_fourth_512,input_ids_fifth_512,attention_masks_fifth_512,input_ids_sixth_512,attention_masks_sixth_512,input_ids_seventh_512,attention_masks_seventh_512,test_size=0.1,random_state=42)\n",
    "\n",
    "    log_dir='dbert_model'\n",
    "\n",
    "    model_save_path='/home/ubuntu/HyperPartisan_Classification_Using_BERT/Stride/bert-stride64-512-0-15labels.h5'\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,monitor='val_accuracy',mode='max',save_best_only=True,save_weights_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "    model_0.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 24 15:39:47 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10          On   | 00000000:06:00.0 Off |                    0 |\n",
      "|  0%   52C    P0    60W / 150W |  21548MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     71513      C   ...conda/envs/nlp/bin/python    21546MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy=0\n",
    "total_weighted_f1=0\n",
    "total_micro_f1=0\n",
    "total_weighted_precision=0\n",
    "total_micro_precision=0\n",
    "total_weighted_recall=0\n",
    "total_micro_recall=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 15:43:35.351212: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_202588\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:27\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - ETA: 0s - loss: 1.5987 - accuracy: 0.8018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 15:46:05.083527: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_204032\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:53\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 2\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 163s 610ms/step - loss: 1.5987 - accuracy: 0.8018 - val_loss: 1.4191 - val_accuracy: 0.9000\n",
      "Epoch 2/5\n",
      "225/225 [==============================] - 130s 578ms/step - loss: 1.2684 - accuracy: 0.9243 - val_loss: 1.3709 - val_accuracy: 0.9000\n",
      "Epoch 3/5\n",
      "225/225 [==============================] - 130s 579ms/step - loss: 1.1462 - accuracy: 0.9421 - val_loss: 1.4071 - val_accuracy: 0.9000\n",
      "Epoch 4/5\n",
      " 52/225 [=====>........................] - ETA: 1:37 - loss: 1.0671 - accuracy: 0.9327"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "history=model_0.fit([train_inp_first_512,train_mask_first_512,train_inp_second_512,train_mask_second_512,train_inp_third_512,train_mask_third_512,train_inp_fourth_512,train_mask_fourth_512,train_inp_fifth_512,train_mask_fifth_512,train_inp_sixth_512,train_mask_sixth_512,train_inp_seventh_512,train_mask_seventh_512],train_label,batch_size=2,epochs=5,validation_data=([val_inp_first_512,val_mask_first_512,val_inp_second_512,val_mask_second_512,val_inp_third_512,val_mask_third_512,val_inp_fourth_512,val_mask_fourth_512,val_inp_fifth_512,val_mask_fifth_512,val_inp_sixth_512,val_mask_sixth_512,val_inp_seventh_512,val_mask_seventh_512],val_label),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_25 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_15[0][0]',               \n",
      "                                thPoolingAndCrossAt               'input_22[0][0]',               \n",
      "                                tentions(last_hidde               'input_16[0][0]',               \n",
      "                                n_state=(None, 512,               'input_23[0][0]',               \n",
      "                                 768),                            'input_17[0][0]',               \n",
      "                                 pooler_output=(Non               'input_24[0][0]',               \n",
      "                                e, 768),                          'input_18[0][0]',               \n",
      "                                 past_key_values=No               'input_25[0][0]',               \n",
      "                                ne, hidden_states=N               'input_19[0][0]',               \n",
      "                                one, attentions=Non               'input_26[0][0]',               \n",
      "                                e, cross_attentions               'input_20[0][0]',               \n",
      "                                =None)                            'input_27[0][0]',               \n",
      "                                                                  'input_21[0][0]',               \n",
      "                                                                  'input_28[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None, 768)         0           ['tf_bert_model[7][0]']          \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None, 768)         0           ['tf_bert_model[8][0]']          \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_bert_model[9][0]']          \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  (None, 768)         0           ['tf_bert_model[10][0]']         \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None, 768)         0           ['tf_bert_model[11][0]']         \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_12 (S  (None, 768)         0           ['tf_bert_model[12][0]']         \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_13 (S  (None, 768)         0           ['tf_bert_model[13][0]']         \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 5376)         0           ['tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_11[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_12[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_13[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          2753024     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           7695        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 112,242,959\n",
      "Trainable params: 112,242,959\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.7731591448931117\n",
      "Weighted F1: 0.7717866725178956\n",
      "Micro F1: 0.7731591448931117\n",
      "Weighted Precision: 0.7792082323978758\n",
      "Micro Precision: 0.7731591448931117\n",
      "Weighted Recall: 0.7731591448931117\n",
      "Micro Recall: 0.7731591448931117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wol_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model_saved= create_model()\n",
    "model_saved.compile(loss=loss,optimizer=optimizer, metrics=[accuracy])\n",
    "model_saved.load_weights('/home/ubuntu/HyperPartisan_Classification_Using_BERT/Stride/bert-stride64-512-0-15labels.h5')\n",
    "\n",
    "for i in range(0,len(val_inp_first_512)):\n",
    "    pred=model_saved.predict([val_inp_first_512[i].reshape(1,512),val_mask_first_512[i].reshape(1,512),val_inp_second_512[i].reshape(1,512),val_mask_second_512[i].reshape(1,512),val_inp_third_512[i].reshape(1,512),val_mask_third_512[i].reshape(1,512),val_inp_fourth_512[i].reshape(1,512),val_mask_fourth_512[i].reshape(1,512),val_inp_fifth_512[i].reshape(1,512),val_mask_fifth_512[i].reshape(1,512),val_inp_sixth_512[i].reshape(1,512),val_mask_sixth_512[i].reshape(1,512),val_inp_seventh_512[i].reshape(1,512),val_mask_seventh_512[i].reshape(1,512)])\n",
    "    pred_label = pred.argmax(axis=1)\n",
    "    pred_labels.append(pred_label)\n",
    "accuracy=accuracy_score(val_label, pred_labels)\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "total_accuracy=total_accuracy+accuracy\n",
    "\n",
    "weighted_f1=f1_score(val_label,pred_labels, average='weighted')\n",
    "print(\"Weighted F1: \"+ str(weighted_f1))\n",
    "total_weighted_f1=total_weighted_f1+weighted_f1\n",
    "micro_f1=f1_score(val_label,pred_labels, average='micro')\n",
    "print(\"Micro F1: \"+ str(micro_f1))\n",
    "total_micro_f1=total_micro_f1+micro_f1\n",
    "\n",
    "weighted_precision=precision_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Precision: \" + str(weighted_precision))\n",
    "total_weighted_precision=total_weighted_precision+weighted_precision\n",
    "micro_precision=precision_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "total_micro_precision=total_micro_precision+micro_precision\n",
    "\n",
    "weighted_recall=recall_score(val_label, pred_labels, average='weighted')\n",
    "print(\"Weighted Recall: \" + str(weighted_recall))\n",
    "total_weighted_recall=total_weighted_recall+weighted_recall\n",
    "micro_recall=recall_score(val_label, pred_labels, average='micro')\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "total_micro_recall=total_micro_recall+micro_recall"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BERT_Concat_15labels.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
