# HyperPartisan Classification Using BERT-Based Methods and Longformers

This repository contains the code and resources for developing a hyperpartisan news article classification system leveraging state-of-the-art transformer models like BERT, ROBERTa, and Longformer.

## Objective

The primary goal of this project is to accurately classify news articles as hyperpartisan or non-hyperpartisan. The project explores and compares the performance of various transformer models and assesses the impact of incorporating Part-of-Speech (POS) tagging on classification accuracy.

## Tools and Models

The project was implemented using the following tools and models:

- **Programming Language:** Python
- **Library:** Huggingface datasets
- **Models:**
  - BERT (Bidirectional Encoder Representations from Transformers)
  - ROBERTa (A Robustly Optimized BERT Pretraining Approach)
  - Longformer (The Long-Document Transformer)

## Outcome

- **Hyperpartisan Classification System:** Developed a robust system capable of classifying news articles into hyperpartisan or non-hyperpartisan categories.
- **Comparative Analysis:** Conducted a detailed comparison between BERT and ROBERTa models to understand their strengths and weaknesses.
- **Longformer Exploration:** Investigated the effectiveness of Longformer in handling lengthy news articles, a common challenge in text classification tasks.

## Getting Started

### Prerequisites

- Python 3.x
- Huggingface datasets library
- PyTorch

### Installation

1. Clone the repository:
   ```bash
     git clone https://github.com/AbineshSivakumar/HyperPartisan_Classification_Using_BERT
   ```
2. Install all the necessary packages
   ```bash
     pip install -r requirements.txt
   ```

### Usage

Run into each method and execute the respective jupyter notebooks

