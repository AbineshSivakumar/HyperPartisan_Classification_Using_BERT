# HyperPartisan Classification Using BERT Based Methods and Longformers
-Objective: To develop a hyperpartisan news article classification system using BERT-based techniques. The goal was to leverage state-of-the-art transformer models like BERT, ROBERTa, and Longformer to accurately classify news articles as hyperpartisan or non-hyperpartisan. Additionally, the project aimed to compare the performance of these models and assess the impact of incorporating POS tagging on classification accuracy.
-Role: Data Preprocessing, Model Training and Fine-tuning, Evaluation and Comparative Analysis
-Tools: Python, Huggingface datasets, BERT, ROBERTa, and Longformer
-Outcome: Successfully developed a hyperpartisan news article classification system using BERT-based techniques, conducted a comparative analysis of BERT and ROBERTa models, and explored the effectiveness of Longformer for handling lengthy news articles.
